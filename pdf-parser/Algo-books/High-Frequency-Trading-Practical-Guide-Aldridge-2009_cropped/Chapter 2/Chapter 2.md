## CHAPTER 2 **Evolution of High-Frequency** Trading

dyances in computer technology have supercharged the transmission and execution of orders and have compressed the holding . periods required for investments. Once applied to quantitative simulations of market behavior conditioned on large sets of historical data, a new investment discipline, called "high-frequency trading," was born.

This chapter examines the historical evolution of trading to explain how technological breakthroughs impacted financial markets and facilitated the emergence of high-frequency trading.

## FINANCIAL MARKETS AND TECHNOLOGICAL INNOVATION

Among the many developments affecting the operations of financial markets, technological innovation leaves the most persistent mark. While the introduction of new market securities, such as EUR/USD in 1999, created large-scale one-time disruptions in market routines, technological changes have a subtle and continuous impact on the markets. Over the years, technology has improved the way news is disseminated, the quality of financial analysis, and the speed of communication among market participants. While these changes have made the markets more transparent and reduced the number of traditional market inefficiencies, technology has also made available an entirely new set of arbitrage opportunities.

Many years ago, securities markets were run in an entirely manual fashion. To request a quote on a financial security, a client would contact his sales representative in person or via messengers and later via telegraph and telephone when telephony became available. The salesperson would then walk over or shout to the trading representative a request for prices on securities of interest to the client. The trader would report back the market prices obtained from other brokers and exchanges. The process would repeat itself when the client placed an order.

The process was slow, error-prone, and expensive, with the costs being passed on to the client. Most errors arose from two sources:

- **1.** Markets could move significantly between the time the market price was set on an exchange and the time the client received the quote.
- **2.** Errors were introduced in multiple levels of human communication, as people misheard the market data being transmitted.

The communication chain was as costly as it was unreliable, as all the links in the human chain were compensated for their efforts and market participants absorbed the costs of errors.

It was not until the 1980s that the first electronic dealing systems appeared and were immediately heralded as revolutionary. The systems aggregated market data across multiple dealers and exchanges, distributed information simultaneously to a multitude of market participants, allowed parties with preapproved credits to trade with each other at the best available prices displayed on the systems, and created reliable information and transaction logs. According to Leinweber (2007), designated order turnaround (DOT), introduced by the New York Stock Exchange (NYSE), was the first electronic execution system. DOT was accessible only to NYSE floor specialists, making it useful only for facilitation of the NYSE's internal operations. Nasdaq's computer-assisted execution system, available to broker-dealers, was rolled out in 1983, with the small-order execution system following in 1984.

While computer-based execution has been available on selected exchanges and networks since the mid-1980s, systematic trading did not gain traction until the 1990s. According to Goodhart and O'Hara (1997), the main reasons for the delay in adopting systematic trading were the high costs of computing as well as the low throughput of electronic orders on many exchanges. NASDAQ, for example, introduced its electronic execution capability in 1985, but made it available only for smaller orders of up to 1,000 shares at a time. Exchanges such as the American Stock Exchange (AMEX) and the NYSE developed hybrid electronic/floor markets that did not fully utilize electronic trading capabilities.

Once new technologies are accepted by financial institutions, their applications tend to further increase demand for automated trading. To wit, rapid increases in the proportion of systematic funds among all hedge

![](_page_2_Figure_1.jpeg)

**FIGURE 2.1** Absolute number and relative proportion of hedge funds identifying themselves as "systematic." *Source:* Aldridge (2009b).

funds coincided with important developments in trading technology. As Figure 2.1 shows, a notable rise in the number of systematic funds occurred in the early 1990s. Coincidentally, in 1992 the Chicago Mercantile Exchange (CME) launched its first electronic platform, Globex. Initially, Globex traded only CME futures on the most liquid currency pairs: Deutsche mark and Japanese yen. Electronic trading was subsequently extended to CME futures on British pounds, Swiss francs, and Australian and Canadian dollars. In 1993, systematic trading was enabled for CME equity futures. By October 2002, electronic trading on the CME reached an average daily volume of 1.2 million contracts, and innovation and expansion of trading technology continued henceforth, causing an explosion in systematic trading in futures along the way.

The first fully electronic U.S. options exchange was launched in 2000 by the New York–based International Securities Exchange (ISE). As of mid-2008, seven exchanges offered either fully electronic or a hybrid mix of floor and electronic trading in options. These seven exchanges are ISE, Chicago Board Options Exchange (CBOE), Boston Options Exchange (BOX), AMEX, NYSE's Arca Options, and Nasdaq Options Market (NOM).

According to estimates conducted by Boston-based Aite Group, shown in Figure 2.2, adoption of electronic trading has grown from 25 percent of trading volume in 2001 to 85 percent in 2008. Close to 100 percent of equity trading is expected to be performed over the electronic networks by 2010.

Technological developments markedly increased the daily trade volume. In 1923, 1 million shares traded per day on the NYSE, while just over 1 billion shares were traded per day on the NYSE in 2003, a 1,000-times increase.

![](_page_3_Figure_1.jpeg)

**FIGURE 2.2** Adoption of electronic trading capabilities by asset class. *Source:* Aite Group.

Technological advances have also changed the industry structure for financial services from a rigid hierarchical structure popular through most of the 20th century to a flat decentralized network that has become the standard since the late 1990s. The traditional 20th-century network of financial services is illustrated in Figure 2.3. At the core are the exchanges or, in the case of foreign exchange trading, inter-dealer networks. Exchanges are the centralized marketplaces for transacting and clearing securities orders. In decentralized foreign exchange markets, inter-dealer networks consist of inter-dealer brokers, which, like exchanges, are organizations that ensure liquidity in the markets and deal between their peers and broker-dealers.

Broker-dealers perform two functions—trading for their own accounts (known as "proprietary trading" or "prop trading") and transacting and clearing trades for their customers. Broker-dealers use inter-dealer brokers to quickly find the best price for a particular security among the network of other broker-dealers. Occasionally, broker-dealers also deal directly with other broker-dealers, particularly for less liquid instruments such as customized option contracts. Broker-dealers' transacting clients are investment banking clients (institutional clients), large corporations (corporate clients), medium-sized firms (commercial clients), and high-net-worth individuals (HNW clients). Investment institutions can in turn be brokerages providing trading access to other, smaller institutions and individuals with smaller accounts (retail clients).

Until the late 1990s, it was the broker-dealers who played the central and most profitable roles in the financial ecosystem; broker-dealers controlled clients' access to the exchanges and were compensated handsomely for doing so. Multiple layers of brokers served different levels of investors. The institutional investors, the well-capitalized professional investment outfits, were served by the elite class of institutional sales brokers that sought volume; the individual investors were assisted by the retail brokers that charged higher commissions. This hierarchical structure existed from the early 1920s through much of the 1990s when the advent of the

![](_page_4_Figure_1.jpeg)

**FIGURE 2.3** Twentieth-century structure of capital markets.

Internet uprooted the traditional order. At that time, a garden variety of online broker-dealers sprung up, ready to offer direct connectivity to the exchanges, and the broker structure flattened dramatically.

Dealers trade large lots by aggregating their client orders. To ensure speedy execution for their clients on demand, dealers typically "run books"—inventories of securities that the dealers expand or shrink depending on their expectation of future demand and market conditions. To compensate for the risk of holding the inventory and the convenience of transacting in lots as small as \$100,000, the dealers charge their clients a spread on top of the spread provided by the inter-broker dealers. Because of the volume requirement, the clients of a dealer normally cannot deal directly with exchanges or inter-dealer brokers. Similarly, due to volume requirements, retail clients cannot typically gain direct access either to inter-dealer brokers or to dealers.

Today, financial markets are becoming increasingly decentralized. Competing exchanges have sprung up to provide increased trading liquidity in addition to the market stalwarts, such as NYSE and AMEX. Following the advances in computer technology, the networks are flattening, and exchanges and inter-dealer brokers are gradually giving way to electronic communication networks (ECNs), also known as "liquidity pools." ECNs employ sophisticated algorithms to quickly transmit orders and to optimally match buyers and sellers. In "dark" liquidity pools, trader identities and orders remain anonymous.

Island is one of the largest ECNs, which traded about 10 percent of NASDAQ's volume in 2002. On Island, all market participants can post their limit orders anonymously. Biais, Bisiere and Spatt (2003) find that the higher the liquidity on NASDAQ, the higher the liquidity on Island, but the reverse does not necessarily hold. Automated Trading Desk, LLC (ATD) is an example of a dark pool. The customers of the pool do not see the identities or the market depth of their peers, ensuring anonymous liquidity. ATD algorithms further screen for disruptive behaviors such as spread manipulation. The identified culprits are financially penalized for inappropriate behavior.

Figure 2.4 illustrates the resulting "distributed" nature of a typical modern network incorporating ECNs and dark pool structures. The lines connecting the network participants indicate possible dealing routes. Typically, only exchanges, ECNs, dark pools, broker-dealers, and retail brokerages have the ability to clear and settle the transactions, although

![](_page_5_Figure_4.jpeg)

**FIGURE 2.4** Contemporary trading networks.

selected institutional clients, such as Chicago-based Citadel, have recently acquired broker-dealer arms of investment banks and are now able to clear all the trades in-house.

## **EVOLUTION OF TRADING METHODOLOGY**

One of the earlier techniques that became popular with many traders was technical analysis. Technical analysts sought to identify recurring patterns in security prices. Many techniques used in technical analysis measure current price levels relative to the rolling moving average of the price, or a combination of the moving average and standard deviation of the price. For example, a technical analysis technique known as moving average convergence divergence (MACD) uses three exponential moving averages to generate trading signals. Advanced technical analysts may look at security prices in conjunction with current market events or general market conditions to obtain a fuller idea of where the prices may be moving next.

Technical analysis prospered through the first half of the 20th century, when trading technology was in its telegraph and pneumatic-tube stages and the trading complexity of major securities was considerably lower than it is today. The inability to transmit information quickly limited the number of shares that changed hands, curtailed the pace at which information was incorporated into prices, and allowed charts to display latent supply and demand of securities. The previous day's trades appeared in the next morning's newspaper and were often sufficient for technical analysts to successfully infer future movement of the prices based on published information. In post-WWII decades, when trading technology began to develop considerably, technical analysis developed into a self-fulfilling prophecy.

If, for example, enough people believed that a "head-and-shoulders" pattern would be followed by a steep sell-off in a particular instrument, all the believers would place sell orders following a head-and-shoulders pattern, thus indeed realizing the prediction. Subsequently, institutional investors began modeling technical patterns using powerful computer technology, and trading them away before they became apparent to the naked eye. By now, technical analysis at low frequencies, such as daily or weekly intervals, is marginalized to work only for the smallest, least liquid securities, which are traded at very low frequencies—once or twice per day or even per week. However, several researchers find that technical analysis still has legs: Brock, Lakonishok, and LeBaron (1992) find that moving averages can predict future abnormal returns, while Aldridge (2009a) shows that moving averages, "stochastics" and relative strength indicators (RSI) may succeed in generating profitable trading signals on intra-day data sampled at hourly intervals.

In a way, technical analysis was a precursor of modern microstructure theory. Even though market microstructure applies at a much higher frequency and with a much higher degree of sophistication than technical analysis, both market microstructure and technical analysis work to infer market supply and demand from past price movements. Much of the contemporary high-frequency trading is based on detecting latent market information from the minute changes in the most recent price movements. Not many of the predefined technical patterns, however, work consistently in the high-frequency environment. Instead, high-frequency trading models are built on probability-driven econometric inferences, often incorporating fundamental analysis.

Fundamental analysis originated in equities, when traders noticed that future cash flows, such as dividends, affected market price levels. The cash flows were then discounted back to the present to obtain the fair present market value of the security. Graham and Dodd (1934) were one of the earliest purveyors of the methodology and their approach is still popular. Over the years, the term *fundamental analysis* expanded to include pricing of securities with no obvious cash flows based on expected economic variables. For example, fundamental determination of exchange rates today implies equilibrium valuation of the rates based on macroeconomic theories.

Fundamental analysis developed through much of the 20th century. Today, fundamental analysis refers to trading on the expectation that the prices will move to the level predicted by supply and demand relationships, the fundamentals of economic theory. In equities, microeconomic models apply; equity prices are still most often determined as present values of future cash flows. In foreign exchange, macroeconomic models are most prevalent; the models specify expected price levels using information about inflation, trade balances of different countries, and other macroeconomic variables. Derivatives are traded fundamentally through advanced econometric models that incorporate statistical properties of price movements of underlying instruments. Fundamental commodities trading analyzes and matches available supply and demand.

Various facets of the fundamental analysis are active inputs into many high-frequency trading models, alongside market microstructure. For example, event arbitrage consists of trading the momentum response accompanying the price adjustment of the security in response to new fundamental information. The date and time of the occurrence of the news event is typically known in advance, and the content of the news is usually revealed at the time of the news announcement. In high-frequency event arbitrage, fundamental analysis can be used to forecast the fundamental value of the economic variable to be announced, in order to further refine the high-frequency process.

Technical and fundamental analyses coexisted through much of the 20th century, when an influx of the new breed of traders armed with advanced degrees in physics and statistics arrived on Wall Street. These warriors, dubbed quants, developed advanced mathematical models that often had little to do with the traditional old-school fundamental and technical thinking. The new quant models gave rise to "quant trading," a mathematical model–fueled trading methodology that was a radical departure from established technical and fundamental trading styles. "Statistical arbitrage" strategies (*stat-arb* for short) became the new stars in the moneymaking arena. As the news of great stat-arb performances spread, their techniques became widely popular, and the constant innovation arms race ensued; the people who kept ahead of the pack were likely to reap the highest gains.

The most obvious aspect of competition was speed. Whoever was able to run a quant model the fastest was the first to identify and trade upon a market inefficiency and was the one to capture the biggest gain. To increase trading speed, traders began to rely on fast computers to make and execute trading decisions. Technological progress enabled exchanges to adapt to the new technology-driven culture and offer docking convenient for trading. Computerized trading became known as "systematic trading" after the computer systems that processed run-time data and made and executed buy-and-sell decisions.

High-frequency trading developed in the 1990s in response to advances in computer technology and the adoption of the new technology by the exchanges. From the original rudimentary order processing to the current state-of-the-art all-inclusive trading systems, high-frequency trading has evolved into a billion-dollar industry.

To ensure optimal execution of systematic trading, algorithms were designed to mimic established execution strategies of traditional traders. To this day, the term "algorithmic trading" usually refers to the systematic execution process—that is, the optimization of buy-and-sell decisions once these buy-and-sell decisions were made by another part of the systematic trading process or by a human portfolio manager. Algorithmic trading may determine how to process an order given current market conditions: whether to execute the order aggressively (on a price close to the market price) or passively (on a limit price far removed from the current market price), in one trade or split into several smaller "packets." As mentioned previously, algorithmic trading does not usually make portfolio allocation decisions; the decisions about when to buy or sell which securities are assumed to be exogenous.

High-frequency trading became a trading methodology defined as quantitative analysis embedded in computer systems processing data and making trading decisions at high speeds and keeping no positions overnight.

The advances in computer technology over the past decades have enabled fully automated high-frequency trading, fueling the profitability of trading desks and generating interest in pushing the technology even further. Trading desks seized upon cost savings realized from replacing expensive trader headcount with less expensive trading algorithms along with other advanced computer technology. Immediacy and accuracy of execution and lack of hesitation offered by machines as compared with human traders have also played a significant role in banks' decisions to switch away from traditional trading to systematic operations. Lack of overnight positions has translated into immediate savings due to reduction in overnight position carry costs, a particular issue in crisis-driven tight lending conditions or high-interest environments.

Banks also developed and adopted high-frequency functionality in response to demand from buy-side investors. Institutional investors, in turn, have been encouraged to practice high-frequency trading by the influx of capital following shorter lock-ups and daily disclosure to investors. Both institutional and retail investors found that investment products based on quantitative intra-day trading have little correlation with traditional buyand-hold strategies, adding pure return, or alpha, to their portfolios.

As computer technology develops further and drops in price, highfrequency systems are bound to take on an even more active role. Special care should be taken, however, to distinguish high-frequency trading from electronic trading, algorithmic trading, and systematic trading. Figure 2.5 illustrates a schematic difference between high-frequency, systematic, and traditional long-term investing styles.

Electronic trading refers to the ability to transmit the orders electronically as opposed to telephone, mail, or in person. Since most orders in today's financial markets are transmitted via computer networks, the term electronic trading is rapidly becoming obsolete.

Algorithmic trading is more complex than electronic trading and can refer to a variety of algorithms spanning order-execution processes as well as high-frequency portfolio allocation decisions. The execution algorithms are designed to optimize trading execution once the buy-and-sell decisions have been made elsewhere. Algorithmic execution makes decisions about the best way to route the order to the exchange, the best point in time to execute a submitted order if the order is not required to be executed immediately, and the best sequence of sizes in which the order should be optimally processed. Algorithms generating high-frequency trading signals make portfolio allocation decisions and decisions to enter or close a

![](_page_10_Figure_1.jpeg)

**FIGURE 2.5** High-frequency trading versus algorithmic (systematic) trading and traditional long-term investing.

position in a particular security. For example, algorithmic execution may determine that a received order to buy 1,000,000 shares of IBM is best handled using increments of 100 share lots to prevent a sudden run-up in the price. The decision fed to the execution algorithm, however, may or may not be high-frequency. An algorithm deployed to generate high-frequency trading signals, on the other hand, would generate the decision to buy the 1,000,000 shares of IBM. The high-frequency signals would then be passed on to the execution algorithm that would determine the optimal timing and routing of the order.

Successful implementation of high-frequency trading requires both types of algorithms: those generating high-frequency trading signals and those optimizing execution of trading decisions. Algorithms designed for generation of trading signals tend to be much more complex than those focusing on optimization of execution. Much of this book is devoted to algorithms used to generate high-frequency trading signals. Common algorithms used to optimize trade execution in algorithmic trading are discussed in detail in Chapter 18.

The intent of algorithmic execution is illustrated by the results of a TRADE Group survey. Figure 2.6 shows the full spectrum of responses from the TRADE survey. The proportion of buy-side traders using algorithms in their trading increased from 9 percent in 2008 to 26 percent in 2009, with algorithms at least partially managing over 40 percent of the

![](_page_11_Figure_1.jpeg)

**FIGURE 2.6** Reasons for using algorithms in trading. *Source:* The TRADE Annual Algorithmic Survey.

total order flow, according to the 2009 Annual Algorithmic Trading Survey conducted by the TRADE Group. In addition to the previously mentioned factors related to adoption of algorithmic trading, such as productivity and accuracy of traders, the buy-side managers also reported their use of the algorithms to be driven by the anonymity of execution that the algorithmic trading permits. Stealth execution allows large investors to hide their trading intentions from other market participants, thus deflecting the possibilities of order poaching and increasing overall profitability.

Systematic trading refers to computer-driven trading positions that may be held a month or a day or a minute and therefore may or may not be high-frequency. An example of systematic trading is a computer program that runs daily, weekly, or even monthly; accepts daily closing prices; outputs portfolio allocation matrices; and places buy-and-sell orders. Such a system is not a high-frequency system.

True high-frequency trading systems make a full range of decisions, from identification of underpriced or overpriced securities, through optimal portfolio allocation, to best execution. The distinguishing characteristic of high-frequency trading is the short position holding times, one day or shorter in duration, usually with no positions held overnight. Because of their rapid execution nature, most high-frequency trading systems are fully systematic and are also examples of systematic and algorithmic trading. All systematic and algorithmic trading platforms, however, are not highfrequency.

Ability to execute a security order algorithmically is a prerequisite for high-frequency trading in a given security. As discussed in Chapter 4, some markets are not yet suitable for high-frequency trading, inasmuch as most trading in these markets is performed over the counter (OTC). According to research conducted by Aite Group, equities are the most algorithmically

![](_page_12_Figure_1.jpeg)

**FIGURE 2.7** Adoption of algorithmic execution by asset class. *Source:* Aite Group.

executed asset class, with over 50 percent of the total volume of equities expected to be handled by algorithms by 2010. As Figure 2.7 shows, equities are closely followed by futures. Advances in algorithmic execution of foreign exchange, options, and fixed income, however, have been less visible. As illustrated in Figure 2.7, the lag of fixed income instruments can be explained by the relative tardiness of electronic trading development for them, given that many of them are traded OTC and are difficult to synchronize as a result.

While research dedicated to the performance of high-frequency trading is scarce, due to the unavailability of system performance data relative to data on long-term buy-and-hold strategies, anecdotal evidence suggests that most computer-driven strategies are high-frequency strategies. Systematic and algorithmic trading naturally lends itself to trading applications demanding high speed and precision of execution, as well as high-frequency analysis of volumes of tick data. Systematic trading, in turn, has been shown to outperform human-led trading along several key metrics. Aldridge (2009b), for example, shows that systematic funds consistently outperform traditional trading operations when performance is measured by Jensen's alpha (Jensen, 1968), a metric of returns designed to measure the unique skill of trading by abstracting performance from broad market influences. Aldridge (2009b) also shows that the systematic funds outperform nonsystematic funds in raw returns in times of crisis. That finding can be attributed to the lack of emotion inherent in systematic trading strategies as compared with emotion-driven human traders.