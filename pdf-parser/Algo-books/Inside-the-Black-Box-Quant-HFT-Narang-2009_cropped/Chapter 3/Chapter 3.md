CHAPTER -

# **Alpha Models: How Quants Make Money**

Prediction is very difficult, especially about the future.

-Niels Bohr

aving surveyed it from the outside, we begin our journey *through* the **black** box by understanding the heart of the actual trading systems that quants use. This first piece of a quant trading system is its *alpha model*, which is the part of the model that is looking to make money and is where much of the research process is focused. *Alpha*, the spelled-out version of the Greek letter  $\alpha$ , generally is used as a way to quantify the *skill* of an investor or the return she delivers independently of the moves in the broader market. By conventional definition, alpha is the portion of the investor's return *not* due to the market benchmark, or, in other words, the value added (or lost) solely because of the manager. The portion of the return that can be attributed to market factors is then referred to as beta. For instance, if a manager is up 12 percent and her respective benchmark is up 10 percent, a quick back-of-the-envelope analysis would show that her alpha, or value added, is  $+2$  percent (this assumes that the beta of her portfolio was exactly 1). The flaw with this approach to computing alpha is that it could be a result of luck, or it could be because of skill. Obviously, any trader will be interested in making skill the dominant driver of the difference between her returns and the benchmark's. Alpha models are merely a systematic approach to adding skill to the investment process in order to make profits. For example, a trend-following trader's ability to systematically identify trends that will persist into the future represents one type of skill that can generate profits.

Our definition of alpha-which I stress is not conventional-is skill in timing the selection and/or sizing of portfolio holdings. A pursuit of alpha holds as a core premise that no instrument is inherently good or bad, and therefore no instrument is worth always owning or perpetually shorting. The trend follower determines *when* to buy and sell various instruments, as does the value trader. Each of these is a type of alpha. In the first case, alpha is generated from the skill in identifying trends, which allows the trend follower to know when it is good to be long or short a given instrument. Similarly, a value trader does not say that a given stock is cheap now and therefore is worth owning in perpetuity. In fact, if a stock is always cheap, it is almost certainly *not* worth owning, because its valuation never improves for the investor. Instead, the idea behind value investing is to buy a stock when it is undervalued and to sell it when it is fairly valued or overvalued. Again, this represents an effort to time the stock.

The software that a quant builds and uses to conduct this timing systematically is known as an *alpha model*, though there are many synonyms for this term: forecast, factor, alpha, model, strategy, estimator, or predictor. All successful alpha models are designed to have some edge, which allows them to anticipate the future with enough accuracy that, after allowing for being wrong at least sometimes and for the cost of trading, they can still make money. In a sense, of the various parts of a quant strategy, the alpha model is the optimist, focused on making money by predicting the future.

To make money, generally some risk, or exposure, must be accepted. By utilizing a strategy, we directly run the risk of losing money when the environment for that strategy is adverse. For example, Warren Buffett has beaten the market over the long term, and this differential is a measure of his alpha. But there have been times when he struggled to add value, as he did during the dot‐com bubble of the late 1990s. His strategy was out of favor, and his underperformance during this period reflected this fact. In the case of alpha models, the same holds true: Whatever exposures they take on are rewarding if they are in favor, and are costly if they are out of favor. This chapter addresses the kinds of alpha models that exist and the ways that quants actually use the forecasts their models make.

## Types of Alpha Models: Theory-Driven and Data-Driven

An important and not widely understood fact is that only a small number of trading strategies exist for someone seeking alpha. But these basic strategies can be implemented in many ways, making it possible to create an incredible diversity of strategies from a limited set of core ideas. The first key to understanding quant trading strategies is to understand the perspectives quants take on science.

Because most quants are trained first in the sciences and only later in finance, quants' scientific backgrounds frequently determine the approach they take to trading over their entire careers. The two major branches of science are theoretical and empirical. *Theoretical scientists* try to make sense of the world around them by hypothesizing *why* it is the way it is. This is the kind of science with which people are most familiar and interact most. For example, viable, controllable, long‐distance airplanes exist largely because engineers apply theories of aerodynamics. *Empirical scientists* believe that enough observations of the world can allow them to predict future patterns of behavior, even if there is no hypothesis to rationalize the behavior in an intuitive way. In other words, knowledge comes from experience. The Human Genome Project is one of many important examples of the applications of empirical science, mapping human traits to the sequences of chemical base pairs that make up human DNA.

The distinction between theoretical and empirical science is germane to quantitative trading in that there are also two kinds of quant traders. The first, and by far the more common, are theory driven. They start with observations of the markets, think of a generalized theory that could explain the observed behavior, and then rigorously test it with market data to see if the theory is shown to be either untrue or supported by the outcome of the test. In quant trading, most of these theories would make sense to you or me and would seem sensible when explained to friends at cocktail parties. For example, "cheap stocks outperform expensive stocks" is a theory that many people hold. This explains the existence of countless value funds. Once precisely defined, this theory can be tested.

The second kind of scientist, by far in the minority, believes that correctly performed empirical observation and analysis of the data can obviate the need for theory. Such a scientist's theory, in short, is that there are recognizable patterns in the data that can be detected with careful application of the right techniques. Again, the example of the Human Genome Project is instructive. The scientists in the Human Genome Project did not believe that it was necessary to theorize what genes were responsible for particular human traits. Rather, scientists merely theorized that the relationships between genes and traits can be mapped using statistical techniques, and they proceeded to do exactly that. Empirical scientists are sometimes derisively (and sometimes just as a matter of fact) labeled *data miners*. They don't especially care if they can name their theories and instead attempt to use data analysis techniques to uncover behaviors in the market that aren't intuitively obvious.

It is worthwhile to note that theory‐driven scientists (and quants) are also reliant on observations (data) to derive theories in the first place. Just like the empiricists, they, too, believe that something one can observe in the data will be repeatable in the future. Empiricists, however, are less sensitive to whether their human minds can synthesize a story to explain the data even if, in the process, they risk finding relationships or patterns in the data that are entirely spurious.

## Theory-Driven Alpha Models

Most quants you will come across are theory driven. They start with some economically feasible explanation of why the markets behave in a certain way and test these theories to see whether they can be used to predict the future with any success. Many quants think that their theories are somewhat unique to them, which is part of the reason so many of them are so secretive. But this turns out, almost always, to be a delusion. Meanwhile, many outside the quant trading world believe that the kinds of strategies quants use are complex and based on complicated mathematical formulae. This generally also turns out to be false.

In fact—and in defiance of both the presumed need for secrecy and the claims that what quants do cannot be understood by those without doctorate degrees—most of what theory‐driven quants do can be relatively easily fit into one of six classes of phenomena: trend, reversion, technical sentiment, value/yield, growth, and quality. *It is worth noting that the kinds of strategies that quants utilize are actually exactly the same as those that can be utilized by discretionary traders seeking alpha.* These six categories can be further understood by examining the data that they use: price‐related data and fundamental data. As we will see throughout this book, understanding the inputs to a strategy is extremely important to understanding the strategy itself. The first two categories of strategies, *trend* and *mean reversion*, are based on price‐related data. *Technical sentiment* strategies are less commonly found, but can be thought of as a third class of price‐based strategies. The remaining three strategies, *value/yield, growth/sentiment*, and *quality*, are based on fundamental and/or fundamental sentiment data.

Many successful quants utilize more than one type of alpha model in conjunction, but to gain a proper understanding of these strategies, we will first break them down individually and discuss the combination of them afterward. Exhibit 3.1 provides a summary and outline for understanding the types of alpha models that quants use.

#### Strategies Utilizing Price-Related Data

First we will focus on alpha models that utilize price‐related data, which are mostly about the prices of various instruments or other information that generally comes from an exchange (such as trading volume). Quants

![](_page_4_Figure_1.jpeg)

EXHIBIT 3.1 A Taxonomy of Theory-Driven Alpha Models

who seek to forecast prices and to profit from such forecasts are likely to be exploiting one of two kinds of phenomena. The first is that an established trend will continue, and the second is that the trend will reverse. In other words, the price can either keep going in the direction it was going already, or it can go in the opposite direction. We call the first idea *trend following* or *momentum*, and we call the second idea *counter-trend* or *mean reversion*. A third idea will be explored as well, which we refer to as *technical sentiment*. This is a far less common type of alpha, but which deserves some discussion.

**Trend Following** Trend following is based on the theory that markets sometimes move for long enough in a given direction that one can identify this trend and ride it. The economic rationale for the existence of trends is based on the idea of consensus building among market participants. Imagine that there is uncertainty about the medium-term outlook for the U.S. economy. The labor picture looks fine, but inflation is running rampant, and trade deficits are blooming. On the other hand, consumers are still spending, and housing is strong. This conflicting information is a regular state of affairs for economies and markets, so that some of the information available appears favorable and some unfavorable. In our example, let's further imagine that the bears have it right—that in fact inflation will get out of control and cause problems for the economy. The earliest adopters of this idea place their trades in accordance with it by, for example, selling bonds short. As more and more data come out to support their thesis and as a growing mass of market participants adopts the same thesis, the price of U.S. bonds may take a considerable amount of time to move to its new equilibrium, and this slow migration from one equilibrium to the next is the core opportunity that the trend follower looks to capture.

It bears mentioning that there is an alternate explanation of why trends happen; it is affectionately known as the *greater fools theory*. The idea here is that, because people believe in trends, they tend to start buying anything that's been going up and selling anything that's been going down, which itself perpetuates the trend. The key is always to sell your position to someone more foolish, and thereby to avoid being the last fool. Either theoretical explanation, coupled with the evidence in markets, seems a valid enough reason to believe in trends.

Trend followers typically look for a "significant" move in a given direction in an instrument. They bet that, once a significant move has occurred, it will persist because this significant move is a likely sign of a growing consensus (or a parade of fools). They prefer this significance because a great risk of trend‐following strategies is whipsawing action in markets, which describes a somewhat rapid up‐and‐down pattern in prices. If, in other words, you buy the S&P because it was up over the past three months (and, symmetrically, sell short the S&P every time it was down over the three months prior), you need the trend to keep going in the same direction *after* the three‐month observation period. If the S&P reverses direction roughly every three months, a strategy such as this would lose money on more or less every trade over that period. There are many ways of defining what kind of move is significant, and the most common terms used to describe this act of definition are *filtering* and *conditioning*. This turns out to be an important source of differentiation among the various players who pursue trend‐following strategies and will be explored further in "Implementing the Strategies."

Perhaps the most obvious and well‐known example of a strategy that depends on trends is in the world of futures trading, also known as managed futures or commodities trading advisors (CTAs). Exhibit 3.2 illustrates the downward trend in equities that began in the fourth quarter of 2007. One way to define a trend for trading purposes, known as a *moving average crossover* indicator, is to compare the average price of the index over a shorter time period (e.g., 60 days) to that of a longer time period (e.g., 200 days). When the shorter‐term average price is below the longer‐term average price, the index is said to be in a negative trend, and when the shorter‐term average price is above the longer‐term average, the index is in a positive trend. As such, a trend follower using this kind of strategy might have gotten short the S&P Index around the end of 2007, as indicated by the point at which the two moving averages cross over each other, and remained short for most or all of 2008.

Some of the largest quantitative asset managers engage in trend following in futures markets, which also happens to be the oldest of all quant trading strategies, as far as I can tell. Ed Seykota built the first computerized version of the mechanical trend‐following strategy that Richard Donchian created some years earlier, utilizing punch cards on an IBM mainframe in

![](_page_6_Figure_1.jpeg)

**S&P 500 Index, October 3, 2007 to December 9, 2008**

Exhibit 3.2 S&P 500 Trend

1970, a year after he graduated from MIT. He was a strong believer in doing ongoing research, and over the course of his first 12 years, he turned \$5,000 into \$15,000,000. He went on to a highly successful three‐decade‐long career, over which he annualized some 60 percent returns.1

Larry Hite represents another interesting example of an early practitioner of trend following. Previously, Hite was a rock promoter in New York who, after experiencing three separate nightclub shootings on a single night, decided a change of career was in order. In 1972, he coauthored a paper that suggested how game theory could be used to trade the futures markets using quantitative systems.2 After turning his attention to trend following, he created Mint Investments in 1981 with two partners; it became the first hedge fund to manage \$1 billion and the first fund to partner with the Man Group, which effectively put Man into the hedge fund business. Mint annualized north of 30 percent per year, net of fees, for its investors over the 13 years it existed under Hite's stewardship. Notably, Mint made some 60 percent in 1987, in no small part by being on the right side of the crash that October.3

Lest it seem like this is an overly rosy picture of trend following, it should be stated clearly: These strategies come with a great deal of risk alongside their lofty returns. The typical successful trend follower earns less than one point of return for every point of downside risk delivered. In other words, to earn 50 percent per year, the investor must be prepared to suffer a loss greater than 50 percent at some point. In short, the returns of this strategy are streaky and highly variable.

This is not only true of trend following. Indeed, each of the major classes of alpha described in this chapter is subject to relatively long periods of poor returns. This is because the behaviors they seek to profit from in the markets are not ever‐present but rather are unstable and episodic. The idea is to make enough money in the good times and manage the downside well enough in the bad times to make the whole exercise worthwhile.

Perhaps quant trading's most important trend follower in terms of lasting impact was a firm called Axcom, which later became Renaissance Technologies. Elwyn Berlekamp, a PhD in engineering from MIT, in 1986 began to consult for Axcom regarding strategy development. Axcom had been struggling during those years, and Berlekamp bought a controlling interest. In 1989, after doing considerable research, Axcom resumed trading with a new and improved strategy. For its first year, the firm was up 55 percent after charging 5 percent management fees and 20 percent incentive fees. At the end of 1990, Berlekamp sold his interest to Jim Simons for a sixfold profit, which might still have been one of the worst trades in history. Renaissance, as the firm was called by then, is now the most successful quant trading firm and probably the most impressive trading firm of any kind. It has evolved a great deal from the trend‐following strategies it used in the mid‐1980s and even from the more sophisticated futures strategies it employed in the early 1990s. It stopped accepting new money with less than \$300 million under management in 1992 and went on to compound this money to approximately \$10 billion some 20 years later, despite eye‐popping 5 percent management fees and 44 percent incentive fees. They have annualized approximately 35 percent per year net of these fees, from 1989 onward, and perhaps most astonishingly, have gotten *better* over the years, despite the increased competition in the space and their own significantly larger capital base.4

It is worth pointing out that quants are not the only ones who have a fondness for trend‐following strategies. It has always been and will likely remain one of the more important ways in which traders of all stripes go about their business. One can find trend following in the roots of the infamous Dutch tulip mania in the seventeenth century, or in the dot‐com bubble of the late twentieth century, neither of which is likely to have been caused by quants. And, of course, many discretionary traders have a strong preference to buy what's been hot and sell what's been cold.

Mean Reversion When prices move, as we have already said, they move in either the same direction they've been going or in the opposite. We have just described trend following, which bets on the former. Now we turn our attention to mean reversion strategies, which bet on prices moving in the opposite direction to that which had been the prevailing trend.

The theory behind mean reversion strategies is that there exists a center of gravity around which prices fluctuate, and it is possible to identify both this center of gravity and what fluctuation is sufficient to warrant making a trade. As in the case of trend following, there are several valid rationales for existence of mean reversion. First, there are sometimes short‐term imbalances among buyers and sellers due simply to liquidity requirements that lead to an instrument being over‐bought or over‐sold. To return to the example mentioned earlier, imagine that a stock has been added to a well‐followed index, such as the S&P 500. This forces any fund that is attempting to track the index to run out and buy the stock, and, in the short term, there might not be enough sellers at the old price to accommodate them. Therefore, the price moves up somewhat abruptly, which increases the probability that the price will reverse again at some point, once the excess demand from index buyers has subsided. Another rationale to explain the existence of mean‐reverting behavior is that market participants are not all aware of each other's views and actions, and as they each place orders that drive a price toward its new equilibrium level, the price can overshoot due to excess supply or demand at any given time.

Regardless of the cause of the short‐term imbalance between supply and demand, mean reversion traders are frequently being paid to provide liquidity because they are bucking current trends. This is sometimes explicitly true in terms of their execution techniques (which we discuss in more detail in Chapters 7 and 14). But regardless of execution tactics, mean reversion traders are indeed betting against momentum, and bear the risk of adverse selection.

Interestingly, trend and mean reversion strategies are not necessarily at odds with each other. Longer‐term trends can occur, even as smaller oscillations around these trends occur in the shorter term. In fact, some quants use both of these strategies in conjunction. Mean reversion traders must identify the current mean or equilibrium and then must determine what amount of divergence from that equilibrium is sufficient to warrant a trade. As in the case of trend following, there are many ways of defining the mean and the reversal. It is worth noting that when discretionary traders implement mean reversion strategies, they are typically known as *contrarians*.

Perhaps the best‐known strategy based on the mean reversion concept is known as *statistical arbitrage* (*stat arb*, for short), which bets on the convergence of the prices of similar stocks whose prices have diverged. While Ed Thorp, founder of Princeton/Newport Partners was probably one of the earliest quantitative equity traders, the trading desk of Nunzio Tartaglia at Morgan Stanley was a pioneer of stat arb and would prove to have lasting impact on the world of finance. Tartaglia's team included scientists such as Gerry Bamberger and David Shaw, and together they developed and evolved a strategy that was based on the relative prices of similar stocks. Stat arb ushered in an important change in worldview, one that focused on whether Company A was over‐ or undervalued *relative to* Company B rather than whether Company A was simply cheap or expensive *in itself*. This important evolution would lead to the creation of many strategies based on forecasts of relative attractiveness, which is a topic we will address in greater detail shortly.

Exhibit 3.3 shows a simplified example of the mean‐reverting behavior evident between similar instruments, in this case Merrill Lynch (MER) and Charles Schwab (SCHW). As you can see, the spread between these two companies oscillates rather consistently in a reasonably narrow range for long periods. This effect allows a trader to wait for significant divergences and then bet on a reversion back to the equilibrium level.

Trend and mean reversion strategies represent a large portion of all quant trading. After all, price data are plentiful and always changing, presenting the quant with many opportunities to trade. It may be interesting to note that trend and mean reversion, though they are theoretically opposite ideas, both seem to work. How is this possible? Largely, it's possible because of different timeframes. It is obviously correct that both strategies can't possibly be made to be exactly opposite while both making money at the same time. However, there is no reason to create both strategies to be exactly the same. Trends tend to occur over longer time horizons, whereas reversions tend to happen over shorter‐term time horizons. Exhibit 3.4 shows this effect in action. You can see that there are indeed longer‐term trends and

![](_page_9_Figure_4.jpeg)

Exhibit 3.3 Mean Reversion between SCHW and MER

![](_page_10_Figure_1.jpeg)

**EXHIBIT 3.4** Trend and Reversion Coexisting

shorter-term mean reversions that take place. In fact, you can also see that the strategies are likely to work well in different regimes. From 2000 to 2002 and again in 2008, a trend strategy likely exhibits better performance, since the markets were trending very strongly during these periods. From 2003 to 2007, mean-reverting behavior was more prevalent. Yet both strategies are likely to have made money for the period as a whole. This can also be examined on other time horizons, and in some cases, mean reversion strategies can work as the longer-term indicator, while momentum can be used as a faster indicator.

**Technical Sentiment.** An interesting third class of price‐related strategies tracks investor sentiment—expressed through price, volume, and volatility behaviors—as an indicator of future returns. A word of caution before delving into these strategies is warranted. Unlike in the cases of momentum and mean reversion (or the fundamental strategies to be outlined later in this chapter), there is no clear economic rationale that gives birth to a strategy. In other words, there are widely varying views on the value and use of sentiment information in forecasting. To some practitioners, a high degree of positive sentiment in some instrument would indicate that the instrument is already overbought and therefore ready to decline. To others, high positive sentiment would indicate that the instrument has support to move higher. For still others, sentiment is only used as a conditioning variable (this concept will be discussed in more detail in "Conditioning Variables"), for example by utilizing a trend‐following strategy only if the volumes that were associated with the price movements were significant, whereas a low‐volume trend might be ignored. It is this last use of sentiment data that is most common. There are, however, several examples of technical sentiment strategies that can be thought of as standalone ways to forecast future direction.

First is to look at the options markets to determine sentiment on the underlying. There are two separate "straightforward" ideas to explore here. One is to look at the volume of puts and calls, and to use this as an indicator of sentiment. If puts have higher volumes relative to calls than they normally do, it might be an indicator that investors are worried about a downturn. If puts have lower volumes versus calls than normal, it might be a bullish sentiment indicator. A second example of options‐based sentiment in equities utilizes the implied volatilities of puts versus calls. It is natural to see some level of difference in the implied volatilities of puts versus calls. This is partially in recognition of the habit of stocks to move down quickly and up slowly, which would indicate that downside volatility is higher than upside volatility, which in turn causes the seller of a put option to demand a higher price (and therefore implied volatility) than would be demanded by the seller of a call option that is equally far out of the money (or in the case that they are both at the money). If one analyzes the historical ratio of put volatility and call volatility, there will likely be some natural ratio (greater than one, due to the phenomenon just described about upside and downside volatility), and divergences from this natural level might be treated as indicative of sentiment. A related idea would be to use implied volatility or a proxy (e.g., credit default swaps, or CDS for short) as an indicator of investor sentiment.

A second example of a technical sentiment strategy analyzes trading volume, open interest, or other related type of inputs as an indicator of future prices. At the shortest timeframes, some higher‐frequency traders evaluate the shape of the limit order book to determine near‐term sentiment. The shape of the order book includes factors such as the size of bids or offers away from the mid‐market relative to the size at the best bid/offer, or the aggregate size of bids versus offers. For slightly longer‐term strategies, analyses of volume can include looking at the trading volume, the turnover (trading versus float), open interest, or other similar measures of trading activity. As I mentioned at the outset of this section, what to do with this kind of information remains up for debate. It can be used as a contrarian indicator (i.e., high-volume or high-turnover stocks are expected to underperform, while low volume or low turnover stocks are expected to outperform) or as a positive indicator. Most of the research I have reviewed, however, focuses on the contrarian approach.

### Strategies Utilizing Fundamental Data

Most strategies utilizing fundamental data in their alpha models can be easily classified into one of three groups: value/yield, growth, or quality. Though these ideas are frequently associated with the analysis of equities, it turns out that one can apply the exact same logic to any kind of instrument. A bond, a currency, a commodity, an option, or a piece of real estate can be bought or sold because it offers attractive value, growth, or quality characteristics. While fundamentals have long been part of the discretionary trader's repertoire, quantitative fundamental strategies are relatively young.

In quantitative equity trading and in some forms of quantitative futures or macro trading, much is owed to Eugene Fama and Kenneth French (known better collectively as Fama‐French). In the early 1990s, they produced a series of papers that got quants thinking about the kinds of factors that quants frequently use in strategies utilizing fundamental data. In particular, "The Cross Section of Expected Stock Returns" coalesced more than a decade of prior work in the area of using quantitative fundamental factors to predict stock prices and advanced the field dramatically.5 Fama and French found, simply, that stocks' betas to the market are not sufficient to explain the differences in the returns of various stocks. Rather, combining betas with historical data about the book‐to‐price ratio and the market capitalization of the stocks was a better determinant of future returns. It is somewhat ironic that an entire domain of quantitative alpha trading owes so much to Eugene Fama, because Fama's most famous work advanced the idea that markets are efficient.

Value/Yield Value strategies are well known and are usually associated with equity trading, though such strategies can be used in other markets as well. There are many metrics that people use to describe value in various asset classes, but most of them end up being ratios of some fundamental factor versus the price of the instrument, such as the price‐to‐earnings (P/E) ratio. Quants tend to invert such ratios, keeping prices in the denominator. An inverted P/E ratio, or an E/P ratio, is also known as *earnings yield.* Note that investors have long done this with dividends, hence the *dividend yield,* another commonly used measure of value. The basic concept of value strategies is that the higher the yield, the cheaper the instrument. The benefit of the conversion of ratios to yields is that it allows for much easier and more consistent analysis.

Let's take earnings as an example: Earnings can (and frequently do) range from large negative numbers to large positive numbers and everywhere in between. If we take two stocks that are both priced at \$20, but one has \$1 of earnings while the other has \$2 of earnings, it's easy to see that the first has a 20 P/E and the second has a 10 P/E, so the second looks cheaper on this metric. But imagine instead that the first has –\$1 in earnings, whereas the second has –\$2 in earnings. Now, these stocks have P/Es of –20 and –10. Having a –20 P/E seems worse than having a –10 P/E, but it's clearly better to only have \$1 of negative earnings than \$2. Thus, using a P/E ratio is misleading in the case of negative earnings. In the case that a company happens to have produced exactly \$0 in earnings, the P/E ratio is simply undefined, since we would be dividing by \$0. Because ratios with price in the numerator and some fundamental figure in the denominator exhibit this sort of misbehavior, quants tend to use the inverted yield forms of these same ratios. This idea is demonstrated in Exhibit 3.5, which shows that the E/P ratio is well behaved for any level of earnings per share for a hypothetical stock with a price greater than \$1 (in the example, we used

![](_page_13_Figure_3.jpeg)

Exhibit 3.5 P/E versus E/P (Earnings Yield)

\$20 per share as the stock price). By contrast, the P/E ratio is rather poorly behaved and does not lend itself well to analysis and is not even properly defined when earnings per share are zero.

There is a bigger theme implied by the example of the treatment of earnings data by quants. Many fundamental quantities are computed or quoted in ways that are not readily used in developing a systematic alpha. These quantities predated the use of computers to trade, and as a result, can have arbitrary definitions and distributions. Quants must transform such data into more usable, well‐behaved variables that can lend themselves more readily to systematic trading applications.

Most often, value is thought of as a strategy that is defined by buying cheap. But this strikes me as being too shallow a definition. In reality, the idea behind value investing is that markets tend to overestimate the risk in risky instruments and possibly to underestimate the risk in less risky ones. Therefore, it can pay off to own the more risky asset and/or sell the less risky asset. The argument for this theory is that sometimes instruments have a higher yield than is justified by their fundamentals simply because the market is requiring a high yield for that kind of instrument at the moment. An investor who can purchase this instrument while it has a high yield can profit from the movement over time to a more efficient, fair price. As it happens, instruments don't usually become cheap solely because their prices don't move while their fundamentals improve drastically. Rather, prices are more often the determinant of value than changing fundamentals, and in the case of a cheap instrument, this implies that the instrument's price must have fallen substantially. So in some sense, the value investor is being paid to take on the risk of standing in the way of momentum. Ray Ball, a professor of accounting at the University of Chicago's Booth School of Business, wrote a paper, "Anomalies in Relationships Between Securities' Yields and Yield‐Surrogates," which echoes the idea that higher‐yielding stocks—those with higher earnings yields—are likely those for which investors expect to receive higher returns and greater risks.6

When done on a relative basis, that is, buying the undervalued security and selling the overvalued one against it, this strategy is also known as a *carry trade*. One receives a higher yield from the long position and finances this with the short position, on which a lower yield must be paid. The spread between the yield received and the yield paid is the *carry*. For instance, one could sell short \$1,000,000 of U.S. bonds and use the proceeds to buy \$1,000,000 of higher‐yielding Mexican bonds. Graham and Dodd, in their landmark book *Security Analysis*, propose that value trading offers investors a margin of safety. In many respects, this margin of safety can be seen clearly in the concept of carry. If nothing else happens, a carry trade offers an investor a baseline rate of return, which acts as the margin of safety Graham and Dodd were talking about.

Carry trading is an enormously popular kind of strategy for quants (and discretionary traders) in currencies, where the currency of a country with higher short‐term yields is purchased against a short position in the currency of a country with relatively low short‐term yields. For example, if the European Central Bank's target interest rate is set at 4.25 percent, whereas the U.S. Federal Reserve has set the Fed Funds rate at 2 percent, a carry trade would be to buy euros against the U.S. dollar. This is a classic value trade because the net yield is 2.25 percent (4.25 percent gained on the euro position, less 2 percent paid in U.S. interest), and this provides a margin of safety. If the trade doesn't work, the first 2.25 percent of the loss on it is eliminated by the positive carry. Similar strategies are employed in trading bonds. In fact, this was one of Long‐Term Capital Management's central trading ideas, until the firm imploded in 1998.

Note that, in currencies and in bonds, the connection between higher yields and higher risk is more widely understood than in equities. In other words, if some instrument has a higher yield than its peers, there may well be a good reason that investors demand this higher yield. The reason is usually that this instrument is more risky than its peers. This can naturally be seen in the juxtaposition of yields on government bonds, AAA‐rated corporate bonds, and various lower‐rated corporate bonds. As riskiness increases, so too do yields to compensate lenders.

Another important example of value trading is in equities, where many kinds of traders seek to define metrics of "cheapness," such as earnings before interest, taxes, depreciation, and amortization (EBITDA) versus enterprise value (EV) or book value to price. Book value per share versus price (*book yield* or *book‐to‐price*) is also a fairly common factor, as it has been among quants since Fama and French popularized it in their papers. Most quant equity traders who use value strategies are seeking relative value rather than simply making an assessment of whether a given stock is cheap or expensive. This strategy is commonly known as *quant long/short* (QLS). QLS traders tend to rank stocks according to their attractiveness based on various factors, such as value, and then buy the higher‐ranked stocks while selling short the lower‐ranked ones. For example, assume that we ranked the major integrated oil companies by the following hypothetical book‐to‐price ratios:

| Company                 | Book‐to‐Price Ratio (Hypothetical) |
|-------------------------|------------------------------------|
| Marathon Oil (MRO)      | 95.2%                              |
| ConocoPhillips (COP)    | 91.7%                              |
| Chevron Corp. (CVX)     | 65.4%                              |
| Exxon Mobil Corp. (XOM) | 33.9%                              |

According to this metric, the higher‐ranked stocks might be candidates for long positions, whereas the lower‐ranked might be candidates for short positions. The presumption is that a stock with a higher book‐to‐price ratio might outperform stocks with lower book‐to‐price ratios over the coming quarters.

Value can be used to time any kind of instrument for which valuations can be validly measured. This is easier in instruments such as individual equities, equity indices, currencies, and bonds. In the case of most commodities, value is usually thought of more to a "cheap/expensive" analysis, via concepts of the expected supplies of a commodity versus the expected demand for that commodity, rather than being focused on yield. There are classes of strategies in the futures markets (not specifically commodity futures, but most often in that group) that focus on yield explicitly as well. *Roll yield* is the spread between the price of a futures contract with some expiry date in the future, versus that of the spot (or that of the contract with a shorter‐dated expiry). In *backwardated* markets, spot prices are higher than futures contracts as they extend out into the calendar. Because there is a convergence of futures contracts up to the spot price, futures in this situation are considered to have positive roll yield. In *contango* markets, spot prices are lower than futures, and so the yield is considered negative.

Growth/Sentiment Growth strategies seek to make predictions based on the asset in question's expected or historically observed level of economic growth. Some examples of such ideas could be gross domestic product (GDP) growth or earnings growth. That a given stock is a growth asset implies nothing about its valuation or yield. The theory here is that, all else being equal, it is better to buy assets that are experiencing rapid economic growth and/or to sell assets that are experiencing slow or negative growth. Some growth metrics, such as the price/earnings‐to‐growth (PEG) ratio (PE ratio vs. EPS growth rate), are basically a forward‐looking concept of value; that is, they compare growth expectations to value expectations to see whether a given instrument is fairly pricing in the positive or negative growth that the trader believes the asset will likely experience. If you expect an asset to grow rapidly but the market has already priced the asset to account for that growth, there is no growth trade to be made. In fact, if the market has priced in a great deal more growth than you expect, it might even be reasonable to short the instrument. But certainly many forms of growth trading are simply focused on buying rapidly growing assets regardless of price and selling assets with stagnant or negative growth, even if they are very cheap (or offer high yields) already.

The justification for growth investing is that growth is typically experienced in a trending manner, and the strongest growers are typically becoming more dominant relative to their competitors. In the case of a company, you could see the case being made that a strong grower is quite likely to be in the process of winning market share from its weaker‐growing competitors. Growth investors try to be early in the process of identifying growth and, hence, early in capturing the implied increase in the future stature of a company. We can see examples of both macroeconomic growth strategies and microeconomic growth strategies in the quant trading world. At the macro level, some foreign exchange trading concepts are predicated on the idea that it is good to be long currencies of countries that are experiencing relatively strong growth, because it is likely that these will have higher relative interest rates in the future than weaker‐growth or recession economies, which makes this a sort of forward‐looking carry trade.

In the quant equity world, the QLS community frequently also utilizes signals relating to growth to help diversify their alpha models. Note that an important variant of growth trading utilized by a wide variety of quants and discretionary equity traders focuses on analysts' earnings estimate revisions (or other aspects of analyst sentiment, including price targets and recommendation levels). Sell‐side analysts working at various brokerage houses publish their estimates and release occasional reports about the companies they cover. The thesis is identical to any other growth strategy, but the idea is to try to get an early glimpse of a company's growth by using the analysts' expectations rather than simply waiting for the company itself to report its official earnings results. Because this strategy depends on the views of market analysts or economists, it is called a *sentiment‐based strategy*. The quant community does not universally agree that sentiment‐based strategies, such as the estimate revision idea just mentioned, are nothing more than variants of growth strategies, but it is my experience that these two are highly enough correlated in practice to warrant their being treated as close cousins. After all, too often Wall Street analysts' future estimates of growth look a lot like extrapolations of recent historical growth.

Quality The final kind of theory‐driven fundamental alpha is what I call *quality*. A quality investor believes that, all else being equal, it is better to own instruments that are of high quality and better to sell or be short instruments of poor quality. The justification for this strategy is that capital safety is important, and neither growth nor value strategies really capture this concept. A strategy focused on owning higher‐quality instruments may help protect an investor, particularly in a stressful market environment. Not coincidentally, these are frequently termed *flight‐to‐quality* environments. This kind of strategy is easily found in quant equity trading but not as commonly in macroeconomic types of quant trading, probably because, historically, countries were not thought of as being particularly risky. Given the unfolding crisis in Europe, we may begin to see quality models deployed in more macroeconomics‐oriented strategies.

I generally find that quality signals fall into one of five categories. First is *leverage*, which would indicate that, based on some measurement of leverage, one should short higher-levered companies and go long less-levered companies, all else equal. An example from the QLS world might look at the debt‐to‐equity ratios of stocks to help determine which ones to buy and sell, the idea being that less‐leveraged companies are considered higher quality than more‐leveraged companies, all else equal.

A second kind of quality signal is *diversity of revenue sources*, which would find those companies or countries with more diverse sources of potential growth to be of higher quality than those with fewer sources. So, all else equal, a company that makes money doing a wide variety of things for a variety of customers should be more stable than a company that makes exactly one kind of widget for some narrow purpose. A special case of this relates to the *volatility of revenues* (or, in the case of companies, profits). Here, taking the example of corporate earnings and stock prices, investors would prefer, all else equal, to own companies whose earnings are more stable (less volatile) relative to companies whose earnings are less stable (more volatile).

A third type of quality signal is *management quality*, which would tend to buy companies or countries that are led by better teams and sell those with worse teams. A great article in *Vanity Fair* relates to this very kind of signal. Entitled "Microsoft's Lost Decade," several key management missteps (according to the article's author) are highlighted as leading to Microsoft's fall from being the largest market capitalization company in the world to being a "barren wasteland." As you might expect, given the types of information involved, this is one of the more difficult types of signals to quantify. However, there are measures found, for example, in companies' financial statements, including changes in discretionary accruals (the idea being, the greater the increase in discretionary accruals, the more likely there are problems with the management's stewardship of the company).

A fourth type of quality strategy is *fraud risk*, which would buy companies or countries where the risk of fraud is low, and sell those where the risk is greater. An example of this kind of strategy from the QLS world is an *earnings quality* signal, which attempts to measure how close are a company's true economic earnings (as measured by, say, the free cash flow) to the reported earnings‐per‐share numbers. Such strategies especially gained prominence in the wake of the accounting scandals of 2001 and 2002 (Enron and WorldCom, for example), which highlighted that sometimes publicly traded companies are run by folks who are trying harder to manage their financial statements than manage their companies.

A final type of strategy relates to the *sentiment* investors have regarding the quality of the issuer of an instrument (again, this can be a company or a country). Generally, quality‐related sentiment strategies are focused on a forward‐looking assessment of the four quality categories above. In other words, one has a prospective view of changes in leverage, revenue diversity, management quality, or fraud risk. However, this type of strategy is not particularly common, as the signals would appear very sporadically, and because there are relatively few sources of sentiment regarding quality, it is also quite difficult to backtest and achieve any kind of statistical significance. In recent years, the growth of the CDS markets has provided a much more regularly available source of quality‐sentiment information. Some investors would also use implied volatility to serve this purpose, but implied volatilities go up because the market itself goes down, because growth expectations are lowered, because a company disappoints expectations on their earnings announcement, or for any number of other reasons unrelated to the quality of the company itself.

Quality's performance over time fluctuates greatly and is highly dependent on the market environment. In 2008, quality was a particularly successful factor in predicting the relative prices of banking stocks. In particular, some quality factors helped traders detect, avoid, and/or sell short those banks with the most leverage or the most exposure to mortgage‐related businesses, thereby allowing these traders to avoid or even profit from the 2008 credit crisis. The aforementioned accounting scandals in the early 2000s also would have been profitable for quality signals. However, for all that these strategies profit when things are very dire, they tend to do poorly when the markets are performing well, and terribly when the equity markets go into a state of euphoria.

We now have a summary of the ways that theory‐driven, alpha‐focused traders (including quants) can make money. To recap, price information can be used for trend or mean reversion strategies, whereas fundamental information can be used for yield (better known as *value*), growth, or quality strategies. This is a useful framework for understanding quant strategies but also for understanding all alpha‐seeking trading strategies. The framework proposed herein provides a menu of sorts, from which a particular quant may "order," creating his strategy. It is also a useful framework for quants themselves and can help them rationalize and group the signals they use into families. Quants sometimes fool themselves into thinking that there are a broader array of core alpha concepts than actually exist.

## Data-Driven Alpha Models

We now turn our attention to data‐driven strategies, which were not included in the taxonomy shown in Exhibit 3.1. These strategies are far less widely practiced for a variety of reasons, one of which is that they are significantly more difficult to understand and the mathematics are far more complicated. Data mining, when done well, is based on the premise that the data tell you what is likely to happen next, based on some patterns that are recognizable using certain analytical techniques. When used as alpha models, the inputs are usually sourced from exchanges (mostly prices), and these strategies typically seek to identify patterns that have some explanatory power about the future.

There are two advantages to these approaches. First, compared with theory‐driven strategies, data mining is considerably more technically challenging and far less widely practiced. This means that there are fewer competitors, which is helpful. Because theory‐driven strategies are usually easy to understand and the math involved in building the relevant models is usually not very advanced, the barriers to entry are naturally lower. Neither condition exists in the case of data‐driven strategies, which discourages entry into this space. Second, data‐driven strategies are able to discern behaviors whether they have been already named under the banner of some theory or not, which allows them to discover *that* something happens without having to understand *why*. By contrast, theory‐driven strategies capture the kinds of behavior that humans have identified and named already, which may limit them to the six categories described earlier in this section.

For example, many high‐frequency traders favor an entirely empirical, data‐mining approach when designing their short‐term trading strategies for equity, futures, and foreign exchange markets. These data‐mining strategies may be more successful in high frequency because, if designed well, they are able to discern how the market behaves without having to worry about the economic theory or rationalization behind this behavior. Since there is not much good literature at this time about the theoretical underpinnings of human and computerized trading behaviors at very short‐term time horizons (i.e., minutes or less), an empirical approach may actually be able to outperform a theoretical approach at this timescale. Furthermore, at this timescale there is so much more data to work with that the empirical researcher has a better chance of finding statistically significant results in his testing.

However, data‐mining strategies also have many shortcomings. The researcher must decide what data to feed the model. If he allows the model to use data that have little or no connection to what he is trying to forecast—for example, the historical phases of the moon for every day over the past 50 years as the input to a forecast of the price of the stock market he may find results that are seemingly significant but are in reality entirely spurious. Furthermore, if the researcher chooses the set of all data generally thought to be useful in predicting markets, the amount of searching the algorithms must conduct is so enormous as to be entirely impractical. To run a relatively thorough searching algorithm over, say, two years of intraday tick data, with a handful of inputs, might take a single computer processor about three months of continuous processing before it finds the combinations of data that have predictive power. If this was not difficult enough, whatever strategies are found in this manner require the past to look at least reasonably like the future, although the future doesn't tend to cooperate with this plan very often or for very long. To deal with this problem, the data‐mining strategy requires nearly constant adjustment to keep up with the changes going on in markets, an activity that has many risks in itself.

A second problem is that generating alphas using solely data‐mining algorithms is a somewhat dubious exercise. The inputs are noisy, containing a great number of false signals that act like traps for data miners. In general, strategies that use data‐mining techniques to forecast markets do not work, though there are a few exceptions.

In spite of (or, perhaps, because of) the aforementioned challenges facing data‐driven quant strategies, there are traders who implement them, and it is worthwhile to understand some of what goes into these types of models. Let's first frame the problem broadly. Data‐driven strategies look at the current market conditions, search for similar conditions in the historical data, and determine the probability that a type of outcome will occur afterwards. The model will choose to make a trade when the historical probabilities are in favor of doing so, and otherwise will not.

It also bears mentioning that, as much as data‐driven quant strategies are often mathematically more difficult to understand, even here there is an analog within the discretionary trading world. Technical analysts, also known as "chartists" because of their use of price and/or volume graphs to detect market patterns, are also looking for repeated patterns in market behavior that lead to predictable outcomes.

So, if data mining quants are primarily looking at current market conditions, searching the history for similar conditions, determining the probabilities of various outcomes in the aftermath of that setup, and making trades in accordance with the probabilities, they must, at a minimum, address several questions.

*What defines the "current market condition"?* Remember, with a quant trading strategy, there is no leeway to be vague. Telling one's computer to "find me situations in the past that look like the situation right now" isn't enough. One must specify precisely what "current" means and what "condition" means. In the case of "current," and not to get too philosophical about the concept of time, but the present can refer to an instantaneous moment, or the last few minutes, or the last 10 years. There is no standard, and the quant must determine her preference in this regard. So, even in this most empirical, data‐driven quant strategy, discretion is a key aspect of the creation of a strategy. In the case of "condition," do we mean merely some aspect of price behavior, or do volumes and/or fundamental characteristics matter also? This is not merely an academic question: It is easy to see that, whether one treats the price behavior of two small‐capitalization technology companies the same way as one treats the behavior of one of those companies versus that of a mega‐cap diversified financial firm is a matter of fundamental beliefs about how the market works.

*What is the search algorithm used to find "similar" patterns?* Hand in hand with this question is another: *What does "similar" mean?* And, also related: *By what method does the algorithm determine the probability of the outcome?* These are the least easily conceptualized and the most technical questions on the list. I can only say that choosing statistical techniques that are appropriate to the dataset is very obviously critical, and that the quant must be careful. One of the most common follies in quant trading is to apply a statistical tool to the wrong problem. There is a great deal of art and judgment that pertains to this decision, making it difficult to generalize a good answer to this question.

*How far into the past will the search be conducted?* A decidedly more straightforward question, conceptually, is how far into the past to look for similar patterns. The trade-off is simple, and it pervades quant research (and discretionary investment management). On the one hand, more recent data matters a lot, because it is the most relevant to the immediate present and near future. While it's debatable whether human behavior ever really changes, it's clear that technology, and therefore the way humans interact with one another, does evolve, and not only this, but it evolves faster as more time passes. Market structures, too, evolve. How relevant would data from the Buttonwood Tree era of the NYSE be to the current world of almost exclusively electronic exchanges? On the other hand, with data‐mining techniques applied to such noisy datasets as capital markets present, statistical significance is always of paramount importance. The greater the amount of data, the greater one's confidence is in the statistical conclusions drawn from the data, for most types of statistical tests. So, while the more recent past is more relevant, the more data, the merrier. The quant (and the investor examining the quant) must determine the appropriate balance between these conflicting traits of statistical analysis applied to systems with dynamic conditions.

## Implementing the Strategies

There are not many ways for alpha‐focused traders to make money, whether they are quants or not. But the limited selection of sources of alpha does not imply that all quants choose one of a handful of phenomena and then have a peer group to which they are substantively identical. There is in fact considerable diversity among alpha traders, far more so than may be evident at first glance.

This diversity stems from the way quants implement their strategies, and it is to this subject that we now turn our attention. There are many characteristics of an implementation approach that bear discussion, including the forecast target, time horizon, bet structure, investment universe, model specification, and run frequency.

#### Forecast Target

The first key component of implementation is to understand exactly what the model is trying to forecast. Models can forecast the direction, magnitude, and/or duration of a move and furthermore can include an assignment of confidence or probability for their forecasts. Many models forecast direction only, most notably the majority of trend‐following strategies in futures markets. They seek to predict whether an asset price will rise or fall, and nothing more. Still others have specific forecasts of the size of a move, either in the form of an expected return or a price target. Some models, though they are far less common, also seek to identify how long a move might take.

The *signal strength* is an important (but not ubiquitous) aspect of quant models. Signal strength is defined by an expected return and/or by confidence. The larger the expected return (i.e., the further the price target is from the current price), the greater the strength of the signal, holding confidence levels constant. Similarly, the more confidence in a signal, the greater the signal strength, holding expected returns constant. In general, though certainly not always, a higher level of signal strength results in a bigger bet being taken on a position. This is only rational. Imagine that you believe two stocks, Exxon Mobil (XOM) and Chevron (CVX), both will go up, but you have either a higher degree of confidence or a larger expected return in the forecast for XOM. It stands to reason that you will generally be willing to take a bigger bet on XOM than on CVX because XOM offers a more certain and/or larger potential return. The same holds for quant models, which generally give greater credence to a forecast made with a relatively high degree of confidence or large expected return. This concept can also influence the approach a strategy will take to executing the trades resulting from signals with varying strengths, which we will address in Chapter 7. However, the use of signal strength also bears some caution. Very large signals are unusual, and therefore there may be less statistical confidence that the relationship between the forecast and the outcome have the same relationship as is the case with smaller signals.

#### Time Horizon

The next key component to understanding implementation of the alpha model is time horizon. Some quant models try to forecast literally microseconds into the future; others attempt to predict behavior a year or more ahead. Most quant strategies have forecast horizons that fall in the range of a few days to several months. Notably, a strategy applied to the very short term can look quite different from the way it would if the exact same idea was applied to the very long term, as illustrated by Exhibit 3.6. As you can see, a "medium‐term" version of the moving‐average‐based trend‐following strategy would have been short the S&P 500 index during the entirety of April and May 2008 because of the downtrend in the markets that began in October 2007. By contrast, as shown in the lower graph in Exhibit 3.6, a shorter‐term version of the same strategy would have been *long* on the S&P for all but three days in mid‐April and for the last days of May. This exhibit illustrates that the same strategy, applied over different time horizons, can produce markedly different—even opposite—positions.

In general, there is more variability between the returns of a one‐ minute strategy and a one‐hour strategy than between a three‐month and a six‐month strategy, even though the interval between the latter pair is significantly longer than that between the first pair. Generalized, we find that *differentiation is greater at shorter timescales than at longer ones*. This general rule especially holds true in more risky environments. This happens because the shorter‐term strategies are making very large numbers of trades compared with the longer‐term versions of the same strategies. Even a small difference in the time horizon of a strategy, when it is being run at a short timescale, can be amplified across tens of thousands of trades per day and in the millions per year. By contrast, three‐ and six‐month versions of the same strategy are simply making a lot fewer trades, so the difference in time horizon does not get amplified. So, for example, a 150‐day moving average versus a 300‐day moving average trend‐following strategy would produce the exact same constant short position in the S&P 500 during April and May 2008 as the trend‐following strategy that uses 60‐ and 100‐day moving averages. By contrast, taking merely 10 days off of the longer moving average from the shorter‐term system so that it now uses 5‐ and 10‐day moving averages causes the system to be short the S&P for several extra days in mid‐April and to add another short trade in mid‐May that the 5‐/20‐ day version would not have done. Instead of being short the S&P for eight trading days out of the total of 43 during these two months, the 5‐/10‐day version would be short for 15 out of the 43 days.

The choice of time horizon is made from a spectrum with a literally infinite number of choices; that is, forecasts can be made for two weeks

![](_page_25_Figure_1.jpeg)

Exhibit 3.6 Same Strategy on Different Time Horizons

into the future, or for two weeks and 30 seconds, or for two weeks and 31 seconds, and so on. Yet adding 30 or 31 seconds to a forecast of two weeks might not cause a great deal of differentiation. Along this line of thinking, a classification may be helpful in understanding the distinctions among quant trading strategies by time horizon. *High‐frequency* strategies are the fastest, making forecasts that go no further than the end of the current trading day. *Short‐term* strategies, the second category, tend to hold positions from one day to two weeks. *Medium‐term* strategies make forecasts anywhere from a few weeks to a few months ahead. Finally, *long‐ term* strategies hold positions for several months or longer. The lines of demarcation between these groups are arbitrary, but in my experience, this shorthand can be helpful in thinking about how various quant strategies might compare with one another.

#### Bet Structure

The next key component of an alpha model is bet structure, which, in turn, is based on how the alpha model generates its forecast. Models can be made to forecast either an instrument in itself or an instrument relative to others. For example, a model could forecast that gold is cheap and its price is likely to rise or that gold is cheap *relative to* silver, and that gold is therefore likely to outperform silver. When looking at relative forecasts, one can forecast the behavior of smaller clusters (e.g., pairs) or larger clusters (e.g., sectors). Smaller clusters have the advantage of being easier to understand and analyze. In particular, pairs are primarily attractive because, in theory, one can carefully select instruments that are directly comparable.

However, pairs have several comparative disadvantages. Very few assets can actually be compared so precisely and directly with one other instrument, rendering a major benefit of pairs trading impracticable. Two Internet companies might each depend significantly on revenues from their respective search engines, but they may differ along other lines. One could have more of a content‐driven business while the other uses advertising to supplement the search engine revenues. Meanwhile, one could find other companies with strong advertising or content businesses, each of which shares some characteristics and sector‐effects with the first pair. Here the trader is presented with a dilemma: Which pairs are actually the best to use? Or to put it another way, how should the trader's pairs best be structured?

Another approach is to make relative bets within larger clusters or groups. Researchers group securities together primarily in an effort to isolate and eliminate common effects among the group. A large part of the point of grouping stocks within their market sector, for example, is to eliminate the impact of a general movement of the sector and thereby focus on the *relative* movement of stocks *within* the sector. It turns out to be extremely difficult to isolate group effects with a group size of merely two. On the other hand, larger clusters allow for a cleaner distinction between group behavior and idiosyncratic behavior, which is beneficial for many quant strategies. As a result, most quants who trade in groups tend to use larger groups than simply pairs when they make relative bets.

Researchers also must choose *how* they create these clusters, either using statistical techniques or using heuristics (e.g., fundamentally defined industry groups). There are many statistical techniques aimed at discerning when things are similar to each other or when they belong together as a group. However, statistical models can be fooled by the data, leading to bad groupings. For example, there may be periods during which the prices of Internet stocks behave like the price of corn. This may cause the statistical model to group them together, but Internet stocks and corn are ultimately more different than they are similar, and most fundamental grouping approaches would never put them together. Furthermore, any time that the market regime changes, the relationships among instruments frequently also change, which can lead the system to mistakenly group things together, even though they no longer will behave like each other.

Alternatively, groups can be defined heuristically. Asset classes, sectors, and industries are common examples of heuristically defined groups. They have the advantage of making sense and being defensible theoretically, but they are also imprecise (for instance, to what industry does a conglomerate such as General Electric belong?) and possibly too rigid. Rigidity in particular can be a problem because over time, similarities among instruments change. Sometimes stocks and bonds move in opposite directions, and sometimes they move in the same direction. Because the correlation between these two asset classes moves in phases, it can be very tricky to analyze the relationship theoretically and make a static, unchanging declaration that they belong in the same group or in different groups. As a result, most grouping techniques (and by extension, most strategies that are based on *relative* forecasts), whether statistically driven or heuristic, suffer from changes in market regime that cause drastic changes in the relationships among instruments.

In evaluating alpha‐oriented strategies, this distinction among bet structures, most notably between directional (single security) bets versus relative (multisecurity) bets, is rather important. The behavior of a given type of alpha model is very different if it is implemented on an instrument by itself than it would be if implemented on a group of instruments relative to each other. It is critical to balance the risks and benefits of the various approaches to grouping. In general, relative alpha strategies tend to exhibit smoother returns during normal times than intrinsic alpha strategies, but they can also experience unique problems related to incorrect groupings during stressful periods. Some quants attempt to mitigate the problems associated with any particular grouping technique by utilizing several grouping techniques in concert. For example, one could first group stocks by their sectors but then refine these groupings using a more dynamic statistical approach that reflects recent correlations among the stocks.

Also, it is worth clarifying one piece of particularly unhelpful, but widely used, hedge fund industry jargon: *relative value*. This term refers to strategies that utilize a relative bet structure, but the *value* part of the term is actually not useful. Certainly strategies that make forecasts based on a notion of the relative valuation of instruments are quite common. However, most strategies called relative value have little to do with value investing. Relative mean reversion strategies, relative momentum strategies, and other kinds of relative fundamental strategies are all commonly referred to as relative value.

#### Investment Universe

A given strategy can be implemented in a variety of instruments, and the quant must choose which ones to include or exclude. The first significant choice a quant makes about the investment universe is *geography*. A short‐term relative mean reversion strategy traded on stocks in the United States might not behave similarly to the same strategy applied to stocks in Hong Kong. The researcher must decide where to apply the strategy. The second significant choice a quant makes about the investment universe relates to its *asset class*. A growth strategy applied to foreign exchange markets might behave differently than one applied to equity indices. The quant must decide what asset classes to trade with each strategy. A third significant choice a quant must make about the investment universe relates to the *instrument class*. Equity indices, as accessed through the futures markets, behave differently than single stocks, even though both belong to the equity asset class. Also, the liquidity characteristics and nature of the other participants in a given market differ from one instrument class to another, and these are some of the considerations quants must make regarding what kinds of instruments to trade. There are also tax implications to consider. Finally, in some cases, quants may include or exclude specific groups of instruments for a variety of reasons.

The choice of an investment universe is dependent on several strong preferences that quants tend to have. First, the quant generally prefers liquidity in the underlying instruments so that estimations of transaction costs are reliable. Second, quants generally require large quantities of high‐ quality data. In general, such data can be found in highly liquid and developed markets. Third, quants tend to prefer instruments that behave in a manner conducive to being predicted by systematic models. Returning to the example of biotechnology stocks, some quants exclude them because they are subject to sudden, violent price changes based on events such as government approval or rejection of their latest drug. Although physicians with a biotech specialization may have some intuitions on this subject, it's simply not something that most quants can model. As a result of these preferences, the most typical asset classes and instruments in which one can find quants participating are common stocks, futures (especially on bonds and equity indices), and foreign exchange markets. Some strategies might trade the fixed‐income asset class using instruments other than futures (e.g., swaps or cash bonds), though these are significantly less common today than they were in the middle or late 1990s. Geographically, the bulk of quant trading occurs in the United States, developed Europe, and Japan, with lesser amounts done in other parts of North America and developed Asia. Quants are almost completely absent from illiquid instruments, or those traded over the counter (OTC), such as corporate or convertible bonds, and are less (but increasingly) common in emerging markets.

This last fact may change going forward as OTC markets become better regulated and electronic. But that also implies that the liquidity of these markets will improve. As such, this notion of liquidity is perhaps the simplest way to summarize in one dimension the salient characteristics of the trading universe for a strategy. After all, more liquid instruments also tend to offer more high‐quality data and to be more conducive to being forecast, on average.

#### Model Definition

An idea for a trading strategy, its core concept, is insufficient for use as a trading strategy: The quant must specifically define every aspect of the strategy before it is usable. Furthermore, any differences in the way a quant chooses to specify or define an idea for her strategy might lead it to behave quite differently from the way other choices would have. For example, there could be multiple ways to define a trend. Some simply compute the total return of an instrument over some historical period, and if that number is positive, a positive trend is identified (a negative return would constitute a negative trend). Other trend traders use moving average approaches, such as the ones illustrated in Exhibits 3.1, 3.3, and 3.4, to look for prices to rise above or below recent average prices and so determine the presence of a trend. Still other trend strategies seek to identify the breakout of the very early stages of a trend, found using specific price patterns they believe are present in this critical phase, but they do not attempt to determine whether a long‐term trend is actually in place or not.

These are but a few of the more common ways a trend can be defined. Just so, each kind of alpha strategy can be defined in various ways, and it is a significant part of the quant's job to decide precisely how to specify the strategy mathematically. This is an area for an investor in quant trading to study carefully because it is often a source of differentiation—and potentially of comparative advantage—for a quant. In the "Time Horizon" section of this chapter, we saw that even a specification about the time horizon of a strategy for timing the stock market can have a dramatic impact on whether it is long or short at a given point in time. Given the importance of time horizon, it is easy to understand the impact of using an entirely different definition of the strategy on its behavior. However, it may be challenging to get a quant to share with an outsider details on exactly how his model is specified. For the nonquant, then, model specification may remain a more opaque aspect of the black box, but exploring this idea as much as possible with a quant trader could, in fact, highlight the reasons for differences in performance that are observed versus the quant's peer group.

One especially important type of specification is in the form of setting parameters for a model. Returning to our trend example, the number of days in each moving average (e.g., a 5‐/10‐day moving average crossover strategy versus a 5‐/20‐day moving average crossover strategy) is a parameter. The specification of parameters is also an area in which some quants utilize machine learning or data‐mining techniques. In the section "Data‐ Driven Alpha Models," we mentioned the idea of fitting models to the data and setting parameter values. This is a problem to which machine learning techniques, which I described earlier as being neither easily nor commonly applied to the problem of finding alpha, are better suited and more widely used. In essence, machine learning techniques are applied to determine the optimal set of specifications for a quant model. Machine learning algorithms are designed to provide an intelligent and scientifically valid way of testing many potential sets of specifications without overfitting.

A subset of the problem of determining parameters relates to how often the models themselves are adjusted for more recent data. This process is known as *refitting* because some of the same work that goes on in the original research process is repeated in live trading in an attempt to refresh the model and make it as adaptive as possible to current market conditions. Because this can be a computationally intensive process, sometimes involving millions or even billions of calculations, many quants refit their models infrequently or not at all. Refitting also leads to a greater risk of overfitting, a very treacherous problem indeed, since spurious and fleeting relationships may be mistaken for valid, lasting ones.

#### Conditioning Variables

Many strategists (those whose job is to create trading strategies) employ conditioning variables to their strategies. These make the strategies more complex, but they also may increase the efficacy of the forecasts generated. There are two basic types of conditioning variables. One kind is a *modifying conditioner*, which takes a given signal and changes whether or how it is used, generally based on characteristics of the signal itself or its results. For example, a strategist may find that utilizing a simple trend indicator, for example, is not a sufficiently interesting strategy to pursue. After all, there are many false starts to worry about with a trend‐following strategy, and many experienced practitioners will admit that, without the "money management" or "risk management" rules they employ, their strategies would be basically uninvestable. These rules, and others to be discussed, can properly be thought of as conditioning variables for the trend‐following strategy.

For example, a stop‐loss is a common conditioning variable to pair with a trend‐following strategy. The idea would be to follow the trend, *unless* that trend has been reversing and causing losses to the position sufficient to trigger a stop‐loss. There are numerous kinds of stops: stop‐losses, profit‐targets (or profit‐stops), and time stops. Stop‐losses have already been described, and are generally employed when strategies have many "false" signals, but where the "good" signals can yield significant profits. Just so, most directional trend‐following strategies make money on a minority of their trades (often less than 40 percent of them!), but the gain on their winners is substantially larger than the losses on their losers (because of stop‐loss techniques).

Profit‐targets are utilized when the strategist believes that the position gets riskier as it generates profits. This is a reasonable enough concept: Markets rarely go in the same direction indefinitely, so it may make sense to take profits if they've been going the same way long enough for the strategy to generate significant profits. Finally, time‐stops are utilized to avoid the problem of holding positions on the basis of signals that may have been triggered far enough in the past as to be considered stale. It's a way of enforcing a refreshing of the bets being taken in the portfolio, among other things.

A second type of conditioning variable is a *secondary conditioner*, which requires the agreement (or some other set of conditions) across multiple types of signals to trigger a tradable forecast. For example, a large portion of fundamental equity analysts are "GARP" devotees, meaning they believe in owning "Growth at a Reasonable Price." If a company is identified as being both growing and inexpensive, it is a candidate to buy. Cheapness on its own would not justify a purchase, just as growth on its own would not. In price‐driven strategies, sometimes trend at various timescales, or trend and mean reversion, are combined. For example, a mean reversion strategy could be conditioned to buy instruments that have experienced price declines, but only when that causes the resulting position to be in the direction of the longer‐term trend (i.e., this strategy would buy dips in up‐trending markets, or short sell run‐ups in down‐trending markets).

Utilizing conditioning variables is how most rule‐based pattern recognition strategies are designed. Like data‐driven strategies, they are looking for repeated patterns in market behavior (basically, more complex patterns than "buy winners/sell losers" or "buy dips/sell run‐ups"), but theory‐driven pattern‐recognition models will begin with predefined rules. Data‐driven traders rely on their algorithms to determine what a "pattern" is in the first place (though, again, within the bounds specified, as discussed in "Data‐Driven Alpha Models").

#### Run Frequency

A final component of building a given alpha model is determining the *run frequency*, or the frequency with which the model is actually run to seek new trading ideas. Some quants run their models relatively infrequently—for example, once per month. At the other extreme, some run their models more or less continuously, in real time. Quants must manage an interesting tradeoff here. Specifically, increasing the frequency of model runs usually leads to a greater number of transactions, which means more commissions paid to brokers and higher transaction costs. Also, more frequent model runs lead to a greater probability that the model is moving the portfolio around based on noisy data that don't actually contain much meaning. This, in turn, would mean that the increased transaction costs would cause little or no incremental improvement in the alpha generated by the strategy and would thereby reduce its overall profitability.

On the other hand, less frequent model runs lead to a smaller number of larger‐sized trades. These are expensive in a different way, namely in terms of the impact these trades can have on the marketplace. If models are run too infrequently, then at those times when they are run they could recommend making very significant changes to the currently held portfolio. This would mean transacting larger blocks of trades, which would likely cost more in terms of moving the market. Less frequent model runs are also prone to problems associated with the moment of observation of markets. If a strategy is run once a month, it could miss opportunities to trade at more favorable prices that occur during the month while the model is dormant. Alternatively, the model may attempt in vain to trade at attractive, but quickly fleeting, prices that occur if there has been some aberration just around the time of the model being run.

Whether more frequent or less frequent model runs are better depends on many other aspects of the strategy, most especially the time horizon of the forecast and the kinds of inputs. In the end, most quants run their models no less than once a week, and many run continuously throughout the day. The slower‐moving the strategy, obviously, the more leeway there is, whereas shorter‐term strategies tend toward continuous, real‐time runs.

#### An Explosion of Diversity

We have described a few of the kinds of important decisions that quants must make in building a given alpha model. To succeed in quant trading, each of these decisions requires good judgment on the part of the quant. In short, successful quants are characterized in part by an incredible attention to detail and tirelessness in seeking the right questions to ask and the best solutions to address them. Nevertheless, for those who do not build quant trading systems but who are interested in understanding them, the kinds of issues discussed in this section are straightforward to understand and provide a useful way to distinguish one quant from another.

A final, important implication of these details of implementation is that they lead to an explosion in the variety of quant trading strategies that actually exist. You can easily see that the number of permutations of a strategy focused on the concept of value, for example, is enormous when accounting for differences in the type, time horizon, bet structure, investable universe, model definition, conditioning variables, and frequency of model run. Just taking the first four types of implementation details listed here and using the simplifying categories we described in this section, there are two types of forecasts (direction and magnitude), four types of time horizon (high frequency, short term, medium term, and long term), two types of bet structures (intrinsic and relative), and four asset classes (stocks, bonds, currencies, and commodities). Therefore one could build 64 different value models (2 × 4 × 2 × 4 = 64 permutations), and this excludes the question of how many ways one can define the idea of value, how one could condition the use of value on other variables, and how often one can look for value. This diversity might seem daunting at first glance, but the framework established here can help anyone interested in understanding what's inside a black box. Exhibit 3.7 revisits the taxonomy of alpha models, expanding it to include the implementation approaches discussed here.

## Blending Alpha Models

Each of the decisions a quant makes in defining a trading strategy is an important driver of its behavior. But there is another extremely important set of choices the quant must make in constructing a trading strategy. Specifically, the quant is not limited to choosing just one approach to a given alpha model. Instead, he is equally free to choose to employ *multiple* types of alpha models. The method used to combine these alpha models is an arena rich with possibilities. The most sophisticated and successful quants tend to utilize several kinds of alpha strategies, including trend and reversion, and

![](_page_34_Figure_1.jpeg)

Exhibit 3.7 Taxonomy of Theory‐Driven Alpha Models

various kinds of fundamental approaches across a variety of time horizons, trade structures, instruments, and geographies. Such quants benefit from alpha diversification in exactly the same way that diversification is helpful in so many other aspects of financial life.

Blending or mixing alpha signals has many analogues in discretionary trading (and decision making) in general. Imagine a mutual fund portfolio manager who has two analysts covering XOM. One analyst, focused on fundamental value in the classic Graham and Dodd sense, expects XOM to rise by 50 percent over the next year. The other analyst, taking a momentum approach, thinks XOM is likely to be flat over the next year. What is the net expectation the portfolio manager should have of the price of XOM, given the two analysts' predictions? This is the core problem that is addressed by blending alpha models, each of which can be likened to an analyst.

The three most common quant approaches to blending forecasts are via linear models, nonlinear models, and machine learning models. A significant fourth school of thought believes that alpha models should not be combined at all. Instead, several portfolios are constructed, each based on the output from a given alpha model. These factor portfolios are then combined using any of the portfolio construction techniques discussed in Chapter 7.

Each of these four approaches to signal mixing has its disciples, and as with most everything else we've discussed, the best way to blend alphas depends on the model. In general, as in the case of an alpha model, the purpose of a method of mixing alpha models is to find the combination of them that best predicts the future. All other things being equal, it is very likely that any reasonably intelligent combination of alphas will do a better job together than any one of them could do individually over time. Consider Exhibit 3.8. Here we can see that Forecasts A and B each occasionally predict future events correctly. This is illustrated in that there is some overlap between Forecast A and the actual outcome and between Forecast B and the actual outcome. But each forecast has only a small amount of success in predicting the future. However, together, Forecasts A and B are about twice as likely to be correct about the future outcomes as either is separately.

Linear models are by far the most common way in which quants combine alpha factors to construct a composite forecast. A linear model is a reasonable facsimile for one of the more common ways that humans normally think about cause‐and‐effect relationships. In linear models, the inclusion of one factor is independent of the inclusion of other factors, and each factor is expected to be additive, independently of the other factors that might be

![](_page_35_Figure_5.jpeg)

Exhibit 3.8 A Visualization of Multiple Forecasts

included or excluded. For example, for a high school student trying to get into a good university, she can think of her grades, standardized test scores, extracurricular activities, recommendations, and essays as being these independent factors in the linear model that predicts her odds of gaining admission. Regardless of the other factors, grades are always important, as is each other factor. As such, a linear model is relevant. If, on the other hand, it was the case that, with high enough test scores, her essays wouldn't matter, a linear model is no longer the correct way to predict her chances of getting in.

The first step in using a linear model in this way is to assign a weight to each alpha factor. To return to our example, if we were trying to predict university admissions, this step would require us to define the relative importance of grades versus, say, test scores. This is typically done using a technique known as *multiple regression*, which is aimed at finding the combination of alpha factors that explains the maximum amount of the historical behavior of the instruments being traded. The presumption is that, if a model reasonably explains the past, it has a reasonable chance of explaining the future well enough to make a profit. These weights are then applied to the outputs of their respective alpha factors, which are usually a forecast or score of some kind. The weighted sum of these multiple forecasts gives us a combined forecast. Or, to be more specific, by summing the products of the weights of each factor and the outputs of each factor, we arrive at a composite forecast or score. This composite can then be used to help determine the target portfolio.

Imagine a trading system with two alpha factors. One of the alpha factors focuses on E/P ratios (and is therefore a yield model), and the other focuses on price trends (and is therefore a trend model). The yield factor forecasts a return of +20 percent over the next 12 months for XOM, whereas the trend factor forecasts a return of –10 percent for XOM over the next 12 months. Based on a historical regression, the models are weighted 70 percent toward the yield factor and 30 percent toward the trend factor. Taking their scores and weights together, the total 12‐month return forecast of our two‐factor model is computed as follows:

70% weight × 20% return forecast for the yield factor comes to +14%.

30% weight × -10% return forecast for the trend factor comes to -3%.

The sum of these two products comes to +11 percent, which is the total expected 12‐month return for XOM using the example above.

A special case of linear models is the equal‐weighted model. Though not highly quantitative, equal‐weighting methods abound among quant traders. The general idea behind equal weighting is that the trader has no confidence in his ability to define more accurate weights and therefore decides to give all the alpha factors equal importance. A variant of this approach gives each factor an "equal risk" weighting, which incorporates the concept that giving a dollar to a highly risky strategy is not the same as giving a dollar to a less risky strategy. In Chapter 6, we cover both these approaches in more detail as they apply to portfolio construction. Still another approach would be to give each factor its own weight discretionarily.

There are many forms of nonlinear models that can be used to combine alpha factors with each other. In contrast to linear models, nonlinear models are based on the premise that the relationship between the variables used to make forecasts either is not independent (i.e., each variable is not expected to add value independently of the others) or else the relationship changes over time. As such, the two main types of nonlinear models are conditional models and rotation models. Conditional models base the weight of one alpha factor on the reading of another factor.

Using the same two factors as earlier, a conditional model might indicate that E/P yields should drive forecasts, but *only* when the price trends are *in agreement* with the E/P yields. In other words, the highest‐yielding stocks would be candidates to be bought only if the price trends of these stocks were also positive. The lowest‐yielding stocks would be candidates to be sold short, but only if the price trends of these stocks were also negative. When the agreement condition is met, the yield factor entirely drives the forecast. But if the price trend doesn't confirm the E/P yield signal, the yield signal is ignored entirely.

Revisiting the linear factor combination demonstrated earlier, our conditional model would generate no signal for XOM because the price trend forecast a negative return, whereas the yield factor forecast a positive return. If, instead, XOM had a positive return forecast from the trend factor, the combined nonlinear model would have a targeted return of +20 percent over the next 12 months for that stock because this is the return expected by the value factor, which now has been "activated" by its agreement with the trend factor. Note that mixing models in this way is similar to utilizing more conditioning variables in the specification of an alpha model (discussed in "Conditioning Variables"), though it is not required that the conditional linear model be an "all or none" type of approach. It is possible to utilize a conditioning variable that simply increases or decreases the weight of a given factor based on the values of other factors at that point in time. An example of a conditional model is shown in Exhibit 3.9.

Another type of conditional model for assigning weights to various forecasts is to consider variables external to those forecasts as the drivers of weights. For example, some practitioners believe that stat arb strategies perform better when market volatility is at elevated levels, and when stocks'

| Value and Momentum Disagree | Value | Momentum | Signal |
|-----------------------------|-------|----------|--------|
|                             | Long  | Short    | None   |
| Value and Momentum Agree    | Value | Momentum | Signal |
|                             | Long  | Long     | Long   |

Exhibit 3.9 A Simple Conditional (Nonlinear) Model for Blending Alphas

correlations to each other are at relatively low levels. Thus, a firm trading both mean reversion stat arb and directional trend strategies might overweight the stat arb component when volatility is high and correlations are low. When the opposite is true in both cases, perhaps they overweight the trend strategies. And at other times, perhaps they equally weight both strategies.

The second nonlinear way to blend alphas uses a rotation approach. Rather than following trends in markets themselves, this type of model follows trends in the performance of the alpha models. These are similar to linear models except that the weights of factors fluctuate over time based on updated calculations of the various signals' weights. As time passes, the more recent data are used to determine weighting schemes in the hope that the model's weights are more relevant to current market conditions. This method usually results in giving higher weights to the factors that have performed better recently. As such, this is a form of trend following in the timing of alpha factors.

Machine learning models are also sometimes used by quants to determine the optimal weights of various alpha factors. As in the case of determining optimal parameters, machine learning techniques applied to the mixing of alpha factors are both more common and more successful than machine learning approaches used to forecast markets themselves. These techniques algorithmically determine the mix of alpha factors that best explains the past, with the presumption that a good mix in the past is likely to be a good mix in the future. As in the case of rotational models, many machine learning approaches to mixing alpha factors periodically update the optimal weights based on the ever‐changing and ever‐growing set of data available. Unlike the example of using machine learning for the generation of actual alpha signals, applying machine learning to determine the weights of various alpha forecasts is more common and significantly more successful. Nevertheless, machine learning remains less widely used than the other techniques for blending alphas described here, and only a relatively small proportion of the universe of quant traders employs these methods.

We have briefly summarized common approaches to *mixing signals,* or combining alpha forecasts. This is a part of the quant trading process that has received precious little attention in the academic literature and trade press, but personally I find it one of the most fascinating questions about quant trading—or any trading. It is exactly the same problem any decision maker faces when looking at a variety of sources of information and opinions: What is the best way to synthesize all available and relevant information into a sensible decision?

It is worth noting that signal mixing shares some similarities with portfolio construction. Both are questions of sizing and combining, after all. However, they are mostly distinct and separate processes. Signal‐mixing models size multiple alpha signals to arrive at one composite forecast per instrument, which is then used in portfolio construction. Portfolio construction models take multiple kinds of signals as inputs, including alpha signals, risk models, and transaction cost models (which we cover in the next two chapters), and attempt to size individual positions correctly, given these inputs.

## Summary

In his excellent book *The Signal and the Noise*, Nate Silver presents a somewhat different—though complementary and related—type of distinction between approaches to forecasting.7 Rather than being a distinction based on the type of science one performs, as laid out in this chapter, it is a distinction based on the branch of statistics to which one subscribes.

One major branch of statistics, now popularly known as Bayesian statistics (after Thomas Bayes), placed a strong emphasis on the role of a "prior" in forecasting. A prior in this case refers to a belief held by the forecaster. For example, imagine that you have heard your spouse (with whom your relationship is generally good) make what seems like an insulting statement in reference to you. But what is the probability that this statement was, in fact, an insult? Bayes would advise that you begin your analysis of this statement by considering what the chances are that your spouse would insult you, before you heard this particular comment. Those odds are probably low, in particular if the relationship is fine to begin with. Let's say that this probability is 10 percent. Next, we would be advised to account for the probability that the statement we heard was not insulting, given that your partner is unlikely to insult you. If the statement sounded really negative (for example, "my husband is an idiot"), this, too, might be unlikely. Let's place these odds also at 10 percent. Finally, one should account for the possibility that you heard your partner make this statement because she is, in fact, insulting you. If she thought you weren't within earshot, perhaps the odds are higher. If she sees you directly in front of her, the odds are naturally expected to be lower. For this example, let's assume that your partner thinks you are not home. This makes the odds that this statement was uttered by your partner as an insult, say, 65 percent.

Accounting for all of this information, Bayes' Theorem would tell you that the odds you were in fact insulted, given your priors, are just under 42 percent.8 This is a pleasingly intuitive result: Your dear spouse is not expected to insult you. However, her statement was quite negative, and it was made in a context that increases one's suspicion of it being meant badly. The resulting odds are lower than they would have been without the prior that your partner is unlikely to insult you to begin with, but much higher than if you hadn't heard this statement under dubious circumstances.

Readers may note the philosophical kinship between a prior and a theory. This is more than a coincidence: The two concepts are closely related. The basic idea behind Bayesian forecasting methods is to allow new information to change a prior. The more that this new information is surprising (i.e., in conflict with the prior), the more we move away from our prior.

Another major branch of statistics is known as frequentist statistics. Frequentist statistics calls for the data to inform us about probabilities and confidence intervals around those probabilities. For example, if the data indicate that a stock that has an Earnings‐to‐Price ratio of 0.03 or lower (i.e., a P/E ratio of 33.33 or higher, or which is negative) has historically declined by 10 percent, with a confidence interval of six percent over the 12 months after this ratio was observed, then you have what you need for your forecast of any specific stock.

Again, there is a philosophical link between this idea and the data‐driven approach described in this chapter. Furthermore, there is even a similarity in the ways in which these distinctions can be shown to be imperfect. Many theory‐driven scientists are big believers in looking at the data, and in allowing the data to drive decisions such as parameter values in a model. Just so, many self‐described Bayesians can be found using frequentist techniques. In fact, in his fascinating review of Silver's book, Larry Wasserman of Carnegie Mellon University makes the point that Silver himself relies heavily on frequentist approaches to ascertaining the efficacy of a model—so much so that Wasserman calls Silver a frequentist.9

Having made so many decisions about what approach to forecasting one should utilize, what sort of alpha should be pursued, how to specify and implement it, and how to combine this alpha with others, the quant is left with an output. The output is typically either a return forecast (expected return = *X* percent) or a directional forecast (expected direction = up, down, or flat). Sometimes quants add elements of time (expected return over the next *Y* days) and/or probability (*Z* percent likelihood of expected return) to help utilize the output effectively in trading decisions. See Exhibit 3.10

![](_page_41_Figure_1.jpeg)

Exhibit 3.10 Schematic of the Black Box

for a recap of the structure of a quant trading system. As we continue our progress through the black box, we will highlight the components discussed.

I am consistently amazed by the juxtaposition of the simplicity and relatively small number of concepts used to manage money quantitatively and the incredible diversity of quant trading strategies as applied in the real world. The decisions quants make in the areas discussed in this chapter are major sources of the significant differences in the returns of traders who may be pursuing the same sources of alpha. Those evaluating quant traders (or quants who are evaluating trading strategies of their own) can use the framework provided in this chapter to help determine the nature of the strategies being traded. We now turn our attention to risk modeling, another key component of a quant trading strategy.

## [Notes](http://www.turtletrader.com/trader%E2%80%90seykota.html)

- 1. www.turtletrader.com/trader‐seykota.html.
- 2. Larry Hite and Steven Feldman, "Game Theory Applications," *Commodity Journal* (May–June 1972).
- 3. Ginger Szala, "Making a Mint: How a Scientist, Statistician and Businessman Mixed," *Futures*, March 1, 1989.
- 4. Gregory Zuckerman, "Renaissance Man: James Simons Does the Math on Fund," *Wall Street Journal*, July 1, 2005.
- 5. Eugene Fama and Kenneth French, "The Cross Section of Expected Stock Returns," *Journal of Finance* 47 (June 1992): 427.