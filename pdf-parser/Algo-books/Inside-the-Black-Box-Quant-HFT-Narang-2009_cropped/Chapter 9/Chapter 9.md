# CHAPTER

# Research

*Everything should be made as simple as possible, but not simpler.* -Albert Einstein

**D** esearch is the heart of quant trading. It is in large part because of well-designed, rigorous, and tireless research programs that the best quants earn their laurels. This chapter gives an overview of what research really means for black-box traders. It focuses mostly on research targeted at developing the alpha models of trading strategies. Research is also done with regard to risk models, transaction cost models, portfolio construction models, execution algorithms, and monitoring tools. Relevant research topics in these other areas will be mentioned as necessary, but the general principles from this section hold true throughout the black box.

The purpose of research is to scrutinize a well-conceived investment strategy. A strategy is a long-term course of action designed to achieve an objective, usually success or victory. In most applied settings, strategies are chosen from a limitless number of alternatives. One can find interesting examples in nearly every field: curing cancer, a baseball game, a war, a court case, or financial planning. In each case, one has many choices of strategy; so how is one chosen? In the case of quant trading, a strategy is chosen based on research, which has its roots in the natural sciences.

# BLUEPRINT FOR RESEARCH: THE SCIENTIFIC METHOD

A characteristic shared among well-behaved quants is their adherence to the *scientific method* in conducting research, which is of course the way science is done in every other field of study. This is critical because it forces rigor and discipline into the single most judgment-driven portion of the entire

quant trading process. Without such rigor, quants could easily be led astray by wishful thinking and emotion rather than the logic and consistency that make scientists useful to the world in so many other disciplines.

The scientific method begins with the scientist observing something in the world that might be explainable. Put differently, the scientist sees a pattern in her observations. For example, in most circumstances, if something is above the ground and is left unsupported, it falls toward the ground. Second, the scientist forms a theory to explain the observations. Sticking with the same theme in our examples, the scientist can theorize that there is something inherent in all things that causes them to move toward each other. This is better known as the *theory of gravity*. Third, the scientist must deduce consequences of the theory. If gravity exists, the orbits of planets should be predictable using the consequences of the theory of gravity. Fourth comes the all‐important testing of the theory. But rather than looking to "prove" a theory, properly done science seeks to find the *opposite* of the consequences deduced, which would therefore *disprove* the theory. In the case of gravity, Newton's theory was used to predict the existence of Neptune, based on motions in the orbit of Uranus that could not be explained by other then‐ known celestial bodies. But this success could at best provide support for Newton's theory and could never actually prove it. Karl Popper, the eminent philosopher of science, labeled this technique *falsification*. A theory that has not yet been disproved can be accepted as true for the moment. But we can never be certain that the next observation we make of the theory will not falsify it. Newton's theory of gravity was never "proved" and in fact was superseded by Einstein's general relativity theory. The latter also has not been proven, and alternatives have been proposed to help explain problems (such as the accelerating expansion of the universe or the unexpectedly high velocities of stars in the outskirts of galaxies) that neither Newton's laws nor Einstein's relativity address in their current form.

Looking at the markets, it is easy to see the parallels with the way quants conduct research. First, let's imagine that a quant researcher observes that the various markets go through phases in which they tend to rise for extended periods, followed by phases in which they tend to fall for awhile. She theorizes that a phenomenon called a *trend* exists, which, for whatever reason, causes the future performance of a market to be in the same direction as its recent historical performance. The consequence of this theory would be that she should be able to achieve a better‐than‐random forecast of how markets will perform, given only information on how these markets have performed before. So, she sets out to test the theory, and lo and behold, she finds that the evidence does not contradict her theory. Using some metric to define the historical trend (such as the moving average crossover example we used in Chapter 3), she sees that she can indeed forecast markets better than random chance is likely to allow. But she can never be sure. At best, she can have enough confidence that her tests were sufficiently rigorous to warrant risking some capital on the validity of this theory.

One important distinction, however, exists between quants and scientists. Scientists conduct research for many purposes, including learning the truth that drives the natural world. And in the natural sciences, a good theory—one that is well supported by the evidence and is widely useful in a variety of practical applications (e.g., Einstein's relativity)—does not require modification to continue to be valid. Quant researchers, by contrast, have no choice but to conduct ongoing research and to take every measure to ensure that their research output is prolific. This is because, though nature is relatively stable, the markets are not. Whether from regulatory changes, the changing whims of the aggregate psychology of investors and traders, the constant competition for alpha among traders, or whatever other phenomena, the markets are in fact highly dynamic processes. For this reason, quant traders must constantly conduct research so that they can evolve with as much rigor and forethought as they used in developing their original strategies.

# Idea Generation

Ideally, quants follow the scientific method in their research. In this regard, the development of theories (or theoretically sound approaches to data mining) is the first key step in the research process. We find four common sources of ideas to be observations of the markets, academic literature, migration, and lessons from the activities of discretionary traders.

The main way that quants come up with their own ideas is by watching the markets. This approach most directly embodies the spirit of the scientific method. An excellent example comes from the history of the oldest of quant trading strategies: trend following in futures contracts. Richard Donchian is the father of trend following. He originally traded stocks, but in 1948, he created Futures, Inc., the first publicly held commodity fund. In December 1960, he published his philosophy toward trading in his newsletter, *Commodity Trend Timing*. 1 He observed that there are sweeping moves in many markets that folks tend to call bull or bear markets; he postulated that one could build a system that would detect that these trends had begun and then ride the wave. He translated his philosophy into the following strategy: If a given market's price is above the highest closing price over the past two weeks, buy that market. If its price goes below the lowest closing price over the past two weeks, sell that market short. In the meantime, hold whatever position you have in that market. Using this incredibly simple system, from 1950 to 1970 he built a successful track record and spawned an industry that now manages hundreds of billions of dollars.

The academic literature in quantitative finance, and finance more generally, is replete with papers on a massive array of topics of interest to quant researchers. For example, many finance papers have been written on clever ways in which corporate chief finance officers (CFOs) attempt to fudge their companies' earnings and other financial figures to retain the confidence of shareholders. Quant firms have taken note, and several now have strategies in their arsenal that look for the kinds of behaviors described in the academic literature for trading opportunities. Many quant firms spend significant time scouring academic journals, working papers, and conference presentations to glean ideas that can be tested using the scientific method. Such a quant could find papers on topics such as the management of financial statements and could test ideas learned from these papers. Perhaps the most classic example of an academic paper that made massive waves in the quant trading community is Harry Markowitz's paper, modestly entitled "Portfolio Selection." As discussed in Chapter 6, in "Portfolio Selection," Dr. Markowitz proposed an algorithm to compute the "optimal" portfolio using a technique called *mean variance optimization.* For all the research that has been done on portfolio construction over the decades since Dr. Markowitz's paper was published, his technique and variants of it remain key tools in the toolbox of quant trading. Aside from the literature in finance, quants also frequently utilize the literature from other scientific fields—such as astronomy, physics, or psychology—for ideas that might be applicable to quant finance problems.

Another common source of new ideas is via the migration of a researcher or portfolio manager from one quant shop to the next. Though many firms attempt to make this more difficult via noncompete and nondisclosure agreements, quants can effectively take ideas from one place to another, and this is to be expected. Any rational quant would want to know what the competition are doing, particularly those who are successful. At least part of the attraction of a potential new hire who has worked elsewhere must be the prospect of learning about the activities, and maybe even some secrets, of competitors. There are countless examples of this sort of thing. Goldman Sachs gave birth to AQR's quantitative approach to global tactical asset allocation and global equity market‐neutral trading. Richard Dennis trained a group of new traders called the Turtles, none of whom had any trading experience, in trend following as a social experiment and to settle a bet with his friend William Eckhardt. D. E. Shaw was created after its founder cut his teeth at Morgan Stanley's statistical arbitrage prop trading desk and has itself spawned several successful alumni, including Two Sigma and Highbridge's quantitative equity manager. In a fascinating case, Renaissance Technologies, famous for its ability to retain talent partly by having its researchers sign iron‐clad noncompete agreements, once lost two of its researchers to Millennium Partners. Renaissance sued Millennium over the incident, and it turned out that the researchers had somehow managed *not* to sign the noncompete agreements while at Renaissance. Nonetheless, the traders were ultimately terminated by Millennium, who simply decided that retaining them was more trouble than it was worth. Sometimes, investors who have peeked behind the curtains as part of their assessment of a given quant shop, and then shared what they've seen with others, act as the carriers of ideas from one quant shop to the next.

Finally, quants learn lessons from the behavior of successful discretionary traders. For example, an old adage among successful traders is "Ride winners and cut losers." This idea can easily be formalized and tested and has come to be known as a *stop‐loss policy,* which involves systematically realizing losses on positions that are not working out. There are many examples of quants working closely with successful discretionary traders in an attempt to codify aspects of the latter's behavior into a trading system. Not all are necessarily bound for success. *Technical trader* is the label applied to a trader who subjectively analyzes graphs of market prices and makes decisions based on "rules" about the implications of various shapes of such graphs. These shapes are given names such as a *head and shoulders pattern* or an *upward triangle pattern*. Many quant funds have come and (mostly) gone that have attempted to re-create such patterns into systematic trading rules. This could be because the idea itself is not based on valid theory, or it might be because the human version is ultimately less rule based, as one might like to believe, condemning a truly systematic implementation to be unsuccessful. However, even here valuable lessons can be learned: Not all successful traders have skill, and a helpful way to begin figuring out what *really* works and doesn't is to put an idea through the grinder of a research process and see if it's still alive at the end.

# Testing

The process of testing is central to research. At first glance, the most common version of this process looks fairly simple. First, build a model and train it on some subset of the data available (the *in‐sample period*). Then test it on another subset of the data to see if it is profitable (the *out‐of‐sample period*). However, research is an activity that is fraught with peril. The researcher is constantly offered opportunities to forgo rigor in favor of wishful thinking. In this section, we address some of the work and challenges inherent in the research process.

#### In-Sample Testing, a.k.a. Training

In quant trading, models are approximations of the world. They are used to predict the future using data as inputs. The first part of the testing process is to *train* a model by finding optimal parameters over an in‐sample period. That sounds rather like a mouthful of marbles, so let's walk through it term by term.

Let's imagine that we want to test the idea that cheap stocks outperform expensive stocks. We even theorize that the metric we will use to define cheapness is the earnings yield (earnings/price), such that a higher earnings yield implies a cheaper stock. But what level of yield is sufficiently high to cause us to think that the stock will outperform? And what level of earnings yield is sufficiently low to imply that a stock is expensive and is likely to underperform? These levels are parameters. In general the parameters of a model are quantities that define some aspect of a model and can affect its performance. These are variables that can be set at whatever level one chooses, and by varying these levels, the model itself is altered and will provide different results.

Imagine that you hire a consultant to help you buy the ideal "optimal" house. The consultant lists all the relevant variables that might factor into your decision, things like the size of the house, its condition at the time of purchase, and the location and school district. If you do not tell him your ideal levels for each of these variables, he can deduce them by observing your reaction to various houses. A big house in a poor neighborhood might generate a lukewarm reaction, whereas a smaller house in a good neighborhood might generate a higher degree of interest for you. In this way, the consultant can deduce that you dislike the first neighborhood and prefer the second, and furthermore that the neighborhood might be more important to you than the size of the house. If he is able to repeat these "experiments," he can continue to fine‐tune the choices he presents to you until he finds the house that matches your desires optimally. To the extent he succeeds in this endeavor, he has performed well.

In this way, optimal parameters in a quant model are those that lead to the best performance based on whatever metrics one chooses to use to measure goodness. Training a model involves simply finding the optimal parameter set, which is usually accomplished by trying a number of them and hoping that at least one set comes out looking appealing. What constitutes appeal is a matter we will discuss in some detail forthwith, but first we consider some other aspects of in‐sample research.

In‐sample research is, in a sense, fun for a quant. In the real world, the quant's model is constantly buffeted by new information and unpredictable events. But the historical data from the in‐sample period are known to the model in their entirety, and nothing about them needs to be predicted. The in‐sample period is like the answer key to a test in grade school. It is the model's best chance to work, because it doesn't have to predict anything. The model simply has to do a reasonable job of explaining the in‐sample period after the fact, with the whole picture available for review. This is the one part of the research process in which there is a high degree of hope.

An important decision lies in the process of in‐sample testing: What exactly constitutes the sample chosen for fitting the model? A sample is characterized by two things: its breadth and its length. Imagine that a researcher plans to build a strategy to trade the approximately 5,000 listed U.S. stocks and that she has at her disposal data starting in 1990 and ending now. As far as the breadth of the in‐sample test, the researcher must choose how many of the stocks to use and decide how to choose the ones that are used. Should she use a broad cross‐section of stocks across sectors and capitalization levels? Should she use a narrower cross‐section, or should she choose all the stocks? As to length of time, the researcher must consider what window of data will be available to use for fitting the model. Will it be the most recent data or the oldest data? Will it be a random set of smaller time windows or the entire set of data from 1990 onward? The most common preference among quants would be to use all the instruments for some subset of the time, but this is by no means universal, since there is a trade‐off here to consider.

By using more data, the quant has a broader array of scenarios and market events that the model has to fit itself to, which can help make it more robust. By the time it has to succeed in real conditions, it has already "seen" and been adapted to the scenarios and environments found in the large in‐sample period. On the other hand, the more data the model is allowed to see while it is being tuned, the greater the risk of creating a model that is nothing more than a good explanation of the past. For this reason, many quants utilize a reasonable cross‐section of the data for the purpose of in‐sample testing and model fitting.

#### What Constitutes a "Good" Model?

Quants utilize a wide variety of metrics to determine the "goodness" of a model. This is true for both the in‐sample part of the process and the out‐of‐sample part of the process, the latter of which we discuss in the next section. I include here a number of statistics (and other output) that quants may use. I illustrate these metrics using a strategy for forecasting the S&P 500. It has a one‐day horizon for its forecast, and it uses an adjustment to a well‐known idea known as the *equity risk premium,* which is calculated by taking the difference between the earnings yield of the S&P 500 and the

10-year Treasury note each day. If the S&P's yield is higher than the bond's, this is viewed as a signal to be long stocks. If the S&P's yield is lower than the bond's, this is a signal to be short stocks. I built this strategy back in the mid-1990s for tactical asset allocation purposes, but I have never traded it, for reasons that will be obvious after we assess it using these metrics. It is shown simply as a way of illustrating the kinds of tests that a strategy is required to pass before being implemented in the real world, with real money. The results I show for the strategy are based on daily closing prices from June 1982 through December 2000.

**Graph of the Cumulative Profits over Time** A graph indicating the cumulative profits over time is one of the most powerful pieces of output in a testing process because, as they say, one picture is worth a thousand words. From a graph of cumulative profits, you can see whether the strategy would have made money, how smoothly, and with what sort of downside risk, just to name a few things. As you can see in Exhibit 9.1, the S&P strategy shows as being profitable over the test period, but its return stream is very lumpy, characterized by long periods of inactivity (several years, in some cases), some sharp losses, and some very steep gains. Immediately a researcher can see that this strategy has some real problems. Is it realistic to want to sit on the sidelines making almost no trades, and certainly no profits, from late 1989 until early 1995?

![](_page_7_Figure_3.jpeg)

FXHIRIT 9.1 Back-Tested Cumulative Profits of the S&P 500 Strategy

Average Rate of Return The average rate of return indicates how well the strategy actually worked (i.e., how much it might have made) in the past. If it didn't work in the testing phase, it's very unlikely to work in real life. As we will see later, testing offers many opportunities for the researcher to believe that making money in trading is a trivially easy exercise. Sadly, this misperception is mainly due to a wide variety of deadly traps. In our S&P 500 example, the total cumulative profits in the simulation were 746 percent, which comes to an average annual rate of return of 12.1 percent before any transaction costs or fees.

Variability of Returns over Time The variability of returns over time, which describes the uncertainty around the average returns, is helpful in deciding whether the strategy is worth owning. In general, the less the variability for a given level of returns, the better a strategy is considered to be. For example, if a strategy averages 20 percent returns per year, with an annual standard deviation of 2 percent (i.e., 67 percent of the time, the annual rate of return should fall within +/–2 percent of the average 20 percent figure, or between 18 and 22 percent), this would be a better outcome than if the same 20 percent average annual return came with 20 percent annual standard deviation (i.e., 67 percent of the time, returns are within 0 and 40 percent). The idea is that one can have more confidence in a given return if the uncertainty around it is low, and more confidence is a good thing.

At my shop, we look at a statistic we dubbed *lumpiness*, which is the portion of a strategy's total return that comes from periods that are significantly above average. This is another way of measuring consistency of returns. Despite the importance of this metric, it is not always the case that consistency should be a primary goal. Nevertheless, it is good to know what to expect as an investor in or practitioner of a strategy, if for no other reason than to discern when the strategy's behavior is changing. In our S&P 500 strategy, the annualized standard deviation of its daily returns over the entire test period was 21.2 percent.

Worst Peak-to-Valley Drawdown(s) This metric measures the maximum decline from any cumulative peak in the profit curve. If a strategy makes 10 percent, then declines 15 percent, then makes another 15 percent, the total compounded return for this period is about +7.5 percent. However, the peak‐to‐valley drawdown is –15 percent. Another way of stating this is that the investor had to risk 15 percent to make 7.5 percent. The lower the drawdown of a strategy, the better. Many quants measure not just one drawdown but several, to get a sense of both the extreme and more routine downside historical risks of their strategies. It is also typical to measure recovery times after drawdowns, which give the researcher a sense of the model's behavior *after* it's done poorly. Long recovery times are generally disliked because they imply that the strategy will remain in negative territory for quite a while if it does go into a large drawdown at some point. The S&P 500 strategy's worst peak‐to‐valley drawdown in the back‐test was –39.7 percent, and it came from being short the S&P 500 in the summer of 1987, before the crash in October actually made that trade look good.

Drawdown information must be handled with care, however. If we were to look at the returns of a convertible bond arbitrage strategy from 1990 through 1997, eight years of data (which is considered a long track record in the hedge fund business) would show you very limited drawdowns. But in 1998, these strategies were badly hurt. The problem was *sample bias*, which means that the sample we used to determine the "worst" drawdown was not a fair representation of the whole array of possible outcomes. Rather, even though it was "long," it covered a period that was almost entirely favorable to this strategy, which would lead to an underappreciation of the potential downside risks. There's not a great solution to this problem: Either the sample over which the drawdown was computed is sufficiently large as to cover a large range of market regimes and both favorable and unfavorable environments (specifically as it relates to the strategy being tested), or it doesn't. If the sample cannot be made larger and more representative of all possibilities (the population, in stats‐speak), then the quant can only exercise some judgment about how much worse things could look if the environment did turn ugly. This is self‐evidently an exercise that depends heavily on the judgment of the researcher, and even then is at best a ballpark figure.

Furthermore, the worst historical drawdown is merely one potential path that even this biased sample could have produced. Imagine that the historical return distribution of a strategy is like a deck of cards. If we turn the cards over in the order that they are already placed in the deck, we get the historical time series. If, however, we shuffle the deck and then turn the cards over in this new order, we get a different time series from the same return distribution. If we do this over and over, thousands of times, we will get many theoretically possible paths from one actual history. This practice is known as *resampling*, and it is done to boost the power of a historical sample. With these thousands of resampled histories, we can compute the worst drawdown(s) of each one, and have a more robust estimate of the potential downside risk of a strategy. It is therefore a sensible practice to recognize again that the deck itself contains only a subset of all the cards that might one day be dealt to us. It may, we worry, contain too many aces and kings, and not enough twos and threes.

Predictive Power A statistic known as the *R‐squared* (R2 ) shows how much of the variability of the thing being predicted can be accounted for by the thing you're using to predict it, or, in other words, how much of the variability in the target is explained by the signal. Its value ranges from 0 to 1, and there are a couple of valid ways to compute it. That said, most statistical packages (including Microsoft Excel) can compute an R2 with minimal effort on the part of the user. A value of 1 implies that the predictor is explaining 100 percent of the variability of the thing being predicted. In case it's not already clear, when we talk about "the thing being predicted," we are of course referring to a stock or a futures contract or some other financial instrument that we want to trade. In quant finance, we're literally trying to predict the future prices/returns/directions of such instruments, making an R2 of 1 basically impossible, unless methodological errors are being made. In fact, a superb R2 in our industry is 0.05 (out of sample, to be discussed later in this chapter). A former employee of mine once said, "If you see an R2 above 0.15 and it's not because you made a mistake, run the other way, because the SEC will arrest you for insider trading if you use it." Note that an R2 of 0.15 implies that some predictor describes 15 percent of the *future* variability of the target of the forecast. As another quant trader put it, "People have gotten rich off a 0.02 R2 ." Exhibit 9.2 shows that the R2 of the S&P 500 strategy was less than 0.01 from 1982 through 2000.

![](_page_10_Figure_2.jpeg)

Exhibit 9.2 R2 of the S&P 500 Strategy

Quants frequently utilize an additional approach to ascertaining predictive power. This approach involves bucketing the returns of the instruments included in the test by the deciles (or any other quantile preferred by the researcher) of the underlying forecasts. In general, a model with reliable predictive power is one that demonstrates that the worst returns are found in the bucket for which the worst returns are expected, with each successive bucket of improving expected returns in fact performing better than the prior bucket. If the returns of the instruments being forecast are not monotonically improving with the forecast of them, it could be an indication that the strategy is working purely by accident.

A bar chart showing the quintile study for the S&P 500 strategy is shown in Exhibit 9.3. As you can see, in this study at least, the strategy looks reasonable. The leftmost bucket of signals coincides with an average return in the S&P 500 (on the subsequent day) of –2.35 percent, and indeed, this is the worst average S&P return of any of the buckets. The second bucket from the left shows that the S&P 500 strategy's second‐most‐bearish group of forecasts for the S&P averages –0.19 percent. As we move to increasingly bullish signals, the S&P's returns continue to improve in accordance with the bullishness of the forecasts, which is what one would hope for. The fact that each bucket's average return is better than the one previous to it is said to imply a monotonic relationship between our alpha signal (the modified equity risk premium signal described earlier) and the target of our forecasts (the S&P 500 index's return over the next day).

![](_page_11_Figure_3.jpeg)

Exhibit 9.3 Quintile Study of S&P 500 Strategy's Signals versus S&P 500 Returns

*Research* 159

Percentage Winning Trades or Winning Time Periods This percentage is another measure of consistency. It tells the researcher whether the system tends to make its profits from a small portion of the trades that happened to do very well or from a large number of trades, each of which might contribute only modestly to the bottom line. Similarly, one can easily measure the total number of winning (positive) periods versus the total number of periods. (This is most often measured by percentage winning, or profitable, days.) In both cases, one tends to have more confidence in strategies with greater consistency. In the S&P strategy, the results of this study are somewhat unusual in that the strategy is not designed to produce a signal every day but instead only when the model perceives that the opportunity is sufficiently attractive to warrant trading at all. As such, the model produces a zero signal 65 percent of the time. It produces winning trades about 19 percent of the time and losing trades about 16 percent of the time. Of the days when it actually has a nonzero signal, it wins approximately 54 percent of the time. This, too, is not a terrible outcome for a strategy.

Various Ratios of Return versus Risk A great many statistics have been proposed as useful measures of *risk‐adjusted return,* which are generally all attempts to measure the "cost" (in terms of risk) of achieving some return. The canonical example is the Sharpe ratio, named after William Sharpe (mentioned earlier in connection with the Nobel Prize in Economics he shared with Harry Markowitz in 1990). The *Sharpe ratio* is computed by taking the average periodic return above the risk‐free rate and dividing this quantity by the periodic variability of returns. The higher the Sharpe ratio, the better. Quants (and many in the investment management business) have shortened this moniker by dropping the word *ratio*. A strategy with a *2 Sharpe* is a strategy that delivers two percentage points of return (above the risk‐free rate) for each point of variability (and this is a rather good Sharpe, if you can get it).

A close cousin of the Sharpe ratio is the *information ratio*, which is different from the Sharpe only in that it eliminates the risk‐free rate from the formula. The information ratio of the S&P 500 timing strategy is a mere 0.57, meaning that the investor receives 0.57 percent in return for every 1 percent in risk taken (again, before transaction costs and before any other fees or costs of implementing the strategy). The *Sterling ratio* (average return divided by the variability of below‐average returns), the *Calmar ratio* (average return divided by the worst peak‐to‐valley drawdown), and the *Omega ratio* (the sum of all positive returns divided by the sum of all negative returns) are also widely used among a number of other risk‐adjusted return metrics. The S&P 500 strategy from 1982 through 2000 displayed a Sterling ratio of 0.87, a Calmar ratio of 0.31, and an Omega ratio of 1.26. Of these ratios, the most discouraging is the low Calmar ratio, which indicates that the strategy generated only 0.31 percent in returns for every 1 percentage point of drawdown it experienced.

Relationship with Other Strategies Many quants utilize several kinds of strategies at once. As such, the quant is effectively managing a *portfolio of strategies*, which can be thought of much like any other kind of portfolio in that diversification is desirable. The quant frequently measures how a proposed new idea will fit in with other, already utilized, ideas, to ensure that the new strategy is in fact adding value. After all, a good idea that doesn't improve a portfolio is not ultimately useful. Though it is common to compute a correlation coefficient between the new idea and the existing portfolio of strategies, many quants measure the value‐added of a new strategy by comparing the results of the existing strategy with and without the new idea. A significant improvement in the results indicates that there is a synergistic relationship between the new idea and the existing strategy.

Time Decay In testing a strategy, one interesting question to ask is, how sensitive is this strategy to getting information in a timely manner, and for how long is the forecast effect sustained in the marketplace? Many quants will seek to understand what their strategies' returns would be if they must initiate trades on a lagged basis after they receive a trading signal. In other words, if a strategy initiated a signal to sell Microsoft (MSFT) on April 28, 2006, the quant can see what the performance of his strategy would be in MSFT if it was not allowed to sell MSFT for one day, two days, three days, and so on. In this way, he can determine his strategy's sensitivity to the timeliness with which information is received, and he can also gain some information about how crowded the strategy is (because more crowding would mean sharper movements to a new equilibrium, i.e., faster degradation of profit potential). Imagine that a researcher develops a strategy to trade stocks in response to changes in recommendations by Wall Street analysts. Increases in the level of analysts' consensus recommendations for a company lead to a targeted long position in that company, whereas deterioration in the aggregate recommendation level would lead to a targeted short position in the company. This strategy is popular and followed by many quants (and discretionary traders). However, its effects are very short‐lived and are very sensitive to the timing of the information received.

An example of this phenomenon is shown in Exhibit 9.4, using MSFT from April through October 2006. As you can see, there were five downgrades on April 28, which caused MSFT to underperform the S&P 500 by about 11.4 percent on the day the downgrades were announced. In fact, the *opening price* of MSFT on the 28th was already down about 11.1 percent because the downgrades all took place before the market opened. As such, the quant trader must be careful not to allow his simulation to assume that he was able to transact in MSFT early enough to capture any of the 11.1 percent change. Instead, to be conservative, he can test what his, say, two‐week performance on the trade would have been if he initiated the trade on various days *after* the initial ratings change.

If he did this, what he would find is that if he sold MSFT *at any time after* the close of April 27 (the night before the recommendation changes were announced), his trade would have actually been pretty mediocre. He would have made money selling MSFT at the close on April 28, May 1 (the next business day), or May 2, but from May 3 through May 12 the trade would have been unprofitable. This illustrates the importance of stress‐ testing a strategy's dependence on timely information, which might not always be available.

Interestingly, delaying the signal's implementation does not always result in a negative outcome. For example, our S&P strategy tends to be

![](_page_14_Figure_4.jpeg)

![](_page_14_Figure_5.jpeg)

Exhibit 9.4 Illustration of Time Decay of Alpha

"early" on its trades, that is, it tends to be short too early and long too early, even though the market subsequently does move in the direction forecast, on average. As such, delaying its entry by merely one day dramatically improves the total return of the strategy, from 746 percent total (12.1 percent annualized) to 870 percent total (12.9 percent annualized). This does not necessarily bode well for the use of such a strategy. In general, it is not comforting to know that you get a signal from your trading strategy that you not only *can* go without implementing for a little while (which would be the better result) but that you actually are *better off* ignoring for at least a full day after you get the signal.

Sensitivity to Specific Parameters It was mentioned earlier that parameters can be varied, and by varying them, differing outcomes are likely. But much can be learned about the quality of a strategy based on how much the outcomes vary as a result of small changes in the parameters. Let's use our P/E‐ based strategy from earlier as an example. Imagine that we think that any P/E ratio that is either above 50 or negative (because of negative earnings) should be considered expensive. Meanwhile, we presume that any P/E ratio below 12 is cheap. Assume we test the strategy according to the previously discussed metrics and find that a low P/E strategy with these parameters (≥50 implies expensive, ≤12 implies cheap) delivers a 10 percent annual return and 15 percent annual variability.

Now imagine that we vary the parameters only slightly so that any stock with a P/E ratio below 11 is cheap and any with a P/E ratio that is negative or above 49 is expensive. If this version of the strategy, with slightly differing parameters, results in a significantly different outcome from the first example, we should mistrust both results and use neither in our model. This is because the model has proven to be overly sensitive to a small change in the values of the parameters, which makes little real‐world sense. Should there be any great difference between a 10 P/E and an 11 P/E, or between a 50 P/E and a 49 P/E? What many researchers look for is smoothness of the goodness of outcomes with respect to parameter values. Near‐neighboring sets of parameters should result in fairly similar results, and if they don't, a researcher should be a bit suspicious about them, because such results may indicate *overfitting*.

#### Overfitting

The previously described metrics represent a sampling of the kinds that quants use to determine whether a given model is good. These metrics are used to judge the quality of a model, both while it is being created and when it is being used. Indeed, many hedge fund investors look at the majority of these metrics as ways of gauging the performance of various traders.

There remains, however, one more extremely important guiding principle in determining the goodness of a quant strategy, and this is an absolute terror of *overfitting*. Overfitting a model essentially implies that the researcher is asking too much of the data. The more classical definition is that a researcher has built a model that closely explains the past, but which is a poor guide to the future. This can happen in several ways.

First, the researcher must be careful about the complexity of a model. Complexity in a model can come from a couple of sources. One is the number of predictive factors. In building a model, a researcher could include thousands of factors to explain the past fluctuations in asset prices. This model could more or less explain exactly what *has happened* in the past. But let us recall that the goal of quant trading models, like the goal of any alpha‐seeking trader, is to predict the future, not to explain the past. And while we all expect the past to provide some guidance as to the future, we also must understand that the past is, at best, an imperfect guide to the future. This in turn implies that to perfectly explain the past is not necessarily useful in predicting the future.

Second, a researcher can create a very complex model in terms of the conditionalities utilized. For example, one might conceive of a strategy that looks for a specific pattern of price behavior in order to determine a long or short position. A simple model might call for a long position to be initiated in an instrument if that instrument is up more than some amount over the past 10 days. A more complex model might call for the long position to be initiated if the instrument is down over the past one day, up over the past 10 days, down over the past 20 days, and up over the past 100 days. Of course, as humans, we're great at rationalizing things, so we might be able to come up with some explanation for why such a strategy is definitely going to work. But it is unequivocally complex, in that there are many "if" statements embedded in it. As such, it is very fragile.

The desire for relatively simple models for use in forecasting is known as *parsimony*. Parsimony is derived from the Latin word *parsimonia,* meaning *sparingness* and *frugality*. Among quants, parsimony implies caution in arriving at a hypothesis. This concept is absolutely central to the research process in quant trading. Models that are parsimonious utilize as few assumptions and as much simplicity as possible in attempting to explain the future. As such, models with large numbers of parameters or factors are generally to be viewed with skepticism, especially given the risks of overfitting.

Parsimony has its roots in a famous principle of a Franciscan friar and logician, William of Occam, known as *Occam's razor*. Occam's razor is roughly translated from the original Latin as follows: Entities must not be multiplied beyond necessity. In science, this has been understood to mean that it is better to use as few assumptions, and as simple a theory, as possible to explain the observations. Karl Popper pointed out in 1992 that simpler theories are better because they are more easily tested, which means that they contain more empirical value. All around, scientists agree that parsimony, the stripping away of unnecessary assumptions and complexity, is simply better science. Einstein's saying, quoted at the beginning of this chapter, adds an important caveat, which is that oversimplifying an explanation is not helpful either.

Looking again at our example of the consultant hired to help you buy a house: If he adds a large number of factors to the mix, such as the color of the guest bathroom tiles or the type of roofing material, given that there is no reason to believe *ex ante* that such factors are priorities for you as his client, his analysis would become muddled and confused. The complex model (in terms of number of factors) might do a decent job of explaining your past behavior, but it is unlikely to do an excellent job of predicting whether you'll like a house you haven't already seen, because it is unlikely all the factors included are actually important in ascertaining your preferences in a house. On the other hand, if the agent uses only two factors—say, the size of the house and its school district—this model might not do a good job of predicting your preferences because it leaves out too many important variables, like the number of bedrooms, the number of bathrooms, the condition of the property, the size of the lot, and so on. Just so, an important part of the quant researcher's job is balancing on the tightrope between trying to explain the past too perfectly and trying to explain it too little. To one side is failure due to overcomplicating the model and to the other is failure due to oversimplifying it.

A related type of overfitting can be seen in strategies that trade only extremely infrequently. Here, we are concerned with the problem that a small number of trades, no matter how profitable, is unlikely to provide strong statistical significance. For example, imagine a model that buys the S&P 500 any time it has a drawdown of at least 40 percent, and buy the S&P back again when it reaches a new high. We will see a strong backtest for this strategy if we run one. This strategy would have enormous profits and relatively small drawdowns. But it also would see exactly three trading signals in the past 40 years! It is hard to get too excited about anything that occurs once every 13 years, and it is certainly risky to put any serious amount of capital at risk in such a strategy.

Another common source of potential overfitting risk comes from the specification of parameters. As a reminder, in Chapter 3, we discussed the fact that many models have parameters. For example, in constructing a trend model, a researcher is making the claim that some characteristic of the change in price over some period in the past is indicative of a likely future continuation of that move. There are several parameters one could imagine being relevant to such a model. For example, what is the historical period over which the price change is computed (this is known as *lookback* in quant circles)? If there is a minimum size for such a move to be considered significant, what is that minimum size? These questions are not central to the definition of the model, but different answers to these questions can nonetheless have significant impact on the returns that the model generates. For example, a trend model lookback of two days may generate returns that are completely uncorrelated to the returns of the *exact same* trend model with a lookback of six months.

Quants can fix parameters in a few ways. One is to set them discretionarily, based on a prior conviction about how the markets function. The results of a backtest will then indicate how much promise the strategy has. Such an approach has the benefit of not being fitted at all. It either works or it doesn't. There are drawbacks to this approach, too, in that it relies heavily on the judgment of the researcher, and the optimal parameter value might lie reasonably far away from the one set in his model. That said, a researcher who has Bayesian tendencies (as discussed in Chapter 3) will likely be inclined to set parameters in this manner.

A second approach to parameter fitting is to have a part of the backtest show the results of the strategy historically over a variety of parameter values, and to select the parameters that optimize the "goodness" of the result. This decision can be subjective or it can be done by an algorithm that reflects some rule‐based approach to parameter selection. We now turn to a discussion of the considerations that apply to the selection of parameter values. Consider Exhibit 9.5.

![](_page_18_Figure_4.jpeg)

Exhibit 9.5 Choosing the Right Parameter Values

Which of these points would you guess is the best choice for a parameter value? Choice A doesn't look so good, because the strategy seems to do poorly when using such a parameter value. Choice C looks enticing because it is the highest point of a broad plateau. But it is so near a cliff's edge that we cannot be sure whether we're at risk of picking unwisely. Choice D seems to have the best outcome, but it is also fairly unreliable, since its near neighbors are universally poor. This leaves Choice B as the best. Even though we haven't picked the highest point on the plateau, we've picked one with a margin of safety around it on both sides. It bears discussing a bit about why this margin of safety is so important.

When we see a lonely peak like the one represented by Point D in Exhibit 9.5, it is likely that our testing has uncovered some spurious coincidence in the fitting period that makes it especially favorable for that single parameter value. Unfortunately, it is likely that this coincidence will not persist into the future. By selecting Point D, in other words, we are implicitly betting that the future will look *exactly* like the past. You might be familiar with the standard performance disclaimer: "Past performance is not an indication of future results." Yet we all tend to judge the success of a trader at least partly by performance, which is a way of saying that we think the past might actually be *some* indication of the future. Similarly, and based on the premises of scientific research in general, all quant trading (and indeed, all science) assumes implicitly that the past can have some value in helping us understand the future. This is why the scientific method starts with observations of the world that can be generalized into a theory. But the appropriate way to think about the past is as a *general guide* to the future, not as an exact copy. We build a model that is itself a generalized description of this general guide to the future, and when phrased this way, it is clear that we want our model to err on the side of caution. As such, sensible quants would refer to Point B as being more "robust" than Point D because Point B has a better chance of being good, but not solely as a result of some accident of the sample data used to find it.

One final note on parameter fitting is that parameters can either be fit once, or they can be fit repeatedly in the future, as new data are generated in the markets. The considerations described here apply to each instance of such a fitting of parameters. However, the act of repeatedly fitting parameters itself increases the complexity of a model. And, depending on exactly how such a re‐fitting is implemented, this approach may also rely on fewer data than are desirable for an exercise so fraught with overfitting peril already.

This leads us directly to a final, important consideration in the subject of overfitting, one which we alluded to in Chapter 3. The fact is that capital markets generate an enormous amount of data. Firms like Renaissance are

famous for collecting terabytes of data each day. Merely capturing every message generated on every U.S. equity exchange requires more than one terabvte of data storage every day. However, the data are extremely noisy. Why capital markets are such a noisy process is a subject for philosophers and economists. But it is an undeniable fact that past data yield extremely little information about future fluctuations. This is the reason that a great out-of-sample  $r^2$  is on the order of 0.04 (out-of-sample testing is the subject of the next section). There is just very little signal buried in all that noise. Something which, therefore, closely explains such a noisy process must be viewed with an extreme amount of skepticism.

Quant researchers must evaluate the theories they are testing. This is done using a wide array of measurements and techniques, but ultimately, a significant amount of discretion is used. It is unquestionably the case that what separates a successful researcher from the rest is good judgment about the kinds of issues raised in this chapter. As a general principle, we may note that good researchers must possess sufficient confidence and skill to believe that theories can be developed or improved on. At least as important, researchers must also be skeptical and humble enough to know and be entirely at peace with the fact that most ideas simply don't work.

#### **Out-of-Sample Testing**

Out-of-sample testing, the second half of the testing process, is designed to tell the researcher whether her formalized theory actually works in real life, without the benefit of seeing the cheat sheet provided during in-sample testing. The model's parameters have by now been fixed using a different set of data (from the in-sample testing period), and it's simply a question of whether the model, with whatever parameters are chosen, really works in a new, out-of-sample data set. Many of the same kinds of statistics as described in this chapter are utilized to make this judgment.

One additional statistic many quants use is the ratio of the  $R^2$  in the out-of-sample test to the  $R^2$  in the in-sample test. This ratio is another way for the researcher to obtain a sense of the robustness of the model. If the outof-sample  $R^2$  is relatively close to the in-sample  $R^2$  (i.e., if the ratio is about half or better), that is considered a good thing. If it is significantly smaller, the researcher must be suspicious about the prospects for his model's success.

There are many approaches to out-of-sample testing. The simplest utilizes all the data that were set aside from the in-sample test. Some researchers utilize a *rolling out-of-sample* technique in which the single oldest data point is discarded and one new data point is added to both the fitting (in-sample) and testing (out-of-sample) period. This process is repeated through the entire available sample of data. The rolling out-of-sample technique is thought to

help refresh the model over time so that it does not depend on a single set of tests that might have been run some years previously. However, depending on the circumstances, this approach can have the weakness of giving the model the benefit of constant knowledge of the recent past, which could reduce its robustness. This trade‐off is extremely subtle and can be debated in any individual instance, rendering impractical any general judgment about its effectiveness. Still another approach utilizes an ever‐growing window of data for ongoing out‐of‐sample testing as time passes and more data are collected.

Though the objective of out‐of‐sample testing is clearly valid, it turns out to be a rather tricky thing to do correctly. Imagine a researcher who completes the model fitting over the in‐sample data. Then, having a model that seems robust, the researcher tests it over the out‐of‐sample data. But the model fails to deliver a good result on this new data set. The researcher, already having invested a lot of time on the model, decides to examine the reasons for the model's failure over the out‐of‐sample period and discovers that the environment changed between the in‐ and out‐of‐sample periods in such a way that the model was making losing trades during the latter. Having learned a useful lesson, the researcher goes back to the model and alters it to account for this new information. He refits the model in sample, and then retests it out of sample. And, lo and behold, it works much better.

Before we break out the champagne, however, we should consider what the researcher has just done. By learning from the out‐of‐sample data and using that information to train the model anew, he has effectively used up his out‐of‐sample data and has caused them effectively to become part of the in‐sample data set. In general, going back and forth between the in‐ and out‐of‐sample data is a terrible idea. This brings up a still more subtle issue, but one that is closely related.

Often we know enough about that happens in the capital markets during the out‐of‐sample period that we tend to build models and select parameter sets that we believe are likely to work out of sample anyway. This sullies the purpose of an out‐of‐sample test because we are, in many respects, looking ahead. For example, we can look back on the Internet bubble of the late 1990s and know that the world and the economy in fact did not change and that negative earnings should not be wildly rewarded in the long run. If we build a strategy today, we can know that it is possible for the Internet bubble to happen but that it eventually bursts. However, we could not have known this with certainty in 1999.

The world finds new and interesting ways to confound our understanding. As such, to test our current best thinking against competition that existed in the past is a form of wishful thinking. This is a subtle and nefarious form of look‐ahead bias, which is a critical problem in research. As researchers become more and more familiar with the out‐of‐sample periods they use to test their ideas' validity, it becomes more likely that they are implicitly assuming they would have known more about the future than in fact they would have known had they been asking the same questions historically. This practice is called *burning data* by some quants.

To mitigate the data‐burning form of look‐ahead bias, some quant shops take reasonably drastic measures, separating the strategy research function from the strategy selection function and withholding a significant portion of the entire database from the researchers. In this way, the researcher, in theory, cannot even see what data he has and doesn't have, making it much more difficult for him to engage in look‐ahead activities. With less draconian restrictions, the researcher might simply not be allowed to know or see what data are used for the out‐of‐sample period, or the portions of data used for in‐ and out‐of‐sample testing might be varied randomly or without informing the researcher. Regardless, as you can easily see, the problem of doing testing is tricky and requires great forethought if there is to be any hope of success.

Another approach is to determine that out‐of‐sample testing is a bit of a myth in the first place, especially for any experienced, observant researcher. As a result, out‐of‐sample testing is forgone in favor of a combination of extra vigilance regarding the in‐sample results, coupled with a minimum of parameter fitting. In this methodology, the quant uses as few parameters as possible, sets the values at some reasonable level, and simply tests the strategy and looks for all those metrics of good performance to have sufficiently high readings.

#### Assumptions of Testing

Another component in the testing process revolves around the assumptions one makes about trading a strategy that is being tested historically. We discuss two examples here: transaction costs and (for equity market‐neutral or long/short strategies) short availability.

We have already discussed transaction costs, of which there are several components: commissions and fees, slippage, and market impact. Interestingly, during the research process there is no empirical evidence of what a trading strategy would actually have cost to implement in the past. This is because the trading strategy wasn't actually active in the past but is being researched in the present using historical market data. Therefore, the researcher must make some assumption(s) about how much his order would really have cost in terms of market impact.

These assumptions can prove critical in determining whether a strategy is good or bad. Let's again look at an extreme case to understand why. Imagine that we assume that transactions are entirely costless. This might make a very high‐frequency trading strategy extremely appealing because, as long as it accurately predicts any movement in price, no matter how small, it will seem to have been worthwhile to trade. Imagine that a model is correct 55 percent of the time and makes \$0.01 per share when it is correct. It loses 45 percent of the time and loses \$0.01 per share when it is wrong. So, for every 100 shares it trades, it could be expected to generate \$0.10. But when it is implemented, it turns out that transactions actually cost \$0.01 per share across all the components of cost, on average. This would imply that the strategy is actually breakeven on 55 percent of its trades (theoretical profit of \$0.01 per share, less the cost of transacting each share of \$0.01) and loses \$0.02 per share on 45 percent of its trades. As a result, rather than making \$0.10 per 100 shares, it is in fact *losing* \$0.90, which is obviously a poor outcome. Stated generally, overestimating transaction costs will cause a quant to hold positions for longer than is likely optimal, whereas underestimating transaction costs will cause a quant to turn over his portfolio too quickly and therefore bleed from the excess costs of transactions. If we have to err in this regard, it makes more sense to overestimate cost than to underestimate, but it is always preferable to get the cost estimation approximately right.

The second kind of assumption a quant must make in testing a market‐ neutral or long/short strategy in equities relates to the availability of short positions. Imagine a U.S. market‐neutral quant trader who, by design, holds a short portfolio that is roughly equal in size to the long portfolio. Over time, the short portfolio adds a significant amount of value by finding overpriced stocks and by making money when the stock market tumbles, thereby reducing the risk inherent in the strategy. However, it turns out that the names the strategy wants to short, and in particular, the most successful short picks, are on *hard‐to‐borrow lists*. Hard‐to‐borrow lists are those stocks that are generally restricted from shorting by the broker, because the broker cannot mechanically locate shares to borrow, which is required in the act of shorting. If the shares cannot be located, the trade would be considered a *naked short sale*, which is illegal in the United States. Therefore, the trade wouldn't have been executed as expected by the back‐test. If the model is ignorant of hard‐to‐borrow issues (and making a model aware of this issue in the past is not trivial, since such historical data are hard to come by), the researcher can easily be fooled into thinking that the short portfolio will be able to deliver value that is, in reality, nonexistent. This is because when he goes to implement the live portfolio, he finds that he is unable to put on the best short trades and is forced to replace these with inferior short trades instead.

# Summary

We have only scratched the surface of the work that a quant must do in research, and must do well, to succeed over time. Research is a highly sensitive

![](_page_24_Figure_1.jpeg)

Exhibit 9.6 Schematic of the Black Box

area within the quant's process. It is where her judgment is most obviously and significantly impactful. Researchers must therefore go about their research with great care because this is the formative stage of a strategy's life. Mistakes made during research become baked into a strategy for its lifetime, and then the systematic implementation of this error can become devastating. Moreover, the research effort is not a one‐time affair. Rather, the quant must continually conduct a vigorous and prolific research program to produce profits consistently over time.

Models are, by definition, generalized representations of the past behavior of the market. More general models are more robust over time, but they are less likely to be very accurate at any point in time. More highly specified models have the chance to be more accurate, but they are also more likely to break down entirely when market conditions change. This trade‐off, between generality and specificity, between robustness and accuracy, is the key challenge faced by quant researchers. While there is no one‐size‐fits‐all answer that I'm aware of to address this challenge, I think Einstein's words provide the best guiding principle: "Everything should be made as simple as possible, but not simpler."

We have now completed our tour through the black box (see Exhibit 9.6), both its component models and the key elements—data and research—that drive it. The coming chapters will focus on the evaluation of quant traders and their strategies.

# Note

<sup>1.</sup> From Richard Donchian's Foundation website: www.foundationservices.cc/RDD2.