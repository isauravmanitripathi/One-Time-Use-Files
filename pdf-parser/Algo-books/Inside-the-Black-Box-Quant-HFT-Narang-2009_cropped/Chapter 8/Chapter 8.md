# CHAPTER

# Data

I'd sell you my kids before I'd sell you my data, and I'm not selling you my kids.

-Anonymous quantitative futures trader

The old adage is "garbage in, garbage out," meaning that if you use bad inputs, you'll get bad outputs. This relates to quant trading because most quants utilize some form of *input/output model*, which is a term that comes from computer science (and that has been borrowed by econometricians). It refers to the way in which information processors (such as computers) communicate with the world around them. One of the things we love about input/ output models is that if you provide the same input a million times, the output should be consistent every time. The process that transforms an input into an output is typically the part that people call the *black box* in quant trading, and we have seen the inside of this box in the preceding chapters. In this chapter, we examine the inputs of quant trading models, namely, the data they depend on.

Mechanically, data reach the black box through data servers, which are connected to one or more data sources. On receipt of these data, the black box processes them for use by the alpha, risk, transaction cost, portfolio construction, and execution models that constitute the internal organs of the quant trading machine. These data servers usually process data using software some quants call *data feed handlers*, which are designed to convert the data to a form in which they can be stored and utilized by the modules of the quant system.

#### THE IMPORTANCE OF DATA

It is difficult to overstate the importance of data, and it can be seen from many perspectives. First, data, as we know, are the inputs to quant trading systems. It turns out that the nature of the inputs to a system dictates what you can do with the system itself. For example, if you were handed a lot of lettuce, tomatoes, and cucumbers, it would be very difficult to build, say, a jet engine. Instead, you might decide that these inputs are most suited for making a salad. To make a jet engine, you more or less need jet engine parts, or at least materials that can handle high velocities and acceleration, high altitude, and a wide range of temperatures. The same is true with quant systems. To the extent that you are given data that focus on macroeconomic activity, it is extremely difficult to build a useful model that doesn't somehow reflect macroeconomic concepts.

Frequently, many details of the model itself are driven by characteristics of the inputs that are used. Refining our example, imagine that you are given slow‐moving macroeconomic data, such as quarterly U.S. GDP figures; furthermore, you receive them only a week after they are released to the public. In this situation, it is unlikely that you can build a very fast trading model that looks to hold positions for only a few minutes. Furthermore, note that the U.S. data you get might be useful for predicting bonds or currency relationships, but they might not be sufficient to build a helpful model of equity markets. U.S. GDP data will also tell you little about what is happening in Uruguay or Poland in any of their securities markets.

The nature of the data you are using is also an important determinant of the database technology you would rationally choose for storage and retrieval, a subject we will discuss in greater detail later in this chapter. Data sometimes even drive decisions about what types of hardware and software make the most sense. Again and again, we see that the nature of data—and even how they are delivered—determines a great deal about what can be done and how one would actually go about doing it.

Still another perspective on the importance of data can be understood by examining the consequences of *not* doing a good job of gathering and handling data. Returning to the idea that quant trading systems are input/ output models, if you feed the model bad data, it has little hope of producing accurate or even usable results. A stunning example of this concept can be seen in the failure of the Mars Climate Orbiter (MCO) in 1999. The \$200 million satellite was destroyed by atmospheric friction because one team of software engineers programmed the software that controlled the craft's thrusters to expect metric units of force (Newtons) while another team programmed the data delivered to the satellite to be in English units (pound‐force). The software model that controlled the satellite's thrusters ran faithfully, but because the data were in the wrong units (causing them to be off by a factor of almost 4.5 times), the satellite drifted off course, fell too close to Mars' surface, and ended up being destroyed. In the aftermath, National Aeronautic and Space Administration (NASA) management did not blame the software error but rather the process used to check and recheck the software and the data being fed to it.1

Problems, however, can be easy to miss. After all, the results frequently are numbers that can be seen to as many decimal places as you care to see. But this is *false precision.* That we have a number that goes out to several decimal places may not mean that we can rely on this number at all. Because the kind of trading with which we are concerned is all about timing, timeliness is critical. If you build a fantastic model to forecast the price of a stock over the next day, but you don't provide it data until a week later, what good is the model? This is an extreme example, but it is almost exclusively the case that the faster you can get accurate information into a good model, the better off you'll be, at least if succeeding is part of your plan.

Bad data can also lead to countless hours of squandered research and, in extreme cases, even to invalid theorization. Data are generally needed to develop a theory about the markets or anything else in science, just as physical scientists utilize their observations of the world to generate their theories. So, if we provide the scientist with incorrect information without her knowledge, she is likely to develop theories that are incorrect when applied to the real world. Bad data lead to bad outcomes. If the data have serious problems, it will be impossible to tell whether a system being tested, no matter how sophisticated the testing nor how elegant the model, is good or bad.

Many quant trading firms recognize this point in their behavior. Most of the best firms collect their own data from primary sources rather than purchasing it from data vendors. They also expend significant resources in the effort to speed up their access to data, to clean data, and even to develop better ways of storing data. Some firms have dozens or even hundreds of employees dedicated exclusively to capturing, cleaning, and storing data optimally.

## Types of Data

There are basically two kinds of data: *price data* and *fundamental data*. Price data are actually not solely related to the prices of instruments; they include other information received or derived from exchanges or transactions. Other examples of price data are the trading volumes for stocks or the time and size of each trade. Indeed, the entire order book, which shows a continuous series of all bids and offers for a given instrument throughout the course of a day as well as the amounts of each, would be considered price‐ related data. Furthermore, we would place anything that can be derived from the levels of various indices (e.g., percent changes computed from the daily values of the S&P 500 index) in the price‐related data category, even if the computed value is not a traded instrument.

The rather broad variety of fundamental data can make it difficult to categorize effectively. In a sense, fundamental data relate to anything besides prices. However, what all types of data have in common is that they are expected to have some usefulness in helping to determine the price of an instrument in the future, or at least to describe the instrument in the present. Also, we can do a bit more to create a reasonable taxonomy of fundamental data. The most common kinds of fundamental data are financial health, financial performance, financial worth, and sentiment. For single stocks, for example, a company's balance sheet is mostly used to indicate the financial health of the company. Meanwhile, for macroeconomic securities (e.g., government bonds or currencies), budget, trade deficit, or personal savings data might serve to indicate the financial health of a nation. Portions of the income and cash‐flow statements (e.g., total net profits or free cash flow) are used to determine financial performance; other portions are used to indicate financial health (e.g., ratios of accruals to total revenue or cash flow to earnings). Similarly, the U.S. GDP figure might be an example of macroeconomic financial performance data, whereas the trade balances figure is an example of macroeconomic financial health data. The third type of fundamental data relates to the worth of a financial instrument. Some common examples of this kind of data in the equities world are the book value or the amount of cash on hand. The last common type of fundamental data is sentiment. How analysts rate a stock, the buying and selling activity of company insiders, and information related to the implied volatility of the options on a stock are examples of sentiment data for stocks; economists' forecasts for GDP growth for next quarter are an example of macroeconomic sentiment data.

We don't want to oversimplify the matter. Clever researchers are constantly looking for new and innovative sources of information that might not be used by other players. Technology advances in the broader marketplace have greatly aided this kind of activity. For example, some firms (and now even some data vendors) quantitatively analyze news stories written in plain English. Quants can systematically parse these stories, extract quantifiable information, and build strategies around this type of data. However, this remains largely an exercise in getting faster and more robust indicators of sentiment (or other types of fundamentals already described), so we believe that sources such as this are still fundamental in nature. We know of at least one company that is attempting to use aggregated global positioning system (GPS) data to determine the level of various types of economic activity more quickly and accurately than is possible using government‐reported statistics. But this too seems to be a potential improvement (even a revolution) in the approach to collecting such data; the nature of the fundamental information being sought is by no means different than it was. This is not to diminish the ingenuity of those who developed such ideas. We simply point out that our classification scheme seems to do a reasonable job of explaining the kinds of data that exist.

An interesting pattern has emerged in our discussion of data. Much of what we saw in the price category of data tended to focus on shorter timescales. We spoke about daily values and even continuous intraday values. Meanwhile, in the fundamental category, we tend to see new information released on the scale of weeks, months, or quarters. One implication we can immediately discern from these differing periodicities is that, in general, trading strategies utilizing price‐related information have the option to be much faster than those utilizing primarily fundamental information. Again, this is simply because the information we have about the securities is refreshed more frequently with price‐related information than it usually is with fundamental data. This statement is not universal, since some fundamental strategies, especially those focused on changes in fundamentals or sentiment, can be very short‐term‐oriented. However, this statement holds most of the time and is a handy rule of thumb to bear in mind when looking at a quant strategy.

#### Sources of Data

One can get data from many sources. Most direct, but also perhaps most challenging, is to get raw data from the primary sources. In other words, a quant would get price data for stocks traded on the New York Stock Exchange directly from the NYSE. This has the benefit of allowing the quant maximum control over the cleaning and storing of data, and it can also have significant benefits in terms of speed. However, there is also a massive cost to doing things this way. It would require building connectivity to every primary source, and if we are speaking about trading multiple types of instruments (e.g., stocks and futures) across multiple geographical markets and exchanges, the number of data sources can explode. With each, software must be built to translate the primary sources' unique formats into something usable by the quant's trading systems.

Examples of the kinds of primary sources and data types include:

- **Exchanges.** Prices, volumes, timestamps, open interest, short interest, order book data.
- **Regulators.** Financial statements from individual companies, filings related to large owners of individual stocks as well as insider buying and selling activities.

- **Governments.** Macroeconomic data, such as employment, inflation, or GDP data.
- **Corporations.** Announcements of financial results and other relevant developments (e.g., changes in dividends).
- **News agencies.** Press releases or news articles.
- **Proprietary data vendors (or data generators).** House‐created data that might be of interest. For example, brokerage firms frequently issue reports about companies, and some firms track and license investment funds‐flow data.

Because of the scope of the work involved in accessing data directly from primary sources, many firms use secondary data vendors to solve some aspects of the data problem. For example, some data vendors take financial statement data from regulatory filings around the world and create quantified databases that they then license to quant traders. In this example, the data vendor is being paid for having solved the problem of building a consistent framework to house and categorize data from many direct sources. But imagine that the quant firm wants to collect both price and fundamental data about companies around the world. It is frequently the case that entirely different companies provide each of these types of data. For instance, for a given stock, there may be one data vendor providing price data and a completely different one providing fundamental data. These data vendors may also differ in the way they identify stocks. One might use the ticker; another might use a SEDOL code or some other identifier.2 With two or more different data sets regarding the same security, the quant will have to find a way to ensure that all the data ultimately find their way into the same company's record in the quant's internal database. The tool used to help with this is frequently called a *security master* in that it is the master file mapping the various ways that data vendors identify stocks to a single, unique identifier method that the quant will use in her trading system.

As you may have guessed, still other firms have cropped up to provide unified databases across many types of vendors and data types. These we can call *tertiary data vendors*, and they are paid to make data easy to access for the quant. They establish connections with many primary and secondary data vendors, build and maintain security masters, and even perform some data‐cleaning activities (a subject we will discuss in more detail presently). As a result, they are immensely popular among many firms. However, we should make it clear that as much benefit as they offer in terms of ease, tertiary data vendors do add another layer between the quant and the original data. This layer can result in loss of speed and possibly in less control over the methods used to clean, store, or access data on an ongoing basis.

#### Cleaning Data

Having established the types and importance of data, we now turn to the kinds of problems quants face in managing these raw materials and how they handle such flaws. Despite the efforts of primary, secondary, and sometimes even tertiary data vendors, data are often either missing or incorrect in some way. If ignored, this problem can lead to disastrous consequences for the quant. This section addresses some of the common problems found with errors and some of the better‐known approaches used to deal with these challenges. It's worth noting that although some of the following data problems seem egregious or obvious to a human, it can be challenging to notice such problems in a trading system that is processing millions of data points hourly (or even within one minute, as in the case of high‐frequency traders).

The first common type of data problem is missing data, as we alluded to already. Missing data occur when a piece of information existed in reality but for some reason was not provided by the data supplier. This is obviously an issue because without data, the system has nothing to go on. Worse still, by withholding just some portion of the data, systems can make erroneous computations. Two common approaches are used to solve the problem of missing data. The first is to build the system so that it "understands" that data can in fact go missing, in which case the system doesn't act rashly when there are no data over some limited time period. For example, many databases automatically assign a value of zero to a data point that is missing. After all, zero and nothing have a lot in common. However, there is a very different implication to the model thinking the price is now zero (for example, if we were long the instrument, we'd be showing a 100 percent loss on the position) versus thinking that the price is unknown at the moment.

To fix this problem, many quants program their database and trading systems to recognize the difference between zero and blank. This frequently means simply using the last known price until a new one is available. The second approach is to try to interpolate what a reasonable value might be in place of the missing data. This is useful for historical data rather than real‐time data, but a variation of the method described here can be used for real‐time data as well.

Let's take an example of a semiconductor company's listed stock. Imagine that we know the price of a semiconductor stock immediately before and immediately after the missing data point (this is why this technique is mainly useful to back‐fill missing data points in a database). We could simply interpolate the price of the stock as being midway between the price immediately before and immediately after the gap. Imagine further that we know how the stock index, the tech sector, the semiconductor industry, and some close competitors performed for the period that is missing. By combining information about the periods around the missing point and the action of related things during the missing period, it is possible to compute a sensible value for the stock's missing data point. Though we aren't guaranteed and in fact aren't terribly likely to get the number exactly right, at least we have something reasonable that won't cause our systems problems.

A second type of data problem is the presence of incorrect values. For instance, decimal errors are a common problem. To take the example of U.K. stocks, they are sometimes quoted in pounds and sometimes in pence. Obviously, if a system is expecting to receive a figure in pounds and it receives a number that doesn't advertise itself as being anything other than pounds, problems can abound. Instead of being quoted as, say, £10, it is quoted as 1,000; that is, 1,000 pence. This can result in the model being told that the price has spiked dramatically upward, which can cause all sorts of other mayhem (for example, a naive system without data checks might want to short the stock aggressively if it suddenly and inexplicably jumped 100‐fold in an instant). Alternatively, a price might simply be wrong. Exchanges and other sources of data frequently put out *bad prints,* which are data points that simply never happened at all or at least didn't happen the way the data source indicates.

By far the most common type of tool used to help address this issue is something we call a *spike filter*. Spike filters look for abnormally large, sudden moves in prices and either smooth these out or eliminate them altogether. Further complicating the matter, it should be noted that sometimes spikes really do happen. In these circumstances, a spike filter may reject a value that is valid, either ignoring it or replacing it with an erroneous value. An interesting example of this is shown in Exhibit 8.1. In this case, during the trading day of July 15, 2008, the U.S. dollar's exchange rate with the Mexican peso quickly fell about 3 percent, then regained virtually all that ground in a matter of seconds.

![](_page_7_Figure_4.jpeg)

Exhibit 8.1 September 2008 Mexican Peso Futures Contract on July 15, 2008

This behavior is not reserved for less commonly traded instruments, however. The 10-year German bund, one of the more liquid futures contracts in the world, dropped about 1.4 percent in a few seconds during the day of March 28, 2008, only to recover immediately (see Exhibit 8.2).

A spike filter might well have called this a bad print, but it really happened. To reduce the impact of this problem, some quants use spike filters to alert a human supervisor to look into the matter further, and the human can then decide, based on what she sees as the facts, on what to do about the strange price. Still another common approach, though useful only if there is more than one source for a given piece of data, is to cross-check a data set given by one provider against one provided by a second source. If they match, it is more likely to be a correct price. If they do not match, one or both of them must be wrong. Of course, what to do when two vendors don't match each other is a whole other ball of wax. A final common approach to cleaning data problems is to utilize the same approach as described earlier in addressing the problem of missing data by looking to the points before and after the "bad" data point and/or by looking to the behavior of related instruments to interpolate an approximate value.

Another very common type of data error relates to corporate actions such as splits and dividends. Imagine a ticker that splits 3:1. Generally, the price drops by about two-thirds to offset the threefold increase in the number of shares.<sup>3</sup> Imagine that the data vendor doesn't record this as a split, and therefore doesn't adjust the back-history to reflect this corporate action. In this scenario, the quant trader's system may be misled to believe that the stock simply dropped 67 percent overnight. This is generally handled by independently tracking corporate actions, together with the humanoversight version of a spike filter, described previously.

![](_page_8_Figure_5.jpeg)

EXHIBIT 8.2 June 2008 German Bund Futures Contract on March 28, 2008

Another frustrating problem is that the data sometimes contain incorrect timestamps. This is generally a problem with intraday or real‐time data, but it has been known to be an issue with other data as well. This is also one of the tougher problems to solve. Obviously, the path of a time series is fairly important, especially since the goal of the quant trader focused on alpha is to figure out when to be long, short, or out of a given security. As such, if the time series is shuffled because of an error in the data source, it can be deeply problematic. A quant researcher could believe her system works when in reality it doesn't,4 or she could believe her system doesn't work when in reality it does.5 If the quant trading firm stores its own data in real time, it can track timestamps received versus the internal clocks of the machines doing the storing and ensure that there are correct timestamps, which is perhaps the most effective way of addressing this issue. But to do so requires storing one's own data reliably in real time and writing software to check the timestamp of each and every data point against a system clock in a way that doesn't slow the system down too much, making this a difficult problem to address. It should be noted that this approach only works for those quants who capture and store their own data in real time. For those that are relying on purchased databases, they can only cross‐check data from various sources.

Finally, a more subtle type of data challenge bears mentioning here. This is known as look‐ahead bias and is a subject to which we will devote attention several times in this book. *Look‐ahead bias* refers to the problem of wrongly assuming that you could have known something before it would have been possible to know it. Another way to phrase this is "getting yesterday's news the day before yesterday." We will examine look‐ahead bias in the chapter on research, but for now, let's examine a particular form of this bias that comes from the data. Specifically, it derives from asynchronicity in the data.

A common example of asynchronicity can be found in the regulatory filings of financial statements (known as 10‐Qs) made by companies each quarter in the United States. Companies report their financial statements *as of* each quarter end. However, these reports are usually released four to eight weeks after the end of the quarter. Let's imagine the first quarter of 2010 has just ended. On May 1, 2010, Acme Concrete Inc. reports that its first‐ quarter earnings were \$1 per share as of March 31 and furthermore that the general analyst community was expecting only \$0.50 per share, making the result a strongly positive surprise. Once the data point is available, most data vendors will report that Acme's earnings per share were \$1 per share as of March 31, even though the number wasn't released until May 1.

Three years later, a quant is testing a strategy that uses earnings data from this vendor. The data indicate that Acme's earnings were \$1 per share for the quarter ending March 31, and her model assumes this to be true, even though in reality she would never have been able to know this until the estimate was released a month later, on May 1. In the back‐test, she sees that her model buys Acme in April because its P/E ratio looks appealing from April 1 onward, given the \$1‐per‐share earnings result, even though the model would not have known about the \$1 earnings figure until May 1 if she had been trading back then. Suddenly the strategy makes a huge profit on the position in early May, when the world, and her model, actually would have found out about the earnings surprise. This kind of problem also happens with macroeconomic data (such as the unemployment rate), which frequently get revised some months after their initial release. Without careful tracking of the revision history for such data, the quant can be left with the same issue as demonstrated in the equity example: believing that she could have had revised data in the past when in fact she would only have had the less accurate initial data release.

If the quant ignores this data error, she can end up making a Type I error again: believing that her strategy is profitable and sound, even though it may in fact only look that way because she's made a substantial data error. To address look‐ahead bias in the data, quants can record the date at which new information is actually made available and only make the data available for testing at the appropriate time. In addition, quants can put an artificial lag on the data they are concerned about so that the model's awareness of this information is delayed sufficiently to overcome the look‐ahead bias issues. Note that look‐ahead issues with regard to data are specific to research, which we will discuss further in the next chapter. In live trading, there is no such thing as look‐ahead bias, and in fact quants would want all relevant data to be available to their systems as immediately as possible.

Another type of look‐ahead bias stemming from asynchronicity in the data is a result of the various closing times of markets around the world. The SPY (the ETF tracking the S&P 500) trades until 4:15 p.m., whereas the stocks that constitute the S&P 500 index stop trading at 4:00 p.m. European stock markets close from 11:00 a.m. to 12:00 p.m., New York time. Asian markets are already closed on a given day by the time New York opens. In many cases, the considerable impact that U.S. news and trading activity have on European or Asian markets cannot be felt until the next trading day.

On Friday, October 10, 2008, for example, the Nikkei 225 fell more than 9 percent for the day. But it was already closed by the time New York opened. European markets closed down between 7 and 10 percent for the same day. At the time of Europe's closing, the S&P 500 was down about 6 percent for the day. Suddenly, however, just after 2:00 p.m. EST on the 10th, with two hours remaining in U.S. trading but the rest of the world already gone for the weekend, the S&P 500 rallied, closing down just over 1 percent. Monday the 13th was a market holiday in Japan. Europe tried to make up ground that Monday, with the key markets closing up over 11 percent but the U.S. market up "only" about 6 percent by midday in New York. However, by the end of the trading day, the U.S. market closed up over 11 percent as well, leaving the European markets behind again. The next day, the Nikkei reopened on the 14th and ended up 14 percent. On their subsequent day, European markets closed up about 3 percent, whereas the U.S. market was down slightly by the end of its own trading day. Ignoring this kind of asynchronicity can be extremely problematic for analyses of closing price data because these closing prices occur at different times on the same day.

These are but a few examples of the many subtle ways in which look‐ ahead bias seeps into the process of research and money management, even for discretionary traders. A key challenge for the quant is deciding how to manage this problem in its myriad forms.

#### Storing Data

Databases are used to store collected data for later use, and they come in several varieties. The first type of database is known as the *flat file*. Flat files are two‐dimensional databases, much like an ordinary spreadsheet. Flat file databases are loved for their leanness, because there is very little baggage or overhead to slow them down. It is a simple file structure that can be searched very easily, usually in a sequential manner (i.e., from the first row of data onward to the last). However, you can easily imagine that searching for a data point near the bottom row of a very large flat file with millions of rows may take rather a long time. To help with this problem, many quants use *indexed flat files*, which add an extra step but which can make searching large files easier. The index gives the computer a sort of cheat sheet, providing an algorithm to search large sets of data more intelligently than a sequential search.

A second important type of data storage is a relational database. Relational databases allow for more complex relationships among the data set. For example, imagine that we want to keep track of stocks not just on their own but also as part of industry groups, as part of sectors, as part of broader indices for the countries of their domicile, and as part of the universe of stocks overall. This is a fairly routine thing to want to do. With flat files, we would have to construct each of these groups as a separate table. This is fine if nothing ever changes with the constituents of each table. But in reality, every time there is a corporate action, a merger, or any other event that would cause us to want to modify the record for a single stock in any one of these tables, we have to remember to update all of them. Instead, in the world of relational databases, we can simply create a database table that contains *attributes* of each stock—for example, the industry, sector, market, and universe it is in. Given this table, we can simply manage the table of information for the stock itself and for its attributes. From there, the database will take care of the rest based on the established relationship. Though relational databases allow for powerful searches, they can also be slow and cumbersome because their searches can span many tables as well as the meta tables that establish the relationships among the data tables.

An important type of relational database is known as a *data cube*, a label I have borrowed from Sudhir Chhikara, the former head of quantitative trading at Stark Investments. Data cubes force consistency into a relational database by keeping all the values for all the attributes of all instruments in a single, three‐dimensional table. For a given date, then, all instruments would be listed in one axis of this table. A second axis would store all the values for a given attribute (e.g., closing price for that date) across the various instruments. The third axis would store other attributes (e.g., earnings per share as of that date). This method has the benefit of simplifying the relationships in a way that is rather useful. In other words, it hardwires certain relationships; furthermore, by keeping all attributes of each instrument available every day, there is no need to search for the last available data point for a given attribute and security. For every day, a data cube is created to store all the relevant data. This approach, too, has its potential disadvantages. Hardwiring the relationships leads to inflexibility, so if the nature of the relationships or the method of querying the data changes, it can be problematic.

Each of these data storage approaches has advantages and disadvantages. It would be easy to make some assumptions and declare one the "best," but the reality is that the best technique is dependent on the problem that needs to be solved. Here, as in so many other parts of the black box, the quant's judgment determines success or failure.

#### Summary

In this chapter, we explained some of the basic concepts of data for use by quant trading systems. Though data are scarcely the most exciting part of a quant strategy, they are so integral and critical to everything quants do and inform so much of how to think about a given quant system that they are well worth understanding.

Next we will dive into the research process as our final stop in the exploration of the black box (Exhibit 8.3).

![](_page_13_Figure_1.jpeg)

![](_page_13_Figure_2.jpeg)

## Notes

- 1. Greg Clark and Alex Canizares, "Navigation Team Was Unfamiliar with Mars Climate Orbiter," Space.com, November 10, 1999.
- 2. SEDOL stands for *Stock Exchange Daily Official List*, which is a list of ostensibly unique security identifiers for stocks in the United Kingdom and Ireland. Other common security identifiers in equity markets are the International Securities Identification Number (ISIN) or Committee on Uniform Security Identification Procedures (CUSIP) number. CUSIPs are primarily relevant for U.S. and Canadian stocks. Many data vendors utilize their own proprietary security identifiers as well.
- 3. For the sake of simplicity, we are ignoring any *split effect*, which many people believe exists; this theory states that stocks tend not to fall as much as expected based on the size of the split because people like to buy nominally lower‐priced stocks.
- 4. In science, this is known as a *Type I error*, which is to accept a false‐positive result in testing a hypothesis. This is the error of believing a hypothesis is true when in fact it is false.
- 5. In science, this is known as a *Type II error*, which is to accept a falsely negative result in the outcome of a test. This is the error of believing a hypothesis is false when in fact it is true.