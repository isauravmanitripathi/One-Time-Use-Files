CHAPTER

# Portfolio Construction Models

No sensible decision can be made any longer without taking into account not only the world as it is, but the world as it will be. -Isaac Asimov

The goal of a portfolio construction model is to determine what portfolio the quant wants to own. The model acts like an arbitrator, hearing the arguments of the optimist (alpha model), the pessimist (risk model), and the cost-conscious accountant (transaction cost model), and then making a decision about how to proceed. The decision to allocate this or that amount to the various holdings in a portfolio is mostly based on a balancing of considerations of expected return, risk, and transaction costs. Too much emphasis on the opportunity can lead to ruin by ignoring risk. Too much emphasis on the risk can lead to underperformance by ignoring the opportunity. Too much emphasis on transaction costs can lead to paralysis because this will tend to cause the trader to hold positions indefinitely instead of taking on the cost of refreshing the portfolio.

Quantitative portfolio construction models come in two major forms. The first family is rule based. Rule-based portfolio construction models are based on heuristics defined by the quant trader and can be exceedingly simple or rather complex. The heuristics that are used are generally rules that are derived from human experience, such as by trial and error.

The second family of quantitative portfolio construction models is optimized. Optimizers utilize algorithms—step-by-step sets of rules designed to get the user from a starting point to a desired ending point—to seek the best way to reach a goal that the quant defines. This goal is known as an *objective function*, and the canonical example of an objective function for an optimizer is to seek the portfolio that generates the highest possible return for a unit of risk. By their nature, optimizers can be more difficult to understand in great detail, but they are straightforward conceptually.

As in the case of blending alpha models, discussed in Chapter 3, portfolio construction models are a fascinating area to study. Furthermore, portfolio construction turns out to be a critical component of the investment process. If a trader has a variety of investment ideas of varying quality but allocates the most money to the worst ideas and the least money to the best ideas, it is not hard to imagine this trader delivering poor results over time. At a minimum, his results would be greatly improved if he could improve his approach to portfolio construction. And yet, actual solutions to the problem of how to allocate assets across the various positions in a portfolio are not exceedingly common. This subject receives rather a lot less time and space in the academic journals and in practitioners' minds than ways to make a new alpha model, for example. This chapter will give you the ability to understand how most quant practitioners tackle this problem.

# Rule-Based Portfolio Construction Models

There are four common types of rule‐based portfolio construction models: equal position weighting, equal risk weighting, alpha‐driven weighting, and decision‐tree weighting. The first two are the simplest and have at their core a philosophy of equal weighting; they differ only in what specifically is being equally weighted. Alpha‐driven portfolio construction models mainly rely on the alpha model for guidance on the correct position sizing and portfolio construction. Decision‐tree approaches, which look at a defined set of rules in a particular order to determine position sizing, can be rather simple or amazingly complex. I describe these approaches from simplest to most complex.

## Equal Position Weighting

Equal position‐weighted models are surprisingly common. These models are used by those who implicitly (or explicitly) believe that if a position looks good enough to own, no other information is needed (or even helpful) in determining its size. There is a further implicit assumption that the instruments are homogeneous enough that they do not need to be distinguished on the basis of their riskiness or otherwise. The notion of the strength of a signal, which, as already discussed, is related to the size of a forecast for a given instrument, is ignored except insofar as the signal is strong enough to be worthy of a position at all. At first glance, this might seem like an oversimplification of the problem. However, some serious quants have arrived at this solution. The basic premise behind an equal-weighting model is that any attempt to differentiate one position from another has two potentially adverse consequences, which ultimately outweigh any potential benefit from an unequal weighting. In other words, they choose an equal‐weighting model because of the many disadvantages they see in unequal weighting.

The first potential problem with unequal weighting is that it assumes implicitly that there is sufficient statistical strength and power to predict not only the direction of a position in the future but also the magnitude and/ or probability of its move relative to the other forecasts in the portfolio. Quants utilizing equal‐weighting schemes believe, instead, that the alpha model is only to be trusted enough to forecast direction, and as long as there is sufficient confidence in a forecast of direction that is sufficiently large to justify trading the instrument at all, it is worth trading at the same size as any other position.

The second potential problem with unequal weighting of a portfolio is that it generally leads to a willingness to take a few large bets on the "best" forecasts and many smaller bets on the less dramatic forecasts. This weighting disparity, however, may lead to the strategy's taking excess risk of some idiosyncratic event in a seemingly attractive position. This can be the case regardless of the type of alpha used to make a forecast. For instance, in momentum‐oriented strategies, many of the strongest signals are those for which the underlying instrument has already moved the most (i.e., has showed the strongest trending behavior). In other words, it might be too late, and the trader risks getting his strongest signals at the peak of the trend, just as it reverses. Similarly, for mean reversion–oriented strategies, many of the largest signals are also for those instruments that have already moved the most and are now expected to snap back aggressively. But frequently, large moves happen because there is real information in the marketplace that leads to a prolonged or extended trend. This phenomenon is known to statisticians as *adverse selection bias*. Mean reversion bets in these situations are characterized as "picking up nickels in front of a steamroller," which is a colorful way of saying that betting on a reversal against a very strong trend leads to being run over if the trend continues, which it often does.

This last benefit can be seen in other scenarios as well. While practitioners do what they can to clean the data they utilize in trading (which we discuss further in Chapter 8), there are occasions in which bad data points end up filtering into a trading strategy. Equal weighting positions, in particular if there are many of them, ensure that the risk of loss associated with the large forecasts that could result from significantly wrong data does not get out of hand. For example, if a stock price is off by a factor of 100 (e.g., it is quoted in pence instead of pounds, as happens occasionally in U.K. equities), it is likely that an alpha model might be fooled into wanting to take an enormous position in this ticker. An equal-weighting scheme can reduce the size of that position such that it does not end up a catastrophic event.

Indeed, since alphas are generally tested against real datasets, most of their statistical significance and strength generally come from the meat of a distribution, not from the tails. If we observe a real tail event (not just some accident of a bad data point) that drives a large alpha forecast, this is perhaps a better trade, but it almost certainly involves dramatically higher risk than a more normative level of alpha. Here, too, an equal position weighting scheme can control the risk associated with such tail observations.

Analogous arguments can be made for almost all alpha strategies, making it easy to construct good arguments against unequal‐weighting positions. Therefore, the basic argument in favor of an equal‐weighted approach is one of mitigating risk by diversifying bets across the largest useful number of positions. It is worth mentioning that equal weights are sometimes subject to constraints of liquidity, in that a position is weighted as close to equally as its liquidity will allow. Such liquidity considerations can be applied to each of the other rule‐based allocation methodologies discussed in this chapter.

## Equal Risk Weighting

Equal risk weighting adjusts position sizes inversely to their volatilities (or whatever other measure of risk, such as drawdown, is preferred). More volatile positions are given smaller allocations, and less volatile positions are given larger allocations. In this way, each position is equalized in the portfolio, not by the size of the allocation but rather by the amount of risk that the allocation contributes to the portfolio. An example is shown in Exhibit 6.1, which shows an example of a two‐stock portfolio. As you can see, the more volatile stock (GOOG) gets a smaller allocation in the portfolio than the less volatile stock (XOM).

The rationale is straightforward. A small‐cap stock with a significant amount of price volatility might not deserve quite the same allocation as a mega cap stock with substantially less volatility. Putting an equal number of dollars into these two positions might in fact be taking a much larger and inadvertent *real* bet on the small cap stock. This is because the small cap stock is much more volatile, and therefore every dollar allocated to that

|      | Equal Weight | Volatility | Volatility‐Adjusted Weight |
|------|--------------|------------|----------------------------|
| GOOG | 50%          | 2.5%       | 39%                        |
| XOM  | 50%          | 2.0%       | 61%                        |

Exhibit 6.1 A Simple Equal Risk–Weighted Portfolio

stock would move the portfolio more than the same dollars allocated to the larger cap (and, likely, less volatile) position. As such, some quants who believe that equal weighting is the most appropriate method will utilize an equal risk–weighting approach in an effort to improve the true diversification achieved.

However, the equal risk–weighting approach also has its shortcomings. Whatever unit of risk is equalized, it is almost always a backward‐looking measurement, such as volatility. Instruments with higher volatilities would have smaller allocations, whereas lower‐volatility instruments would have larger allocations. But what if the less volatile instruments suddenly became the more volatile? This is not merely a hypothetical question. For many years, bank stocks were very stable. Then, in 2008, they suddenly became highly volatile, more so even than many technology stocks. Any backward‐ looking analysis of the volatility of stocks that didn't emphasize the last debacle among financial stocks (10 years earlier, in 1998) would likely have been misled by the steady behavior of these stocks for the decade prior to 2008, and therefore an equal‐risk model is likely to have held much larger positions in banks than were warranted once volatility spiked in 2008.

## Alpha-Driven Weighting

A third approach to rule‐based portfolio construction determines position sizes based primarily on the alpha model. The idea here is that the alpha model dictates how attractive a position is likely to be, and this signal is the best way to size the position correctly. Still, most quants who utilize this approach would not allow the size of the largest position to be unlimited. As such, they would use the risk model to provide a maximum size limit for a single position. Given the limit, the strength of the signal determines how close to the maximum the position can actually be. This is much like grading on a curve, where the best score receives the largest position size, and the scores below the best receive smaller sizes. The types of constraints used with this approach to portfolio construction can also include limits on the size of the total bet on a group (e.g., sector or asset class).

For example, one could constrain individual positions to be less than 3 percent of the portfolio and each sector to be less than 20 percent. There still needs to be a function that relates the magnitude of the forecast to the size of the position, but these functions can be straightforward, and in general, the bigger the forecast, the larger the position. Alpha weighting is favored by some quants because it emphasizes making money, which is after all the goal of the whole exercise. However, some quant strategies, such as futures trend following, that utilize this method can suffer sharp drawdowns relatively frequently. This is because these models usually have the largest signals when a price trend is already well established. As the trend proceeds, the size of the position grows, but this will often leave the trader with his largest position just when the trend reverses. Caution is therefore advisable when utilizing an alpha‐driven portfolio construction algorithm, because such an approach causes a heavy reliance on the alpha model being right—not only about its forecast of the direction of an instrument but also about the size of the move the instrument will make.

## Summary of Rule-Based Portfolio Construction Models

Regardless of which type of rule‐based portfolio construction model is used, the alpha model, risk model, and t‐cost model can be incorporated in portfolio building. In an equal‐weighted model, for example, constraints on the equal weighting can exist because certain instruments are too expensive to transact in, according to the transaction cost model. These considerations can be accounted for within the alpha model itself, for example by adding a conditioning variable that sets the expected return (or score, or whatever other form of forecast) to "0" if the expected return is less than the expected transaction cost threshold. Thus, any signal that comes out of the alpha model can now be equally weighted. Obviously, the exact nature of the interaction between the other components of the black box and the portfolio construction model depends entirely on the type of portfolio construction model. For example, an equal-weighting approach may make use of a risk model in an entirely different way from an alpha‐weighting approach.

To summarize, rule‐based portfolio construction models can be extremely simple (as in the case of an equal‐weighted portfolio) or rather complex (in the case of an alpha‐weighting with many types of constraints). The challenge common to all of them is to make the rules that drive them rational and well‐reasoned.

# Portfolio Optimizers

Portfolio optimization is one of the most important topics in quantitative finance. This is one of the first areas in quant finance to receive the attention of serious academic work; in fact, the case could easily be made that the father of quantitative analysis is Harry Markowitz, who published a landmark paper entitled "Portfolio Selection."1 He invented a technique known as *mean variance optimization*, which is still ubiquitous today, though much sophistication has been built around its core. In 1990, he shared a Nobel Prize with William Sharpe for both their contributions to the understanding of the quantitative analysis of portfolio construction.

Portfolio optimizers are based on the principles of *modern portfolio theory* (MPT), which are canonical in the asset management industry. The core tenet of MPT is that investors are inherently risk averse, meaning that if two assets offer the same return but different levels of risk, investors will prefer the less risky asset. A corollary is that investors will take on extra risk only if they expect to receive extra return as compensation. This introduced the concept of *risk‐adjusted return*. *Mean variance optimization* is a formal way of building portfolios based on MPT. Mean and variance are two of the inputs to the optimizer, and the output is a set of portfolios that have the highest return at each level of risk. The *mean* in question is the average expected return of each asset being evaluated. *Variance* is a proxy for the expected risk of each asset and is computed as the standard deviation of the returns of the various assets one is considering owning. A third input to the optimizer is the *expected correlation matrix* of these same assets. Using these inputs, the optimizer delivers a set of portfolios that offer the highest possible return for various levels of risk, known as the *efficient frontier*.

Quant trading strategies that utilize risk and transaction cost models, in addition to alpha models, also need to account for the information contained in (and any constraints associated with) those models. For example, the portfolio optimizer might be required to solve for the optimal (i.e., maximum risk‐adjusted return) portfolio, which accounts for the expected returns of each potential holding, the variability of those holdings, the correlation of those holdings to one another, and which minimizes exposure to various prespecified risk factors as specified in the risk model. Several additional inputs are utilized by quants in real trading applications, including (a) the size of the portfolio in currency terms; (b) the desired risk level (usually measured in terms of volatility or expected drawdown); and (c) any other constraints, such as a *hard‐to‐borrow list* provided by a prime broker in equity trading, which reduces the size of the universe with which the optimizer can work. These inputs are not required by the optimizer, and the first two are also mostly arbitrary, but they help yield a portfolio that is practical and useful to the quant trader.

The reason this technique is known as optimization is that it seeks to find the maximum (optimal) value of a function that has been specified by the researcher. This function is known as the *objective function*, where *objective* is used in the sense of *goal*. The optimizer seeks this goal by an algorithm that conducts a directed search among the various combinations of instruments available to it. As it examines the return and risk characteristics of a given combination, it compares this with previously examined combinations and detects what seems to cause the portfolio's behavior to improve or degrade. By this method, the optimizer is able to rapidly locate a series of optimal portfolios, which are those for which returns cannot be bested

by those of any other portfolio at a given level of risk. What is allowed or disallowed is determined by the alpha model, risk model, and transaction cost model. The objective function that many quants use is the same as the original: maximizing the return of a portfolio relative to the volatility of the portfolio's returns. However, an infinite array of objective functions can be used. For example, one could specify an objective function that will cause the optimizer to maximize portfolio return relative to peak-to-valley drawdown instead of return volatility. The use of return versus risk is itself entirely optional, and one could very easily optimize an objective function focused entirely on the total expected return of a portfolio.

We can graphically illustrate the technique of optimization as shown in Exhibit 6.2. Here, we see that on the  $X$  (horizontal) and  $Z$  (depth) axes of the graph are every possible combination of ownerships of two imaginary instruments, ABC and DEF. The Y-axis (vertical) shows the expected Sharpe ratio of each possible portfolio containing ABC and DEF. The Sharpe ratio was chosen simply for illustrative purposes, as a typical objective function for an optimizer. Imagine further that we have a positive return expectation (forecast) on ABC, and an equal but negative forecast for DEF. The optimizer searches for which portfolio produces the maximum value for the objective function, which in this case is to be 100 percent long ABC and

![](_page_7_Figure_3.jpeg)

EXHIBIT 6.2 Visual Representation of the Search Space for an Optimization

100 percent short DEF. The optimizer obviously does not look at a graph to pick the point, but this visual can be helpful in illustrating what an optimizer is attempting to achieve.

# Inputs to Optimization

The inputs required for an optimizer, as already mentioned, are expected returns, expected volatility, and a correlation matrix of the various instruments to be considered for the portfolio. It is worth understanding where practitioners get the estimates and expectations used in optimization, since they are critical to the model itself. We consider each of the aforementioned inputs in order.

Expected Return In more traditional finance, such as private wealth management, expected returns are usually set to equal very long‐term historical returns because usually the goal is to create a strategic asset allocation that won't need to be dynamically readjusted. By contrast, quants tend to use their alpha models to drive expected return. As we mentioned in our discussion of alpha models, the output of the alpha model typically includes an expected return and/or an expected direction, or some other output that indicates the attractiveness of each potential portfolio holding (e.g., a score). Forecasts of direction can be used as forecasts of return simply by making all positive forecasts equal and all negative forecasts equal (often subject to minimum threshold parameters, so that at least the return forecasts have to be of some significant size before making a bet). In this kind of optimization, it is not important to have a precise forecast of return, but rather a forecast of the attractiveness of each potential position in terms of the expected return. So directional forecasts are indifferent between the expected return of each position, and the only relevant feature of the forecast is its sign.

Expected Volatility Many practitioners, whether in traditional finance or in quant trading, tend to use historical measures for the second input to the optimizer, namely volatility. Some, however, develop and use their own forecasts of volatility. The most common approaches to forecasting volatility utilize stochastic volatility models. Stochastic, in Greek, means *random*. In statistics, a stochastic process is one that is somewhat predictable but that has some element of unpredictability or randomness built in. In case you're wondering what statisticians mean by the word *process*, they are referring to some continuous series of changes, which is basically a synonym for a time series in this context. The basic idea behind the stochastic family of volatility forecasting methods is that volatility goes through phases in which it is at high levels, followed by periods in which it is at low levels (i.e., the somewhat predictable phases of the volatility cycle), with occasional jumps (the somewhat random and unpredictable part). The most widely used such technique is called Generalized Autoregressive Conditional Heteroskedasticity (GARCH), which was proposed in 1986 in the *Journal of Econometrics* by the Danish econometrician Tim Bollerslev.2 Other approaches to stochastic volatility modeling and variants of the original GARCH forecast abound. All these techniques basically share the notion that volatility goes through clustered periods of relative calm, followed by periods of swings, followed by a return to calm, and so forth. This can be seen in Exhibit 6.3 as being a relatively useful way to describe market volatility. From 2000 to 2003, the S&P 500 was rather volatile. This was followed by a period of calm from mid‐2003 to mid‐2007, and after that by another period of extreme volatility from mid‐2007 through 2008. Even during the relatively calm period, short, seemingly periodic bursts in volatility occurred. GARCH types of models do a reasonable job of forecasting volatility in this sort of pattern.

Indeed, there exist many other approaches to forecasting volatility, and they can be understood in much the same way that we evaluated strategies for forecasting price. They tend to make forecasts based on ideas of trend, reversion, or some fundamental model of volatility; they can be made over various time horizons; they can forecast either the volatility of a single instrument or the relative volatility of more than one instrument, and so forth.

![](_page_9_Figure_3.jpeg)

Exhibit 6.3 Historical S&P 500 Volatility

GARCH forecasts, for example, are a way of understanding how a time series behaves. The "A" in the acronym GARCH stands for Autoregressive, which is a statistical term that characterizes a mean reverting process. A negative value for autoregression implies that the time series exhibits trending behavior (which is also called autocorrelative). In this case, the time series relates to the volatility of an instrument.

Expected Correlation The third input to the optimizer is the correlation matrix. Correlation is at heart a measure of the similarity of the movements of two instruments, expressed in a number between –1 and +1. A +1 correlation implies exact similarity, whereas a –1 correlation implies that the two instruments are exactly opposite, or *anti‐*correlated. A 0 correlation is perfect *non*‐correlation and implies that the two instruments are entirely dissimilar, but not opposite. An interesting fact about correlation is that it says nothing about the trend in the instruments over time. For example, imagine two companies in the same industry group, such as airline companies. If the first company is simply outcompeting the other and winning market share, the first will likely have a positive trendline, while the second may well have a negative trendline (assuming the overall market is roughly flat). Nevertheless, these two companies will likely have a high positive correlation, because their returns are still driven heavily by the overall market, by their sector, and by their industry, not to mention the more specific market factors associated with being an airline company (e.g., the price of oil).

There are a number of problems with using standard correlation measures in quant trading, most of which we will address at various points later. Most relevant for the moment, the measurement of the relationships between two instruments can be very unstable over time. They can even be unreliable over long time periods. For example, imagine a portfolio with two investments: one in the S&P 500 and one in the Nikkei 225. Taking the data on both since January 1984, we can see that these two indices correlate at a level of 0.37 since inception. The range of correlations observed using weekly returns over any consecutive 365 calendar days (a *rolling year*) is shown in Exhibit 6.4. Please note that we choose to use weekly returns in this case, rather than daily returns, because of the time‐zone difference between the United States and Japan. In general, there is a one‐day lag between the movements that occur in Japan, with respect to the moves that occur in the United States. This can be handled either by using less frequent than daily returns (as in this example) or by lagging the Japanese returns by a day.

You can see that the level of correlation observed between the S&P 500 and the Nikkei 225 depends quite a lot on exactly when it is measured. Indeed, this correlation reaches the lowest point in the sample (+0.01) in October 1989 and by mid‐2008 was at its highest point (+0.66). What's

![](_page_11_Figure_1.jpeg)

Exhibit 6.4 Rolling Yearly Correlation between S&P 500 and Nikkei 225

worse, the correlation between these indices went from +0.02 to +0.58, and then back to +0.01 all during the course of about four years, from November 1985 until October 1989. Even using a rolling five‐year window, the range is +0.21 to +0.57.

If the strategy specifies appropriate groupings of instruments, as in our earlier example of industry groups, the stability of the correlations over time improves. This specification can be made either in the definition of *relative* in a relative alpha strategy and/or in the specification of the risk model. So, for example, if the model groups together companies such as XOM and CVX, this can be seen as reasonable, because these two companies have much in common. Both have market capitalizations on the same general scale, both are oil companies, both are based in the United States and have global operations, and so on. Meanwhile, a comparison between CVX and Sun Microsystems (JAVA) might be less defensible based on fundamental factors, such as the fact that JAVA isn't an oil company but is a much smaller capitalization company in the technology sector. Somewhat predictably, this theoretical difference in the comparability between these two pairs of stocks (XOM vs. CVX, CVX vs. JAVA) also bears out in the data, as shown in Exhibit 6.5.

As you can see, CVX and XOM correlate relatively well over the entire period of more than 20 years. The lowest correlation level observed between this pair is approximately 0.40, and the highest is 0.89. The correlation over the entire period is 0.70. Meanwhile, CVX and JAVA correlate poorly, at a level of only 0.14 over the whole sample, with a minimum two‐year

![](_page_12_Figure_1.jpeg)

Exhibit 6.5 Correlation Over Time between Similar and Dissimilar Instruments

correlation of –0.14 and a maximum of 0.36. Furthermore, the correlation between CVX and XOM changes more smoothly over time than that between CVX and JAVA. Though both pairs can be said to be somewhat unstable, it is quite clear that grouping CVX with XOM is less likely to be problematic than grouping CVX with JAVA. To be clear, the instability of correlations among financial instruments is more or less a fact of the world. It is not the fault of optimizers, nor of correlation as a statistic, that this happens to be the case in the finance industry.

The main source of this instability is that the relationships between financial instruments are often governed by a variety of dynamic forces. For example, if the stock market is experiencing a significant downdraft, it is probable that the correlation between CVX and JAVA will be temporarily higher than usual. If, on the other hand, there is uncertainty about oil supply, this may affect CVX but not JAVA, and correlation may be reduced temporarily. If either company has significant news, this can cause decoupling as well.

## Optimization Techniques

There are many types of optimizers. They range from basic copies of Markowitz's original specification in 1952 to sophisticated machine learning techniques. This section provides an overview of the most common of these approaches.

Unconstrained Optimization The most basic form of an optimizer is one that has no constraints; for example, it can suggest putting 100 percent of a portfolio in a single instrument if it wants. Indeed, it is a quirk of unconstrained optimizers that they often do exactly that: propose a single‐instrument portfolio, where all the money would be invested in the instrument with the highest risk‐adjusted return.

Constrained Optimization To address this problem, quants figured out how to add constraints and penalties in the optimization process, which forces more "reasonable" solutions. Constraints can include position limits (e.g., not more than 3 percent of the portfolio can be allocated to a given position) or limits on various groupings of instruments (e.g., not more than 20 percent of the portfolio can be invested in any sector). An interesting conundrum for the quant, however, is that, if the unconstrained optimizer would tend to choose unacceptable solutions, to the extent that constraints are applied it can become the case that the constraints drive the portfolio construction more than the optimizer. For example, imagine a portfolio of 100 instruments, with the optimizer limited to allocating no more than 1.5 percent to any single position. The average position is naturally 1 percent (1/100 of the portfolio). So, the very best positions (according to the alpha model) are only 1.5 times the average position, which is relatively close to equal‐weighted. This is fine, but it somewhat defeats the purpose of optimizing.

Another class of constraints for optimization involves the integration of risk models. Here, too, there are several ways to implement a constraint (as discussed in Chapter 4), including penalties and hard limits. If, for example, we simply want to eliminate sector risk, a simple way might be to modify the correlation matrix so that all stocks within a given sector are given high positive correlations. The optimizer would otherwise solve for the best solution given this modified correlation structure. Alternatively, we could introduce a penalty function that penalizes a small amount of sector risk at a low level (i.e., the expected return needed to overcome this penalty would be itself relatively small); but, as the level of sector risk increases to double the prior level, the expected return needed to justify this increase is substantially larger than double the alpha required at the lower level of sector risk. In other words, the expected marginal reward must increase substantially faster than the expected marginal risk, and the larger the increase in risk, the faster the expected return must increase to make the optimizer accept the trade‐off and allow the increased risk exposure.

Transaction costs, too, can be addressed in various ways. One can build an empirical model of every stock's market impact function, and use this set of individual market impact models to feed into the optimizer. Alternatively, one could simply specify a market impact function that takes inputs such as volatility, (dollar) volume, and the order size, and have a generalized solution for a market impact model. These are but two of the many ways that quants can account for expected transaction costs as an input to a portfolio optimization.

The mathematics and programming that achieve optimization are designed to account for the types of inputs mentioned above, iteratively solving for the trade‐off that maximizes the objective function (for example, the expected return versus the expected volatility) of a portfolio. The optimizer is trying to solve a lot of problems at once, potentially: maximize returns per unit of risk, accounting for correlation and volatility, while staying within various hard limits (e.g., maximum position size constraints), and while accounting for risk factor exposures and transaction costs. While it's complicated, compared to many other aspects of a systematic trading strategy, these are generally very well‐understood solutions, and there are many canned, off‐the‐shelf (including free, open source codebase) packages that compute these solutions relatively painlessly.

If we think back to Exhibit 6.2, visually, constraining an optimization involves cutting out regions of the surface that do not satisfy the conditions. For example, imagine that we impose a market exposure constraint, such that the maximum difference between the bets on ABC and DEF is 20 percent. In this case, most of the surface will be ignored by the optimizer, and only the parts of the surface that satisfy the maximum exposure constraint are searched. This is illustrated in Exhibit 6.6. As you can see by comparing this exhibit to Exhibit 6.2, the excluded areas of search due to the constraint on net exposure are the flat, somewhat triangular "wings" along the plane at a 0 Sharpe Ratio. Note that, since we are limiting the region for the optimizer to search through, it is possible to add so many constraints that there is no solution.

It is also worth recognizing that our example is extremely oversimplified. We have a portfolio of only two assets, high correlation, and one simple constraint. In a more realistic scenario, the surface is unlikely to be so simple‐looking, with a clear trend toward a single peak. In a more complex case, there may be many peaks scattered around various regions of the overall space. The algorithm used to seek the optimal outcome must be designed with a specific trade‐off in mind, namely between thoroughness and speed. A faster, less thorough optimization algorithm might find a local peak in the curve and stop looking, even though somewhere else in the universe of possible portfolios, there is an even better portfolio to consider. A more

![](_page_15_Figure_1.jpeg)

**EXHIBIT 6.6** Visual Representation of Constraining the Search Space for an Optimization

thorough search algorithm might find that *globally optimal* solution, but its search might take far too long to be practicable.

**Black-Litterman Optimization** Fischer Black, of Black-Scholes fame, and Bob Litterman, of Goldman Sachs, in 1990 produced a new optimization method that was first introduced in an internal memo at Goldman but was later published in 1992 in the Financial Analysts Journal.<sup>3</sup> Their Black-Litter*man optimizer* addresses some of the problems associated with errors in the measurement of inputs to an optimizer. Most important, they proposed a method of blending an investor's expectations with a degree of confidence about those expectations, and these with the historical precedent evident in the data. For example, imagine that CVX and XOM correlate at 0.7 historically, but going forward, a trader's alpha model forecasts that XOM will rally while CVX will fall. In this case, the correlation between CVX and XOM over the period being forecast may be quite low, perhaps even negative, despite the evidence from history. Black-Litterman provided a way to adjust historically observed correlation levels by utilizing the investor's forecasts of return for the various instruments in question. Furthermore, to the extent that the investor has greater confidence in some forecasts and less

in others, this fact can be incorporated. If the investor forecasts significant divergence between instruments that historically have correlated at a high level but has a low level of confidence in the forecast, something much closer to the historical level of correlation is used. To the extent that the investor has greater confidence, the forecast returns play a more important role in determining the correlation coefficient utilized by the Black-Litterman optimizer. Some quants prefer this method of optimization because it allows for a more holistic approach to combining the alpha model with the other inputs to optimization.

**Grinold and Kahn's Approach: Optimizing Factor Portfolios** Another kind of optimizer that bears mentioning is described in Grinold and Kahn's seminal Ac*tive Portfolio Management.*<sup>4</sup> This kind of portfolio optimization technique is directly aimed at building a portfolio of signals, whereas most optimizers try to size positions. The method of optimizing proposed by Grinold and Kahn is fairly widely used. The idea of this approach is to build *factor portfolios*, each of which is usually rule-based (in fact, very often equal-weighted or equal risk-weighted) portfolios based on a single type of alpha forecast. So, for example, one could imagine building a momentum portfolio, a value portfolio, and a growth portfolio. Each of these portfolios is in turn simulated historically, as though it were making stock picks through the past. For instance, the value factor's portfolio would look back at the historical data and simulate the results it would have achieved by buying undervalued instruments and shorting overvalued instruments through this historical sample, as though it were reliving the past. In this way, a time series of the returns of these simulated factor portfolios is generated. These simulated factor portfolio returns are then treated as the instruments of a portfolio by the optimizer.

One benefit of this approach is that the number of factor portfolios is typically much more manageable, usually not more than about 20, corresponding to the number of individual factors in the alpha model. What is therefore being optimized is not a portfolio of thousands of instruments but rather the mixing of a handful of factor portfolios. This is certainly an easier hurdle to clear in terms of the amount of data needed. Factor portfolio optimization allows for the inclusion of the risk model, transaction cost model, portfolio size, and risk target as inputs, in much the same way as described for other optimizers.

Given the weight of each model, we ultimately need to ascertain the weight of each position. The way that each position's weight is computed in this approach is perhaps easiest to understand by example. Imagine we have two alpha factors, both of which yield only a directional forecast (i.e.,  $+1$  for a buy signal or  $-1$  for a sell signal). We have 100 stocks in the factor portfolios, which are equally weighted for simplicity's sake. This means that each stock

is 1 percent of each factor portfolio. Let's assume that the factor optimization procedure dictated that we should have a 60 percent weight on the first factor portfolio and a 40 percent weight on the second. The allocation to any stock in this example is 1 percent (the weight of each name in each factor portfolio) times the signal given by that factor (i.e., long or short) times the weight of each factor portfolio. Let's say that the first alpha factor's forecast for a given company is +1, and the second is –1. So the total allocation to the company is [(1%) \* (+1) \* (60%)] + [(1%) \* (–1) \* (40%)] = +0.2%, meaning that we would be long 0.2 percent of our portfolio in this company.

Resampled Efficiency In *Efficient Asset Management*, Richard Michaud proposed yet another approach to portfolio construction models.5 Rather than proposing a new type of optimization, however, Michaud sought to improve the inputs to optimization. His *Resampled Efficiency* technique may address oversensitivity to estimation error. Michaud argues that this is in fact the single greatest problem with optimizers. Earlier, we gave the example of the instability of the correlation between the S&P 500 and the Nikkei 225. This implied that, if we used the past to set expectations for the future—in other words, to estimate the correlation between these two instruments going forward—we are reasonably likely to have the wrong estimate at any given time, relative to the actual correlation that will be observed in the future. A quant will have such estimation errors in the alpha forecasts, in the volatility forecasts, and in the correlation estimates. It turns out that mean variance optimizers are extremely sensitive to these kinds of errors in that even small differences in expectations lead to large changes in the recommended portfolios.

Michaud proposes to resample the data using a technique called *Monte Carlo simulation* to reduce the estimation error inherent in the inputs to the optimizer. A Monte Carlo simulation reorders the actually observed results many times, thereby creating a large number of time series all based on the same underlying observations. For example, imagine we are testing a trend‐ following strategy that is based on the closing prices of the S&P 500 from 1982 through 2008. But now we want to get a sense of how robust the strategy might be if the future doesn't look exactly like the past. So, we can take the return distribution of the S&P 500, which tells us how often the S&P gains or loses various amounts, and use it to create a large number of alternate histories for the index. By reshuffling the returns in this way, we have less dependence on the past looking just like the future, because we now have thousands of "pasts" over which to test our strategy. Interestingly, the average return and the volatility of returns will remain the same across all these alternate histories because they are based on the same underlying return distribution. But now we can see how often our strategy performs well or poorly across all these hypothetical scenarios and therefore how likely it is to work well or poorly in a future that might not resemble the past precisely. This technique is thought to produce more robust predictions than are possible from simply using only the actual sequence of returns the instrument exhibited, in that the researcher is capturing more aspects of the behavior of the instrument. It is this intuition that is at the heart of Monte Carlo simulations.

A word of warning regarding such resampling techniques, however: Re‐using a historical distribution is only really useful if you have sufficient confidence that the sample in the historical distribution is a fair representation of the whole population. For example, if you were to use the S&P's daily returns from 1988 through 2006, you might believe you had a very good data sample: approximately 19 years of daily returns. However, you would be missing many of the largest negative observations, because both the 1987 crash and the 2007–2008 bear market would be missing. Specifically, using the 19‐year sample, you'd have only nine days in your sample during which the S&P declined by more than 4 percent, and only 11 days where the S&P rose by more than 4 percent. By including those extra three years of data, you would see an extra 19 days on which the S&P declined by more than 4 percent (including one day larger than –20%), as well as 16 extra days on which the S&P rose by more than 4 percent (including one day of almost +12%).

Data-Mining Approaches to Optimization As a final note on the types of optimizers, we turn our attention briefly to data‐mining approaches applied to portfolio construction models. Some quants use machine learning techniques, such as supervised learning or genetic algorithms, to help with the problem of optimization. The argument in favor of machine learning techniques in portfolio construction is that mean variance optimization is a form of data mining in that it involves searching many possible portfolios and attempting to find the ones that exhibited the best characteristics, as specified by the objective function of the optimizer. But the field of machine learning aims to do much the same thing, and it is a field that has received more rigorous scientific attention in a wide variety of disciplines than portfolio optimization, which is almost exclusively a financial topic. As such, there may be good arguments for considering machine learning approaches to finding the optimal portfolio, especially due to the quality of those algorithms relative to the mean variance optimization technique.

## Final Thoughts on Optimization

One interesting byproduct of portfolio optimization is that there are instances in which an instrument that is forecast to have a positive return in the future by the alpha model might end up as a short position in the final portfolio (or vice versa). How can this happen? Imagine we are trading a group of equities in the United States and that one of the constraints imposed on the optimization by the risk model is that the portfolio must be neutral to each industry group. In other words, for every dollar of long positions within, say, the software industry, we must have a corresponding dollar of short positions within the same industry (to create a zero net position in the software industry). But what if we have positive return expectations for every stock in the software industry? The optimizer would likely be long those software companies with the highest positive return expectations and short those software companies with the lowest positive return expectations.

Certainly, among sophisticated quants that use optimizers to build their portfolios, the most simplistic optimization techniques (particularly unconstrained) are in the minority. Still, though the intuition behind optimization is sound, the technique itself is perhaps the most properly labeled black box part of the quant trading system. The output is sometimes confusing relative to the inputs because of the complexity of the interactions among an alpha model, a risk model, and a transaction cost model, along with the constraints of size and desired risk level. Compounding the complexity, we have to consider the interaction among various kinds of alpha factors within the alpha model. That said, it is highly likely that the larger positions in the portfolio are those with the strongest expected returns. The strange behavior described here—having a position in the opposite direction as the alpha model's forecast—is observable mainly with the smaller positions in the portfolio because it is among these that the expected returns can be overcome by transaction cost or risk management considerations.

This last phenomenon is sometimes known as the substitution effect. If we have a higher forecasted return for ABC than we do for DEF, it might be expected that our portfolio should reflect this. However, if ABC is also expected to be dramatically more expensive to trade, and if ABC and DEF are reasonably correlated, the optimizer might well choose to invest in DEF instead of ABC.

# Output of Portfolio Construction Models

Regardless of the type of portfolio construction approach used, the output of the quantitative portfolio construction model is a targeted portfolio: the desirable individual positions and the targeted sizes of each. This target portfolio is compared to the current portfolio, and the differences are the trades that need to be done. In the case that a brand‐new portfolio is being built from scratch, all the positions recommended by the portfolio construction model will need to be executed. If, instead, the quant is rerunning the portfolio construction model as he would do periodically in the normal course of business, he would need to do only the incremental trades that close the gap between the newly recommended portfolio and the existing portfolio he holds.

# How Quants Choose a Portfolio Construction Model

I have observed that the significant majority of quants using rule‐based allocation systems seem to take an "intrinsic" alpha approach (i.e., they forecast individual instruments rather than forecasting instruments relative to each other). Most, but not all, of these are actually futures traders. Meanwhile, quants utilizing optimizers tend to be focused on a "relative" alpha approach, most typically found among equity market neutral strategies. There is no obvious reason for the difference in the preferred portfolio construction approach for relative and intrinsic traders. However, it is likely that quants that use relative alpha strategies already believe implicitly in the stability of the relationships among their instruments. After all, in a relative alpha paradigm, the forecast for a given instrument is as much a function of that instrument's behavior as it is about the behavior of the instruments to which the first is being compared. If these relationships are unstable, the strategy is doomed to start with, because its first premise is that certain comparisons can be made reliably. If the relationships are stable, however, it is entirely logical and consistent that the quant can rely on them for portfolio construction as well.

Meanwhile, if a quant takes an intrinsic alpha approach, he is making an implicit statement that his portfolio is largely made up of a series of independent bets, so relying on a correlation matrix (one of the key inputs to the optimizer) might not be very useful. Instead, this kind of quant would focus efforts more directly on risk limits and alpha forecasts subject to transaction costs. This more direct approach to portfolio construction is usually best implemented with a rule‐based model. It is interesting to note that the kind of alpha model a quant builds is likely to impact the choice of portfolio construction model that makes the most sense to use.

# Summary

We have described the two major families of portfolio construction models. Rule‐based models take a heuristic approach, whereas portfolio optimizers utilize logic rooted in modern portfolio theory. Within each family are numerous techniques and, along with these, numerous challenges. How does the practitioner taking a rule‐based approach justify the arbitrariness of the rules he chooses? How does the practitioner utilizing optimization address the myriad issues associated with estimating volatility and correlation? In choosing the "correct" portfolio construction technique, the quant must judge the problems and advantages of each, and determine which is most suitable, given the type of alpha, risk, and transaction cost models being used.

All of these techniques share one common thread, however: They are taking the expected returns (from the alpha model's forecasts) and transforming those into a portfolio. This transformation can be extremely simple or very complex, and the choice is determined by the approach that the quant researcher takes to the problem. However, all of these approaches are attempting to maximize the goodness of the outcome. What determines goodness is also entirely up to the researcher. For example, some seek to maximize the Sharpe ratio, others seek to maximize the ratio of return to maximum peak‐to‐valley drawdown, and still others might seek to maximize the expected return without consideration given to the level of risk. In each case, the researcher can choose also whether and what to constrain as far as risk exposures. Still, the goal is to maximize the goodness of the outcome, subject to any relevant constraints.

We have completed the penultimate stop on the trip through the inside of the black box, as seen on our road map (Exhibit 6.7). Next we will see how quants actually implement the portfolios that they derived using their portfolio construction models.

![](_page_21_Figure_4.jpeg)

Exhibit 6.7 Schematic of the Black Box