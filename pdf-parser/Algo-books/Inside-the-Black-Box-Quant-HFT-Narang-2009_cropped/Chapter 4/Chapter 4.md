# CHAPTER Z

# Risk Models

The market can remain irrational longer than you can remain solvent.

-John Maynard Keynes

**D** isk management should not be thought of solely as the avoidance of risk<br>or reduction of loss. It is about the intentional selection and sizing of exposures to improve the quality and consistency of returns. In Chapter 3, we defined *alpha* as a type of exposure from which a quant trader expects to profit. But we also noted that, from time to time, there can be a downside to accepting this exposure. This is not what we classify as risk per se. By pursuing a specific kind of alpha, we are explicitly saying that we *want* to be invested in the ups and downs of that exposure because we believe we will profit from it in the long run. Though it would be great fun to accept only the upside of a given alpha strategy and reject the losses that can be associated with it, sadly, that is not possible. However, there are other exposures that are frequently linked to the pursuit of some kind of alpha. These other exposures are not expected to make us any money, but they frequently accompany the return-driving exposure. These exposures are *risks*.

Risk exposures generally will not produce profits over the long haul, but they can impact the returns of a strategy day to day. More important still, the quant is not attempting to forecast these exposures, usually because he cannot do so successfully. But the fact remains that one of the great strengths of quant trading is to be able to measure various exposures and to be intentional about the selection of such exposures. This chapter deals with how quants define, measure, and control risks.

Imagine a relative alpha strategy that focuses on the value (yield) of various stocks, buying higher-yielding stocks and selling short lower-yielding stocks. This strategy clearly can lose money if "cheap" (higher-yield) stocks

underperform "expensive" (lower‐yield) stocks, according to whatever definition the quant chooses for "cheapness" (or yield). That risk is inherent to the pursuit of a value strategy, even if the quant has reason to believe that value strategies should make money in the long term. However, a value strategy without further specification can end up taking significant sector bets in addition to the intentional bet on value. After all, it's easy to see that stocks within a sector tend to move together. So if one technology stock has gotten very cheap, there's a reasonable chance that many other technology stocks have also gotten cheap. This means that an unconstrained value‐hunting strategy is likely to end up with a net long position in the technology sector (in this example). But there is no evidence that there exists a long‐term benefit of overweighting one industry or sector versus another.

More important, assume that the strategy has neither the intention nor the capability to forecast the performance of various sectors. Therefore, sector exposure would be considered a form of risk in our framework, because sector performance is not being intentionally forecast, but having net exposure to various sectors can alter the strategy's results day to day. So the key to understanding risk exposures as they relate to quant trading strategies is that risk exposures are those that are not intentionally sought out by the nature of whatever forecast the quant is making in the alpha model.

If alpha models are like optimists, risk models are like pessimists. Risk models exist largely to control the size of desirable exposures or to deal with undesirable types of exposures. Their job is to raise hell about things that can cause losses or uncertainty, particularly those bets that are unintentionally made or are incidental byproducts of the alpha model. Risk models both highlight and attempt to remove undesirable exposures from a portfolio.

There are, however, only a few things you can do with a given type of exposure, aside from simply accepting it outright. Mostly you can limit its size or eliminate it altogether. The function of risk management in the investment process is to determine which of these courses of action is most prudent for each kind of exposure and to provide that input to the portfolio construction model. In general, risk models reduce the amount of money a quant can make, but this is a trade-off many quants are willing to accept. Managing risk has the day‐to‐day benefit of reducing the volatility of a strategy's returns. But it also has the far more important benefit of reducing the likelihood of large losses. In many ways, the failures of investment managers, in general (quant or not), are usually precipitated by failures to manage risk. This can be seen with Long‐Term Capital Management (LTCM) in 1998, with Amaranth in 2006, with U.S. quant equity traders in August 2007, and with a great many investors in the fall (no pun intended) of 2008.

# Limiting the Amount of Risk

Size limiting is an important form of risk management. It is easy to imagine having a tremendously good trading idea, seemingly a sure thing, but without some sense of risk management, there can be a temptation to put all one's capital into this single trade. This is almost always a bad idea. Why? Because, empirically, a sure thing rarely exists, so the correct way to size a trade in general is certainly *not* to put all your chips on it. Otherwise it is likely that in the process of going all in, at some point the trader will go bankrupt. In other words, it is prudent to take just as much exposure to a trade as is warranted by the considerations of the opportunity (alpha) and the downside (risk). Quantitative risk models that are focused on limiting the size of bets are common, and many are quite simple. The following sections explain how they work.

There are several kinds of quantitative risk models that limit size, and they vary in three primary ways:

- **1.** The manner in which size is limited.
- **2.** How risk is measured.
- **3.** What is having its size limited.

## Limiting by Constraint or Penalty

Approaches to the size limits come in two main forms: hard constraints and penalties. Hard constraints are set to draw a line in terms of risk. For instance, imagine a position limit that dictates that no position will be larger than 3 percent of the portfolio, no matter how strong the signal. However, this hard limit may be somewhat arbitrary (e.g., imagine a 3.00 percent position size limit; why is a 3.01 percent position so much worse?), so quants sometimes build penalty functions that allow a position to increase beyond the limit level, but only if the alpha model expects a significantly larger return (i.e., a much larger expected return than was required to allow the position merely to reach the limit size in the first place). The penalty functions work so that the further past the limit level we go, the more difficult it becomes to increase the position size additionally. So, using our example, it would be far easier to see a 3.01 percent position than to see a 6 percent position, because the latter is further from the limit than the former.

In this way, the model attempts to address the idea that an opportunity can sometimes be so good as to warrant an exception to the rule. In a sense, penalty functions for size limits can be thought of as making rules to govern exceptions.

The levels of limits and/or penalties can be determined in the same ways as most other things in the quant world, namely either from theory or from the data (the latter via data‐mining approaches). Theory‐driven approaches mostly look like an arbitrary level that is set, tested, and, if needed, adjusted until it produces an acceptable outcome. So, to return to the earlier example of a 3 percent limit on position sizes, the quant researcher could have started with a risk limit of 5 percent because his experience seemed to dictate that this was a reasonable level to choose. But through testing and simulating the historical results of this strategy, he could have come to realize that a far more appropriate level is 3 percent, which better balances the ability to make sizeable bets when attractive opportunities appear against the necessity of recognizing that any given trade could easily go wrong. Data‐driven approaches are more varied and can include machine learning techniques to test many combinations of limits or simply testing various limit levels and letting the historical data empirically determine the final outcome. Either way, these levels and the severity of any penalty functions are parameters of the risk model that the quant must set, based on either research or heuristics.

## Measuring the Amount of Risk

There are two generally accepted ways of measuring the amount of risk in the marketplace. The first is longitudinal and measures risk by computing the standard deviation of the returns of various instruments over time, which is a way of getting at the concept of uncertainty. In finance circles, this concept is usually referred to as *volatility*. The more volatility, the more risk is said to be present in the markets.1

The second way to measure risk is to measure the level of similarity in the behavior of the various instruments within a given investment universe. This is frequently calculated by taking the cross‐sectional standard deviation of all the relevant instruments for a given period. The larger the standard deviation, the more varied the underlying instruments are behaving. This means that the market is less risky because the portfolio can be made of a larger number of diversified bets. This can be seen easily at the extreme: If all the instruments in a portfolio are perfectly correlated, then as one bet goes, so go all the other bets. This concept is known among quants as *dispersion*. Dispersion can also be measured by the correlation or covariance among the instruments in a given universe. Here, too, the more similarly the instruments are behaving, the more risky the market is said to be.

There are many other, less commonly utilized, approaches to measuring risk as well. These include the use of measures such as credit spreads or credit default swaps (CDSs), or the use of implied volatilities.

## Where Limits Can Be Applied

Size‐limiting models such as these can be used to govern many kinds of exposures. One can limit the size of single positions and/or groups of positions, such as sectors or asset classes. Alternatively, one can limit the size of exposure to various types of risks. For example, in equity trading, one can limit the exposure of a model to market bets (such as a +/–5 percent net exposure limit) or to market capitalization bets. In general, risks that are subjected to limits or penalties are those that are not being forecast explicitly by the alpha model. If an alpha model attempts to forecast individual stocks but makes no attempt to forecast the stock market as a whole, it may be prudent to constrain the size of the bet that the portfolio can ultimately take on the stock market.

Still another component of a risk model may be to govern the amount of overall portfolio leverage. Leverage can be controlled in a variety of ways. For example, one can manage money under the premise that when opportunities abound, more leverage is desirable, whereas when fewer opportunities are present, less leverage is desirable. Alternatively, many quants attempt to offer their investors or bosses a relatively constant level of risk. Using volatility and dispersion as proxies for risk, quants can measure the amount of risk in markets and vary their leverage accordingly to produce a more stable level of risk. The most common tool used for this purpose is known as a *value at risk* (VaR) model, but there are others that are similar philosophically. These models typically consider the dollar amount of exposures in a portfolio and, based on current levels of volatility, forecast how much the portfolio can be expected to gain or lose within a given confidence interval. For instance, most VaR models calculate what a daily single standard deviation move in portfolio returns will be, based on current volatility levels. The way that these models control risk in the face of rising volatility is to reduce leverage. Therefore, in general, the higher the reading of risk in a VaR model, the lower the level prescribed for leverage.

In Chapter 10, we discuss some of the significant problems with these kinds of risk models. For now I will simply point out that the core purpose of such risk models seems to me to be flawed. Other kinds of investments, such as stocks, bonds, mutual funds, private equity, or fine wine, do not attempt to offer fixed levels of volatility. Why should quants want to manage risk in this manner, or be asked to do so? Furthermore, if a quant is good at forecasting volatility or dispersion, there are far more interesting and productive ways to utilize these forecasts (for example, in the options markets) than there are in a risk model that governs leverage. These kinds of models often cause traders to take too little risk in more normal times and too much risk in very turbulent times. Nevertheless, they are wildly popular.

A more theoretically sound approach, though substantially harder to implement practically, seeks to increase leverage when the strategy has better odds of winning and to decrease risk when the strategy has worse odds. The trick, of course, is to know when the odds are on one's side. Some quants solve this problem by allowing the level of leverage to vary with the overall strength and certainty of the predictions from the alpha model, which seems to be a reasonable approach.2

# Limiting the Types of Risk

Though limiting the amount of an exposure is important, some approaches to risk modeling focus on eliminating whole types of exposure entirely. Imagine that an investor's analysis indicates that Chevron Corporation (CVX) is likely to outperform Exxon Mobil Corporation (XOM). But the trade the investor makes is simply to go long CVX while ignoring XOM. If the market drops precipitously afterward, or if the oil sector performs very poorly, the investor will most likely lose money on the trade, despite the correctness of his thesis. This is because the investor is exposed to market directional risk and to oil sector risk, even though he didn't have any particular foresight as to where the market or the oil sector was going. The investor could have substantially eliminated the unintentional or accidental market direction risk if he had expressed his analysis by buying CVX and shorting an equivalent amount of XOM. This way, whether the market rises, falls, or does nothing, he is indifferent. He is only affected by being right or wrong that CVX would outperform XOM.

As a general rule, it is always better to eliminate any unintentional exposures, since there should be no expectation of being compensated sufficiently for accepting them. Quantitative risk models designed to eliminate undesired exposures come in two familiar flavors: theoretical and empirical. Each is discussed in detail in the subsequent sections.

It is also worth noting that alpha models can (and often do) incorporate risk management concepts. Let's assume that a quant is building a relative alpha strategy. A significant amount of work is required to match what "relative" means to the exposures he intends to take or hedge. Revisiting an earlier example, if the quant is building a relative alpha strategy to forecast equity returns, he might not believe he has a valid way to forecast the returns of the sectors to which these equities belong. In this case, the quant may design his bet structures so that he is making forecasts of stocks' returns *relative* to their sectors' returns, which means that he never has a bet on the direction of the sector itself, only on which stocks will outperform and which stocks will underperform the sector. This, in turn, helps him eliminate sector bets, which is clearly a risk management exercise as much as it is alpha generation. As such, it is theoretically possible (and not infrequently seen in practice) to incorporate all the needed components of his risk model fully into his alpha model by specifying the alpha model such that it only forecasts exactly the exposures from which it expects to make money and structures its bets to avoid exposure to nonforecasted factors. Although not all quant strategies do this, it is worth remembering to look inside the alpha model for elements of risk management, especially for those evaluating a quant strategy.

## Theory-Driven Risk Models

Theory‐driven risk modeling typically focuses on named or *systematic* risk factors. Just as in the case of theory‐driven alpha models, systematic risks that are derived from theory are those for which the quant can make a reasonable, economic argument. Theory‐driven risk modeling uses a set of predefined systematic risks, which enables the quant to measure and calibrate a given portfolio's exposures.

It is important to note that the use of the term *systematic* in defining risk is completely different from the use of the term *systematic* in describing quant strategies. Systematic risks are those that cannot be diversified away. In the world of single stocks, the market itself is a systematic risk because no amount of diversification among various single stocks eliminates an investor's exposure to the performance of the market itself. If the market is up a lot, it is extremely likely that a portfolio that is long stocks is also going to be up. If the market is down a lot, it is extremely likely that a portfolio that is long stocks will be down. Sector risk is another example of systematic risk, as is market capitalization risk (i.e., small caps versus large caps). A practical example of such a problem, and one that has been well documented by the hedge fund replication crowd, is that an unconstrained market‐neutral value model will very likely be making a bet on small caps outperforming large caps.3

The world of fixed income, similarly, contains a host of systematic risks. For example, whether one owns corporate bonds or government bonds, owners of these bonds are all subject to interest rate risk; that is, the risk that rates will go up, regardless of the level of diversification of the actual portfolio of bonds. Similar examples can be found in any asset class and frequently also across asset classes. Any economically valid grouping of instruments, in other words, can be said to share one or more common systematic risk factors. An investor who traffics in any of those instruments, then, should be aware of this risk factor and should be either making intentional bets on it or eliminating his exposure.

## Empirical Risk Models

Empirical risk models are based on the same premise as theory‐driven models, namely that systematic risks should be measured and mitigated. However, the empirical approach uses historical data to determine what these risks are and how exposed a given portfolio is to them. Using statistical techniques such as *principal component analysis* (PCA), a quant is able to use historical data to discern systematic risks that don't have names but that may well correspond to named risk factors.4 For example, a PCA run on bond market data using Treasury bonds across various maturities usually shows that the first (most important) risk factor statistically corresponds to the level of interest rates, or what a theory‐driven risk model might call *interest rate risk*. PCA and other statistical models are commonly used in equity markets as well, and these models typically find that the market itself is the first, most important driver of returns for a given stock, usually followed by its sector. These statistical risk models are most commonly found among statistical arbitrage traders, who are betting on exactly that component of an individual stock's returns that is *not* explained by systematic risks. It is important to note that such statistical methods may discover entirely new systematic risk factors, which a reasonable observer might be inclined to acknowledge exist but for which names have not been assigned. On the other hand, statistical risk models are subject to being fooled by the data into finding a risk factor that will not persist for any useful amount of time into the future. It is also possible for a statistical risk model to find spurious exposures, which are just coincidences and not indicative of any real risk in the marketplace. This is a delicate problem for the researcher.

## How Quants Choose a Risk Model

Quants are attracted to theory‐driven risk models because the risk factors they encapsulate make sense. It is hard to make the argument that market risk does not exist as a strong systematic risk factor in equities. Note that this is much the same reasoning that supports theoretical approaches to alpha modeling: Any reasonable person can understand the theory and see that it is likely to be true. This in turn can give the quant faith in the models when it isn't performing very well. Warren Buffett, for example, didn't change his stripes just because he dramatically underperformed the stock market during the Internet bubble. He was able to keep the faith in no small part because his approach to markets has very strong theoretical underpinnings.

Quants that choose empirical risk models typically seek the benefits of adaptiveness. Theoretical risk models are relatively rigid, meaning that the risk factors are not altered often (otherwise the theory would not have been very strong in the first place). Yet the factors that drive markets do change over time. For a while in early 2003, daily reports about the prospect, and later the progress, of the U.S. invasion of Iraq drove stock, bond, currency, and commodity markets almost singlehandedly. More recently, in early 2008, commodity prices were a significant factor. At other times, expectations of how much the Federal Reserve might cut or raise rates are the key drivers of market behavior. As markets evolve, the data that the markets produce reflect this evolution, and these data drive empirical risk models. For these reasons, an empirical model may be more adaptive to ever‐changing market conditions by detecting through new data whatever factors are implicitly driving markets. There are two stages to this adaptation. During the early phases of a market regime change (for example, when equity investors rapidly change their behavior from risk seeking to risk aversion), the quant is using now irrelevant historical data to determine relationships and measure risk factors. Thus, during this phase, the empirical risk model will be modeling market risks incorrectly. Later, if the new behavior persists, the empirical risk model eventually will catch up to the newly prevailing theme driving markets, and all will be well again.

Besides exhibiting a weakness during a regime change, a basic understanding of statistics reveals another problem with empirical risk models. To achieve statistical significance and reduce the potential for measurement error in computing relationships among various instruments, empirical risk models require a rather large amount of data. But this leads to a trade-off that could squelch most of the adaptiveness benefits of empirical risk models. The more data that are used—that is, the further back into history we must look—the less adaptive a model can be, because each new data point is but one of a very large number. If we use two years' worth of rolling daily data, or approximately 520 trading days, each new day adds a new data point and causes the oldest one to fall out of the sample. So for every day that passes, only two days' data have changed out of 520. It will therefore take a long time to turn the ship and have the empirical model find the new drivers of risk from the data. However, if the quant attempts to improve adaptiveness by shortening the historical window used, the power of the statistics diminishes significantly so that there cannot be sufficient confidence in the measurements to act on them.

Still, there may be benefits to empirical risk models. If the theoretical risk models are any good at being right, an empirical model should capture these effects without having to know the names of the factors beforehand. If market risk is indeed a big driver of stock prices, an empirical model should pick this up from the data. If the data don't bear it out, what good is the theory? Furthermore, the competing objectives of statistical significance and adaptiveness can be dealt with in part by using intraday data. For example, if a quant uses one‐minute intraday snapshots of price activities instead of simply a single closing price for each day, he is able to extract almost 400 data points for *each day* in his sample, which allows him to use far fewer days to achieve the same statistical significance as another quant using a single data point for each day (the closing price).

Ultimately, because of the comfort level with the concepts involved in theory‐driven risk modeling, most quants tend to use theory‐driven risk models rather than empirical risk models. It is worth noting that these two kinds of risk models are not mutually exclusive. Quants may perfectly reasonably use a combination of both, if they deem it appropriate. A small minority of managers also attempt to use their judgment and discretion to monitor market behavior and, should it become clear to them—for example, from the way that the financial media and their peers in the business are behaving—that there is a "new" risk factor that is driving markets, they build a made‐to‐order risk factor to measure this temporary phenomenon. When they see that the new driver has faded in importance, they can remove it from the risk model, again using their judgment.

It is worth mentioning that quants have the option, as is the case with most of the modules of the black box, to build their own risk model or to purchase one that is off the shelf. Most premade risk models are not of the empirical variety because empirical solutions require a specifically set universe of instruments, and the analytical techniques are usually relatively easy to implement with simple price data. Also, the vast majority of premade risk models are useful only for equity trading strategies. Several purveyors of risk models—such as BARRA, Northfield, Axioma, and Quantal—have made a healthy business of licensing their software to quant traders. The advantage of buying risk models is that they are ready to be deployed immediately, without extensive R&D by the quant trader, and usually at least reasonably well thought through. However, they are also by nature somewhat generic. There are advantages to building risk models as well, primarily because they can be customized to the specific needs of the particular quant trader.

# Summary

Risk management is frequently misunderstood to be an exercise designed to reduce risk. It is really about the selection and sizing of exposures, to maximize returns for a given level of risk. After all, reducing risk almost always comes at the cost of reducing return. So, risk management activities must focus on eliminating or reducing exposure to unnecessary risks but also on taking risks that are expected to offer attractive payoffs. This is true whether one uses a systematic investment process or a discretionary one. The main difference between the two is that quants typically use software to manage risk, whereas discretionary traders, if they use software in the risk management process at all, primarily attempt merely to measure risk in some way, without any systematic process for adjusting their positions in accordance with predefined guidelines.

Whether a quant uses a theoretical or empirical risk model or some hybrid thereof, the goal is the same: The quant wants to identify what systematic exposures are being taken, measure the amount of each exposure in a portfolio, and then make some determination about whether these risks are acceptable. What is good about these kinds of analyses, along with many of the other quantitative risk‐modeling approaches, is that they require the quant to be intentional about risk taking, rather than slapping together some positions that seem like good trades and more or less ignoring the incidental exposures these trades may share. For example, if oil prices become a dominant theme in investors' sentiment about the markets, positions across a variety of sectors and asset classes can be driven by oil. This can lead to a significant downside if a previously profitable trend in the price of oil reverses. A risk model may allow the quant to see this kind of exposure and make a choice about whether to do something about it. This is an important point. Quantitative approaches to risk management, by virtue of seeking to measure and make explicit what exposures are driving a portfolio, put the power into the hands of the portfolio manager to make rational, deliberate decisions. Of course, whether this intentionality is helpful or hurtful depends on the judgment of the portfolio manager, even among quants. But at least quantitative risk management techniques offer the opportunity to see what risks are present in a portfolio and to what extent.

In the next chapter, we examine transaction cost models, which are the final providers of input to help determine the most desirable target portfolio for a quant. Before doing so, let's look at Exhibit 4.1 to examine our progress through this journey inside the black box.

![](_page_10_Figure_3.jpeg)

Exhibit 4.1 Schematic of the Black Box

# Notes

- 1. Uncertainty has broadly been adopted as being synonymous with risk. There is usually not much justification for its use, other than expediency for the purposes of relatively easy computations to answer the question "How much risk is there?".
- 2. This concept was formalized in the Kelly criterion, in a paper by John L. Kelly, Jr., in the *Bell System Technical Journal* in 1956. The *Kelly criterion* provides a systematic way of sizing the risk taken on each of a series of bets based on the bettor's edge, which maximizes the expected gains by the end of the series of bets. The edge is defined as a combination of the payoff for winning and the odds of winning. This concept has been widely applied in gambling and somewhat in investing. The noted quant Edward Thorp is credited with first applying the Kelly criterion to trading strategies. However, some critics of the Kelly betting strategy point out that a critical assumption of this criterion is that each bet is expected to be independent of the next, which is true in many forms of gambling, for example. However, in investing, bets can be serially correlated, which is to say that returns to investment strategies tend to be streaky. As such, in general many investors who believe in the concept of the Kelly criterion use a derivative version of the strategy, such as "half Kelly," to bet less than Kelly suggests. Useful background on Kelly and the criterion can be found on William Poundstone's website or in his book about Kelly, called *Fortune's Formula*.
- 3. This phenomenon exists, if for no other reason, because the value investor tends to buy stocks that have fallen in price, which tend therefore to have experienced a shrinkage in their market capitalization. A market‐neutral value investor would also tend to sell expensive stocks, which are likely to have rallied and therefore will have experienced market capitalization appreciation as well.
- 4. *Principal components analysis* (PCA) is a statistical technique used to reduce the complexity of a set of instruments down to a manageable set of risk factors, each of which is called a *vector*. Each vector represents a statistically derived systematic risk among the instruments and is derived by analyzing the historical relationships among all the instruments in the set.