<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Copulas: Estimation
   </b>
  </h1>
  <p block-type="Text">
   Copulas are a tool for modeling and capturing the dependence of two or more random variables (rv's). In the work of Sklar [22], the term
   <i>
    copula
   </i>
   was used for the first time; it is derived from the Latin word
   <i>
    copulare
   </i>
   , which means to connect or to join. Similarly, Hoeffding had already studied distributions under "arbitrary changes of scale" in the 1940s; see [7].
  </p>
  <p block-type="TextInlineMath">
   The main purpose of a copula is to disentangle the dependence structure of a random vector from its marginals. A
   <math display="inline">
    d
   </math>
   -dimensional copula is defined as a function
   <math display="inline">
    C : [0, 1]^d \rightarrow [0, 1]
   </math>
   , which is a cumulative distribution function (cdf) with uniform&lt;sup&gt;a&lt;/sup&gt; marginals. On one hand, this leads to the following properties:
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     1.
     <math display="inline">
      C(u_1, \ldots, u_d)
     </math>
     is increasing in each component
     <math display="inline">
      u_i, i \in \{1, \ldots, d\}.
     </math>
    </li>
    <li block-type="ListItem">
     2.
     <math display="inline">
      C(1, \ldots, 1, u_i, 1, \ldots, 1) = u_i
     </math>
     for all
     <math display="inline">
      1 \le i \le d
     </math>
     .
    </li>
    <li block-type="ListItem">
     3. For
     <math display="inline">
      a_i &lt; b_i
     </math>
     ,
     <math display="inline">
      1 &lt; i &lt; d
     </math>
     , C satisfies the rectangle inequality
    </li>
   </ul>
  </p>
  <p block-type="Equation">
   <math display="block">
    \sum_{i_1=1}^2\cdots\sum_{i_d=1}^2(-1)^{i_1+\ldots+i_d}C(u_{1,i_1},\ldots,u_{d,i_d})\geq 0,
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    u_{j,1} = a_j
   </math>
   and
   <math display="inline">
    u_{j,2} = b_j
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   On the other hand, every function satisfying
   <math display="inline">
    (1)
   </math>
   –
   <math display="inline">
    (3)
   </math>
   is a copula. Furthermore,
   <math display="inline">
    C(1, u_1, \ldots, u_{d-1})
   </math>
   is again a copula and so are all
   <math display="inline">
    k
   </math>
   -dimensional marginals with
   <math display="inline">
    2 &lt; k &lt; d
   </math>
   .
  </p>
  <p block-type="Text">
   The construction of multivariate copulas is difficult. There is a rich literature on uni- and bivariate distributions but many of these families do not have obvious multivariate generalizations.&lt;sup&gt;b&lt;/sup&gt; Similarly, it is not at all straightforward to generalize a two-dimensional copula to higher dimensions. For example, consider the construction of threedimensional copulas. A possible attempt is to try
   <math display="inline">
    C_1(C_2(u_1, u_2), u_3)
   </math>
   where
   <math display="inline">
    C_1, C_2
   </math>
   are bivariate copulas. However, already for
   <math display="inline">
    C_1 = C_2 = \max\{u_1 + u_2  1, 0
   </math>
   } (the countermonotonicity copula, introduced in the section
   <i>
    Important Copulas
   </i>
   ) this procedure fails. See [18, Section 3.4] for further details. Also Chapter 4 in [11] gives an overview of construction of multivariate copulas with different concepts. In particular, it discusses the construction of a
   <math display="inline">
    d
   </math>
   -dimensional copula given the set of
   <math display="inline">
    d(d-1)/2
   </math>
   bivariate margins.
  </p>
  <p block-type="Text">
   The class of Archimedean copulas (see also the section Archimedean Copulas) is an important class for which the construction of multivariate copulas can be performed quite generally. A common example of a three-dimensional Archimedean copula is given by the following exchangeable Archimedean copula:
  </p>
  <p block-type="Equation">
   <math display="block">
    C(u_1, u_2, u_3) = \phi^{-1}(\phi(u_1) + \phi(u_2) + \phi(u_3)) \quad (1)
   </math>
  </p>
  <p block-type="Text">
   with appropriate generator
   <math display="inline">
    \phi
   </math>
   . However, for appropriate
   <math display="inline">
    \phi_1, \phi_2
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    \phi_1^{-1}(\phi_2 \circ \phi_1^{-1}(\phi_1(u_1) + \phi_1(u_2)) + \phi_2(u_3)) \quad (2)
   </math>
  </p>
  <p block-type="Text">
   also gives a three-dimensional copula (see [14], Section 5.4.3). It is of course possible that
   <math display="inline">
    \phi_1
   </math>
   and
   <math display="inline">
    \phi_2
   </math>
   are generators of different types of Archimedean copulas.
  </p>
  <p block-type="TextInlineMath">
   The key to the separation of marginals and dependence structure is the
   <i>
    quantile transformation
   </i>
   . Let U be a standard uniform rv and
   <math display="inline">
    F^{-1}(y) :=
   </math>
   <math display="inline">
    \inf \{x : F(x) \ge y\}
   </math>
   be the generalized inverse of F. Then
  </p>
  <p block-type="Equation">
   <math display="block">
    P\left(F^{-1}(U) \le x\right) = F(x) \tag{3}
   </math>
  </p>
  <p block-type="TextInlineMath">
   This result is frequently used for simulation: the generation of uniform rv's is readily implemented in typical software packages and if we are able to compute
   <math display="inline">
    F^{-1}
   </math>
   , we can sample from F using equation
   <math display="inline">
    (1)
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   On the contrary, the probability transformation is used to compute copulas implied from distributions, see the following section Copulas derived from Distributions. Consider
   <math display="inline">
    X
   </math>
   having a continuous distribution function
   <math display="inline">
    F_2
   </math>
   , then
   <math display="inline">
    F(X)
   </math>
   is standard uniform.&lt;sup&gt;c&lt;/sup&gt;
  </p>
  <h4>
   Sklar's Theorem
  </h4>
  <p block-type="Text">
   It is not surprising that every distribution function inherently embodies a copula function. On the other hand, any copula entangled with some marginal distributions in the right way leads to a proper multivariate distribution function. This is the important contribution of Sklar's theorem [22]. Ran
   <math display="inline">
    F
   </math>
   denotes the range of
   <math display="inline">
    F
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   <b>
    Theorem
   </b>
   Consider a
   <math display="inline">
    d
   </math>
   -dimensional
   <math display="inline">
    cdf
   </math>
   <math display="inline">
    F
   </math>
   with marginals
   <math display="inline">
    F_1, \ldots, F_d
   </math>
   . There exists a copula C such that
  </p>
  <p block-type="Equation">
   <math display="block">
    F(x_1, ..., x_d) = C(F_1(x_1), ..., F_d(x_d)) \quad (4)
   </math>
  </p>
  <p block-type="TextInlineMath">
   for all
   <math display="inline">
    x_i
   </math>
   in
   <math display="inline">
    [-\infty, \infty]
   </math>
   ,
   <math display="inline">
    i = 1, \ldots, d
   </math>
   . If
   <math display="inline">
    F_i
   </math>
   is continuous for all
   <math display="inline">
    i = 1, ..., d
   </math>
   then C is unique; otherwise,
   <i>
    C
   </i>
   is uniquely determined only on Ran
   <math display="inline">
    F_1 \times \ldots \times
   </math>
   Ran
   <math display="inline">
    F_d
   </math>
   . On the other hand, consider a copula C and univariate cdf's
   <math display="inline">
    F_1, \ldots, F_d
   </math>
   . Then F as defined in equation
   <math display="inline">
    (4)
   </math>
   is a multivariate cdf with marginals
   <math display="inline">
    F_1, \ldots, F_d.
   </math>
  </p>
  <p block-type="Text">
   It is important to note that for discrete distributions, copulas are not as natural as they are for continuous distributions; compare [8].
  </p>
  <p block-type="TextInlineMath">
   In the following, we therefore concentrate on continuous
   <math display="inline">
    F_i
   </math>
   ,
   <math display="inline">
    i = 1, \ldots, d
   </math>
   . It is interesting to examine the consequences of representation
   <math display="inline">
    (2)
   </math>
   for the copula itself. Using that
   <math display="inline">
    F^{\circ}F^{-1}(y) = y
   </math>
   for any continuous
   <math display="inline">
    \text{CDF } F
   </math>
   , we obtain
  </p>
  <p block-type="Equation">
   <math display="block">
    C(\mathbf{u}) = F\left(F_1^{-1}(u_1), \dots, F_d^{-1}(u_d)\right) \tag{5}
   </math>
  </p>
  <p block-type="Text">
   While relation (4) is usually the starting point for simulations that are based on a given copula and given marginals, relation (5) rather proves to be the theoretical tool to obtain the copula from any multivariate distribution function. This equation also allows to extract a copula directly from a multivariate distribution function.
  </p>
  <h2>
   <b>
    Invariance Under Transformations
   </b>
  </h2>
  <p block-type="TextInlineMath">
   An important property of a copula is that it is invariant under strictly increasing transformations: for strictly increasing functions
   <math display="inline">
    T_i : \mathbb{R} \to \mathbb{R}, i =
   </math>
   <math display="inline">
    1, \ldots, d
   </math>
   the rv's
   <math display="inline">
    X_1, \ldots, X_d
   </math>
   and
   <math display="inline">
    T_1(X_1), \ldots, T_d(X_d)
   </math>
   have the same copula.
  </p>
  <h2>
   <b>
    Bounds of Copulas
   </b>
  </h2>
  <p block-type="Text">
   Hoeffding and Fréchet independently derived that a copula always lies in between certain bounds; compare Figure 1. This is because of the existence of some extreme cases of dependency, co- and countermonotonicity. The so-called
   <i>
    Fréchet-Hoeffding bounds
   </i>
   are given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \max\left\{\sum_{i=1}^{d} u_i + 1 - d, 0\right\} \le C(\mathbf{u}) \le \min\left\{u_1, \dots, u_d\right\}
   </math>
   (6)
  </p>
  <p block-type="Text">
   which holds for any copula
   <math display="inline">
    C
   </math>
   . Although a comonotonic copula exists in any dimension
   <math display="inline">
    d
   </math>
   , there is no countermonotonicity copula in the case of dimensions
   <math display="inline">
    \text{greater than two.}^{\text{d}}
   </math>
  </p>
  <p>
   <img src="_page_1_Figure_12.jpeg"/>
  </p>
  <p>
   <b>
    Figure 1
   </b>
   According to the Fréchet–Hoeffding bounds, every copula has to lie inside of the pyramid shown in the graph. The surface given by the bottom and back side of the pyramid (the lower bound) is the countermonotonicity copula
   <math display="inline">
    C(u, v) = \max\{u + v - 1, 0\}
   </math>
   , while the front side (the upper bound) is the comonotonicity copula,
   <math display="inline">
    C(u, v) = \min(u, v)
   </math>
  </p>
  <h2>
   Important Copulas
  </h2>
  <p block-type="Text">
   First, the
   <i>
    independence copula
   </i>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \prod_{i=1}^{d} u_i \tag{7}
   </math>
  </p>
  <p block-type="Text">
   Random variables are independent if and only if their copula is the independence copula.
  </p>
  <p block-type="Text">
   The comonotononicity copula or the Fréchet-
   <i>
    Hoeffding upper bound
   </i>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \min\left\{u_1,\ldots,u_d\right\} \tag{8}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Random variables
   <math display="inline">
    X_1, \ldots, X_d
   </math>
   are called
   <i>
    comono
   </i>
   tonic, if their copula is as in equation (8). This is equivalent to
   <math display="inline">
    (X_1,\ldots,X_d)
   </math>
   having the same distribution as
   <math display="inline">
    (T_1(Z), \ldots, T_d(Z))
   </math>
   with some rv Z and strictly increasing functions
   <math display="inline">
    T_1, \ldots, T_d
   </math>
   . Therefore, comonotonicity refers to perfect dependence in the sense where all rv's are, in an increasing and deterministic way, depending on
   <math display="inline">
    Z
   </math>
   .
  </p>
  <p block-type="Text">
   The other case of perfect dependence is given by countermonotonicity. The
   <i>
    countermonotonicity cop
   </i>
   ula reads
  </p>
  <p block-type="Equation">
   <math display="block">
    \max\left\{u_1 + u_2 - 1, 0\right\} \tag{9}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Two rv's with this copula are called
   <i>
    countermonotonic
   </i>
   . This is equivalent to
   <math display="inline">
    (X_1, X_2)
   </math>
   having the same distribution as
   <math display="inline">
    (T_1(Z), T_2(Z))
   </math>
   for some rv Z and
   <math display="inline">
    T_1
   </math>
   being increasing and
   <math display="inline">
    T_2
   </math>
   being decreasing or vice versa. However, the Fréchet-Hoeffding lower bound as given in equation (6) is
   <i>
    not
   </i>
   a copula for
   <math display="inline">
    d &gt; 2
   </math>
   ; see [14], Example 5.21.
  </p>
  <h4>
   Copulas Derived from Distributions
  </h4>
  <p block-type="Text">
   The probability transformation&lt;sup&gt;e&lt;/sup&gt; allows to obtain the copula inherent in multivariate distributions: for a multivariate cdf
   <math display="inline">
    F
   </math>
   with continuous marginals
   <math display="inline">
    F_i
   </math>
   , the inherent copula is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    C(\mathbf{u}) = F\left(F_1^{-1}(u_1), \dots, F_d^{-1}(u_d)\right) \qquad (10)
   </math>
  </p>
  <p block-type="TextInlineMath">
   For example, for a multivariate normal distribution, the implied copula is called Gaussian copula. For a d-dimensional rv X, the correlation matrix&lt;sup&gt;f&lt;/sup&gt;
   <math display="inline">
    \Gamma
   </math>
   is obtained from the covariance matrix by scaling each component to variance 1. Therefore,
   <math display="inline">
    \Gamma
   </math>
   is given by the entries
   <math display="inline">
    \text{Corr}(X_i, X_j)
   </math>
   ,
   <math display="inline">
    1 \leq i, j \leq d
   </math>
   (see Correlation
  </p>
  <p block-type="Text">
   <b>
    Risk
   </b>
   ). For such a
   <i>
    correlation matrix
   </i>
   <math display="inline">
    \Gamma
   </math>
   the Gaussian copula is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi_{\Gamma}(\Phi^{-1}(u_1), \dots, \Phi^{-1}(u_d)) \tag{11}
   </math>
  </p>
  <p block-type="Text">
   In a similar fashion, one obtains the
   <i>
    t
   </i>
   -copula or the Student copula
  </p>
  <p block-type="Equation">
   <math display="block">
    t_{\nu,\Gamma}(t_{\nu}^{-1}(u_1)\ldots,t_{\nu}^{-1}(u_d))\tag{12}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <b>
    Γ
   </b>
   is the correlation matrix,
   <math display="inline">
    t_{\nu}
   </math>
   is the cdf of the one dimensional
   <math display="inline">
    t_{\nu}
   </math>
   distribution, and
   <math display="inline">
    t_{\nu}
   </math>
   is the cdf of the multivariate
   <math display="inline">
    t_{\nu,\Gamma}
   </math>
   distribution. The mixing nature of the
   <math display="inline">
    t
   </math>
   -distribution leads to a dramatically different behavior in the tails, which is an important property in applications. See the section
   <i>
    Tail Dependence
   </i>
   .
  </p>
  <h4>
   Archimedean Copulas
  </h4>
  <p block-type="TextInlineMath">
   An important class of analytically tractable copulas are the Archimedean copulas. For the bivariate case, consider a continuous and strictly decreasing function
   <math display="inline">
    \phi : [0, 1] \longrightarrow [0, \infty]
   </math>
   with
   <math display="inline">
    \phi(1) = 0
   </math>
   , called the
   <i>
    generator
   </i>
   . Then
   <math display="inline">
    C(u_1, u_2)
   </math>
   given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \begin{cases} \phi^{-1}(\phi(u_1) + \phi(u_2)) &amp; \text{if } \phi(u_1) + \phi(u_2) \le \phi(0) \\ 0 &amp; \text{otherwise} \end{cases}
   </math>
   (13)
  </p>
  <p block-type="TextInlineMath">
   is a copula if and only if
   <math display="inline">
    \phi
   </math>
   is convex; see [18], Theorem 4.1.4. If
   <math display="inline">
    \phi(0) = \infty
   </math>
   the generator is said to be
   <i>
    strict
   </i>
   and
   <math display="inline">
    C(u_1, u_2) = \phi^{-1}(\phi(u_1) + \phi(u_2)).
   </math>
  </p>
  <p block-type="Text">
   For the multivariate case, there are different possibilities of generalization. A relatively special case is when the copula is of the form
  </p>
  <p block-type="Equation">
   <math display="block">
    \phi^{-1}(\phi(u_1) + \ldots + \phi(u_d)) \tag{14}
   </math>
  </p>
  <p block-type="TextInlineMath">
   These are the so-called exchangeable Archimedean copulas and
   <math display="inline">
    [15]
   </math>
   give a complete characterization of such
   <math display="inline">
    \phi
   </math>
   leading to a copula of the form (7). One may also consider asymmetric specifications of multivariate Archimedean copulas; see [14] Section 5.4.2 and 5.4.3. We present some examples of Archimedean copulas in bivariate case: from the generator
   <math display="inline">
    (-\ln u)^{\theta}
   </math>
   one obtains the bivariate Gumbel copula or Gumbel-Hougaard copula as follows:
  </p>
  <p block-type="Equation">
   <math display="block">
    \exp\left(-\left[(-\ln u_1)^{\theta} + (-\ln u_2)^{\theta}\right]^{\frac{1}{\theta}}\right) \tag{15}
   </math>
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   where
   <math display="inline">
    \theta \in [1, \infty)
   </math>
   . For
   <math display="inline">
    \theta = 1
   </math>
   it coincides with the independence copula, and for
   <math display="inline">
    \theta \rightarrow \infty
   </math>
   , it converges
  </p>
  <p block-type="Text">
   to the comonotonicity copula. The Gumbel copula has tail dependence in the upper right corner.
  </p>
  <p block-type="Text">
   The
   <i>
    Clayton copula
   </i>
   is given by&lt;sup&gt;g&lt;/sup&gt;
  </p>
  <p block-type="Equation">
   <math display="block">
    \left(\max\left\{u_1^{-\theta} + u_2^{-\theta} - 1, 0\right\}\right)^{-\frac{1}{\theta}} \tag{16}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \theta \in [-1, \infty) \setminus \{0\}
   </math>
   . For
   <math display="inline">
    \theta \to 0
   </math>
   it converges to the independence copula, and for
   <math display="inline">
    \theta \to \infty
   </math>
   to the comonotonicity copula. For
   <math display="inline">
    \theta = -1
   </math>
   we obtain the Fréchet-Hoeffding lower bound. The generator
   <math display="inline">
    \theta^{-1}(u^{-\theta}-1)
   </math>
   of the Clayton copula is strict only if
   <math display="inline">
    \theta &gt; 0
   </math>
   . In this case
  </p>
  <p block-type="Equation">
   <math display="block">
    C_{\theta}^{Cl}(u_1, u_2) = \left(u_1^{-\theta} + u_2^{-\theta} - 1\right)^{-\frac{1}{\theta}} \qquad (17)
   </math>
  </p>
  <p block-type="TextInlineMath">
   The generator
   <math display="inline">
    \ln(e^{-\theta}-1) - \ln(e^{-\theta u}-1)
   </math>
   leads to the Frank copula given by
  </p>
  <p block-type="Equation">
   <math display="block">
    -\frac{1}{\theta} \ln \left( 1 + \frac{(e^{-\theta u_1} - 1) \cdot (e^{-\theta u_2} - 1)}{e^{-\theta} - 1} \right) \tag{18}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for
   <math display="inline">
    \theta \in \mathbb{R} \setminus \{0\}
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The
   <i>
    generalized Clayton copula
   </i>
   is obtained from the generator
   <math display="inline">
    \theta^{-\delta}(u^{-\theta}-1)^{\delta}
   </math>
   :
  </p>
  <p block-type="Equation">
   <math display="block">
    \left( \left[ (u_1^{-\theta} - 1)^{\delta} + (u_2^{-\theta} - 1)^{\delta} \right]^{\frac{1}{\delta}} + 1 \right)^{-\frac{1}{\theta}} \tag{19}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with
   <math display="inline">
    \theta &gt; 0
   </math>
   and
   <math display="inline">
    \delta \ge 1
   </math>
   . Note that for
   <math display="inline">
    \delta = 1
   </math>
   the standard Clayton copula is to attained; compare [18], Example 4.19.
  </p>
  <p block-type="Text">
   Further examples of Archimedean copulas may be found in [18], and in particular one may consider Table 4.1 therein as well as Sections
   <math display="inline">
    4.5
   </math>
   and
   <math display="inline">
    4.6
   </math>
   (Table 1).
  </p>
  <p>
   Table 1 List of some copulas. For the Gumbel, Clayton, Frank and the Marshall-Olkin copula, only the bivariate versions are stated. References to the multivariate versions are given in the text. Further copulas may be found in [18], Table 4.1 (p. 94) and Sections 4.5 as well as 4.6
  </p>
  <table>
   <tbody>
    <tr>
     <th>
      Name
     </th>
     <th>
      Copula
     </th>
     <th>
      Paramter range
     </th>
    </tr>
    <tr>
     <td>
      Independence or product copula
     </td>
     <td>
      <math display="block">
       \n\Pi(\mathbf{u}) = \prod u_i\n
      </math>
     </td>
     <td>
     </td>
    </tr>
    <tr>
     <td>
      Comonotonicity copula or
      <br/>
      Fréchet–Hoeffding upper bound
     </td>
     <td>
      <math display="block">
       M(\mathbf{u}) = \min\left\{u_1, \ldots, u_d\right\}
      </math>
     </td>
     <td>
     </td>
    </tr>
    <tr>
     <td>
      Countermonotonicity copula or
      <br/>
      Fréchet–Hoeffding lower bound
     </td>
     <td>
      <math>
       W(u_1, u_2) = \max \{u_1 + u_2 - 1, 0\}
      </math>
     </td>
     <td>
     </td>
    </tr>
    <tr>
     <td>
      Gaussian copula
      <sup>
       (a)
      </sup>
     </td>
     <td>
      <math>
       C_{\mathbf{F}}^{Ga}(\mathbf{u}) = \Phi_{\mathbf{F}}(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_d))
      </math>
     </td>
     <td>
     </td>
    </tr>
    <tr>
     <td>
      <math>
       t
      </math>
      - or Student copula
      <sup>
       (a)
      </sup>
     </td>
     <td>
      <math>
       C_{v,\Gamma}^{t}(\mathbf{u}) = t_{v,\Gamma}(t_{v}^{-1}(u_{1}), \ldots, t_{v}^{-1}(u_{d}))
      </math>
     </td>
     <td>
     </td>
    </tr>
    <tr>
     <td>
      Gumbel copula or
      <br/>
      Gumbel–Hougaard copula
     </td>
     <td>
      <math display="block">
       C_{\theta}^{Gu}(u_1, u_2) = \exp\left(-\left[(-\ln u_1)^{\theta} + (-\ln u_2)^{\theta}\right]^{\frac{1}{\theta}}\right)
      </math>
     </td>
     <td>
      <math>
       \theta \in [1, \infty)
      </math>
     </td>
    </tr>
    <tr>
     <td>
      Clayton copula
      <math>
       ^{(b)}
      </math>
     </td>
     <td>
      <math>
       C_{\theta}^{Cl}(u_1, u_2) = (\max\{u_1^{-\theta} + u_2^{-\theta} - 1, 0\})^{-\frac{1}{\theta}}
      </math>
     </td>
     <td>
      <math>
       \theta \in [-1, \infty)
      </math>
     </td>
    </tr>
    <tr>
     <td>
      Generalized Clayton
      <sup>
       (b)
      </sup>
      copula
     </td>
     <td>
      <math display="block">
       C_{\theta,\delta}^{Cl}(u_1,u_2) = \left( \left[ (u_1^{-\theta} - 1)^{\delta} + (u_2^{-\theta} - 1)^{\delta} \right]^{\frac{1}{\delta}} + 1 \right)^{-\frac{1}{\theta}}
      </math>
     </td>
     <td>
      <math>
       \theta \ge 0, \delta \ge 1
      </math>
     </td>
    </tr>
    <tr>
     <td>
      <math>
       \text{Frank copula}^{(b)}
      </math>
     </td>
     <td>
      <math display="block">
       C_{\theta}^{Fr}(u_1, u_2) = -\frac{1}{\theta} \ln \left( 1 + \frac{(e^{-\theta u_1} - 1) \cdot (e^{-\theta u_2} - 1)}{e^{-\theta} - 1} \right)
      </math>
     </td>
     <td>
      <math>
       \theta \in \mathbb{R}
      </math>
     </td>
    </tr>
    <tr>
     <td>
      Marshall–Olkin copula or
      <br/>
      generalized Cuadras-Augé copula
     </td>
     <td>
      <math display="block">
       C_{\alpha_1,\alpha_2}(u_1,u_2) = \min \left\{ u_2 \cdot u_1^{1-\alpha_1}, u_1 \cdot u_2^{1-\alpha_2} \right\}
      </math>
     </td>
     <td>
      <math>
       \alpha_1, \alpha_2 \in [0, 1]
      </math>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <sup>
    &amp;
   </sup>
   lt;sup&gt;(a)&lt;/sup&gt;Here
   <math display="inline">
    \Gamma
   </math>
   is a correlation matrix, that is, a covariance matrix where each variance is scaled to 1
  </p>
  <p>
   <sup>
    &amp;
   </sup>
   lt;sup&gt;(b)&lt;/sup&gt; For the (generalized) Clayton and the Frank copula, the case
   <math display="inline">
    \theta = 0
   </math>
   is given as the limit for
   <math display="inline">
    \theta \to 0
   </math>
   , which leads to the independence copula in both cases
  </p>
  <h3>
   The Marshall-Olkin Copula
  </h3>
  <p block-type="Text">
   The
   <i>
    Marshall–Olkin copula
   </i>
   is a copula with singular component. For intuition, consider two components that are subject to certain shocks that lead to failure of either one or both the components. The shocks occur at times that are assumed to be independent and exponentially distributed. Denote the realized shock times by
   <math display="inline">
    Z_1, Z_2
   </math>
   and
   <math display="inline">
    Z_{12}
   </math>
   . Then we obtain for the probability that the two components live longer than
   <math display="inline">
    x_1
   </math>
   and
   <math display="inline">
    x_2
   </math>
   , respectively,
  </p>
  <p block-type="Equation">
   <math display="block">
    P(Z_1 &gt; x_1)P(Z_2 &gt; x_2)P(Z_{12} &gt; \max\{x_1, x_2\})
   </math>
   (20)
  </p>
  <p block-type="Text">
   This extends to the multivariate case in a straightforward way; compare [4] and [18]. The related copula equals
  </p>
  <p block-type="Equation">
   <math display="block">
    \min\left\{u_2 \cdot u_1^{1-\alpha_1}, u_1 \cdot u_2^{1-\alpha_2}\right\} \tag{21}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with
   <math display="inline">
    \alpha_i \in [0, 1]
   </math>
   . A similar family is given by the Cuadras-Augé copulas
  </p>
  <p block-type="Equation">
   <math display="block">
    \min \{u_1, u_2\} \cdot (\max \{u_1, u_2\})^{\alpha} \tag{22}
   </math>
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    \alpha \in [0, 1]
   </math>
   ; see [3].
  </p>
  <h2>
   <b>
    Measures of Dependence
   </b>
  </h2>
  <p block-type="Text">
   Measures of dependence summarize the dependence structures of rv's. There are three important concepts: linear correlation, rank correlation, and tail dependence. A further concept of dependence is association; see [6, 17].
  </p>
  <h4>
   Linear Correlation
  </h4>
  <p block-type="Text">
   Linear correlation is a well-studied concept. It is a dependence measure, which is useful only for elliptical distributions (see Multivariate Distribu
   <b>
    tions
   </b>
   ). This is because elliptical distributions are fully described by mean vector, covariance matrix, and a characteristic generator function. As mean and variances are determined by the marginal distributions, the copulas of elliptical distributions depend only on the covariance matrix and the generator function. Linear correlation, therefore, has a distinguished role in this class, which it does not have in other multivariate models.
  </p>
  <h4>
   Rank Correlation
  </h4>
  <p block-type="Text">
   Rank correlations describe the dependence structure of the ranks, that is, the dependence structure of the considered ry's when transformed to uniform marginals using the probability transformation. Most importantly, this implies a direct representation in terms of the underlying copula; compare equation
   <math display="inline">
    (5)
   </math>
   . We consider Kendall's tau and Spearman's rho, which also play an important role in nonparametric statistics.
  </p>
  <p block-type="TextInlineMath">
   For rv's
   <math display="inline">
    \mathbf{X} = X_1, \ldots, X_d
   </math>
   with marginals
   <math display="inline">
    F_i, i =
   </math>
   <math display="inline">
    1, \ldots, d
   </math>
   ,
   <i>
    Spearman's rho
   </i>
   is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \rho_S(\mathbf{X}) := \text{Corr}\left(F_1(X_1), \dots, F_d(X_d)\right) \tag{23}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Corr is the correlation matrix whose entries are given by
   <math display="inline">
    \text{Corr}(F_i(X_i), F_i(X_i)).
   </math>
  </p>
  <p block-type="TextInlineMath">
   Consider an independent copy
   <math display="inline">
    \tilde{\mathbf{X}}
   </math>
   of
   <math display="inline">
    \mathbf{X}
   </math>
   . Then
   <i>
    Kendall's tau
   </i>
   is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \rho_{\tau}(\mathbf{X}) := \text{Cov}\left[\text{sign}\left(\mathbf{X} - \tilde{\mathbf{X}}\right)\right] \tag{24}
   </math>
  </p>
  <p block-type="TextInlineMath">
   For
   <math display="inline">
    d = 2
   </math>
   .
  </p>
  <p block-type="Equation">
   <math display="block">
    \rho_{\tau}(X_1, X_2) = P\left( (X_1 - \tilde{X}_1) \cdot (X_2 - \tilde{X}_2) &gt; 0 \right)
   </math>
   <math display="block">
    -P\left( (X_1 - \tilde{X}_1) \cdot (X_2 - \tilde{X}_2) &lt; 0 \right)
   </math>
   (25)
  </p>
  <p block-type="Text">
   which explains this measure of dependency.
  </p>
  <p block-type="Text">
   Both measures have values in
   <math display="inline">
    [-1, 1]
   </math>
   ; they are 0 for independent variables (while there might also be nonindependent rv's with zero rank correlation) and they equal
   <math display="inline">
    1 (-1)
   </math>
   for the comonotonic (countermonotonic) case. Moreover, they can directly be derived from the copula of
   <math display="inline">
    X
   </math>
   ; see [14], Proposition 5.29. For example,
  </p>
  <p block-type="Equation">
   <math display="block">
    \rho_S(X_1, X_2) = 12 \int_0^1 \int_0^1 \left( C(u_1, u_2) - u_1 u_2 \right) \, \mathrm{d}u_1 \, \mathrm{d}u_2 \tag{26}
   </math>
  </p>
  <p block-type="TextInlineMath">
   In the case of a bivariate Gaussian copula one obtains&lt;sup&gt;h&lt;/sup&gt;
   <math display="inline">
    \rho_S(X_1, X_2) = \frac{6}{\pi} \arcsin \frac{\rho}{2}
   </math>
   and a similar expression for
   <math display="inline">
    \rho_{\tau}
   </math>
   .
  </p>
  <p block-type="Text" class="has-continuation">
   For other examples and certain bounds that interrelate those two measures, we refer the reader to [18],
  </p>
  <p block-type="Text">
   Sections
   <math display="inline">
    5.1.1-5.1.3
   </math>
   . For multivariate extensions see, for example, [21] and [23].
  </p>
  <h4>
   Tail Dependence
  </h4>
  <p block-type="TextInlineMath">
   We distinguish between
   <i>
    upper
   </i>
   and
   <i>
    lower
   </i>
   tail dependence. Consider two rv's
   <math display="inline">
    X_1
   </math>
   and
   <math display="inline">
    X_2
   </math>
   with marginals
   <math display="inline">
    F_1, F_2
   </math>
   and copula C. Upper tail dependence means intuitively that with large values of
   <math display="inline">
    X_1
   </math>
   also large values of
   <math display="inline">
    X_2
   </math>
   are expected. More precisely, the
   <i>
    coefficient
   </i>
   of upper tail dependence is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_u := \lim_{q \nearrow 1} P\left(X_2 &gt; F_2^{-1}(q) \middle| X_1 &gt; F_1^{-1}(q)\right) \tag{27}
   </math>
  </p>
  <p block-type="TextInlineMath">
   provided the limit exists and
   <math display="inline">
    \lambda_u \in [0, 1]
   </math>
   . The
   <i>
    coeffi
   </i>
   cient of lower tail dependence is
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_{l} := \lim_{q \searrow 0} P\left(X_{2} \leq F_{2}^{-1}(q) \middle| X_{1} \leq F_{1}^{-1}(q)\right) \quad (28)
   </math>
  </p>
  <p block-type="TextInlineMath">
   If
   <math display="inline">
    \lambda_u &gt; 0
   </math>
   ,
   <math display="inline">
    X_1
   </math>
   and
   <math display="inline">
    X_2
   </math>
   are called
   <i>
    upper tail dependent
   </i>
   , while for
   <math display="inline">
    \lambda_u = 0
   </math>
   they are asymptotically independent in the upper tail; this applies analogously for
   <math display="inline">
    \lambda_l
   </math>
   . For continuous cdf's, Bayes' rule gives
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_l = \lim_{q \searrow 0} \frac{C(q, q)}{q} \tag{29}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_u = 2 + \lim_{q \searrow 0} \frac{C(1-q, 1-q) - 1}{q} \tag{30}
   </math>
  </p>
  <p block-type="Text">
   A Gaussian copula has no tail dependence if the correlation is not equal to 1 or
   <math display="inline">
    -1
   </math>
   . For the bivariate
   <i>
    t
   </i>
   -distribution
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_l = \lambda_u = 2t_{\nu+1} \left( -\sqrt{\frac{(\nu+1)(1-\rho)}{1+\rho}} \right) \tag{31}
   </math>
  </p>
  <p block-type="TextInlineMath">
   provided
   <math display="inline">
    \rho &gt; -1
   </math>
   . Note that even for zero correlation this copula shows tail dependence.
  </p>
  <p block-type="Text">
   Tail dependence is a key quantity for joint quantile exceedances; see Example 5.34 in [14]: a multivariate Gaussian distribution will give a much smaller probability to the event that all returns from a portfolio are below the 1% quantiles of their respective distributions than a multivariate
   <math display="inline">
    t
   </math>
   -distribution. This is because of the difference in the tail dependence.
  </p>
  <h4>
   Association
  </h4>
  <p block-type="TextInlineMath">
   A relatively stronger concept than correlation is the so-called association introduced in [6]. If
   <math display="inline">
    Cov(X, Y) &gt; 0
   </math>
   , then one would consider X and Y as somehow associated. If, moreover,
   <math display="inline">
    Cov(f(X))
   </math>
   ,
   <math display="inline">
    g(Y) \ge 0
   </math>
   for all pairs of nondecreasing functions
   <math display="inline">
    f, g
   </math>
   , they would be considered more strongly associated. If
   <math display="inline">
    Cov(f(X, Y), g(X, Y)) &gt; 0
   </math>
   for all pairs of functions
   <math display="inline">
    f, g
   </math>
   which are nondecreasing in each argument, an even stronger dependence holds. Random variables
   <math display="inline">
    X_1, \ldots, X_d =: \mathbf{X}
   </math>
   are called
   <i>
    associated
   </i>
   if
   <math display="inline">
    \text{Cov}(f(\mathbf{X}), g(\mathbf{X})) \ge 0
   </math>
   for all
   <math display="inline">
    f, g
   </math>
   that are nondecreasing and the covariance exists. Examples of associated rv's include independent rv's, positively correlated normal variables, and also the generalized exponential distribution turning up in the Marshall-Olkin copula.
  </p>
  <h3>
   Sampling from Copulas
  </h3>
  <p block-type="TextInlineMath">
   Consider given marginals
   <math display="inline">
    F_1, \ldots, F_d
   </math>
   and a given copula C. The first step is to simulate
   <math display="inline">
    (U_1, \ldots, U_d)
   </math>
   with uniform marginals and copula
   <math display="inline">
    C
   </math>
   . By equation (3), the vector
   <math display="inline">
    (F_1^{-1}(U_1), \ldots, F_d^{-1}(U_d))
   </math>
   has copula
   <math display="inline">
    C
   </math>
   and the desired marginals.
  </p>
  <p block-type="Text">
   If the copula is inherited from a multivariate distribution, the task reduces to simulating this multivariate distribution, for example, Gaussian or
   <math display="inline">
    t
   </math>
   -distribution.
  </p>
  <p block-type="Text">
   If the copula is Archimedean, this task is more demanding and we refer the reader to [14], Algorithm 5.48 for details.
  </p>
  <h4>
   Conclusion
  </h4>
  <p block-type="Text">
   On one hand, copulas are a very general tool to describe dependence structures and have been successfully applied in many cases. However, the immense generality is also a drawback in many applications and also the static characteristic of this measure of dependence has been criticized; see the referenced literature. Obviously, the application of copulas has been a great success to a number of fields, and they are a frequently used concept especially in finance. They serve as an excellent tool for calibrating dependence structures or stress testing portfolios or other products in finance and insurance as they allow to interpolate between extreme cases of dependence.
  </p>
  <h1>
   <b>
    Literature
   </b>
  </h1>
  <p block-type="Text">
   The literature on copulas is growing fast. The vital article on copulas
   <b>
    Copulas in Insurance
   </b>
   by P. Embrechts gives an excellent overview of the literature and applications. An introduction to copulas that extends this note in many ways may be found in [20]. For a detailed exposition of copulas with different applications in view, we refer the reader to [4, 14, 19]. Reference [2] gives additional examples and [13] analyzes extreme financial risks. Estimation of copulas is discussed in [14], Section 5.5, [1] and [10]. For an in-depth study of copulas consider [11, 18]. Interesting remarks of the history and the development of copulas may be found in [7]. For more details on Marshall–Olkin copulas, in particular the multivariate ones, see [4, 18]. In the modeling of Levy ´ processes (
   <i>
    see
   </i>
   <b>
    Levy Processes ´
   </b>
   ) one considers dependency of jumps where the measure is no longer a probability measure. This leads to the development of so-called Levy copulas; compare [12] and ´
   <b>
    Levy ´ Copulas
   </b>
   . The pitfalls mentioned with linear correlation are discussed in detail in [5] or [14 Chapter 5.2.1]. For a discussion on the general difficulties in the application of copulas, we refer the reader to [9, 16].
  </p>
  <h2>
   <b>
    End Notes
   </b>
  </h2>
  <p block-type="Text">
   a
   <i>
    .
   </i>
   Although standard, it is not necessary to consider uniform marginals (
   <i>
    see
   </i>
   <b>
    Copulas in Insurance
   </b>
   ).
  </p>
  <p block-type="Text">
   b
   <i>
    .
   </i>
   One example is the exponential distributions whose multivariate extension leads to the Marshall–Olkin copula, introduced in the following paragraph.
  </p>
  <p block-type="Text">
   c
   <i>
    .
   </i>
   See, for example [14], Proposition 5.2.
  </p>
  <p block-type="Text">
   d
   <i>
    .
   </i>
   See [14], Example 5.21, for a counterexample.
  </p>
  <p block-type="Text">
   e
   <i>
    .
   </i>
   For a rv
   <i>
    X
   </i>
   with continuous cdf
   <i>
    F
   </i>
   , the rv
   <i>
    F (X)
   </i>
   is standard uniform, see Section 1.
  </p>
  <p block-type="Text">
   f
   <i>
    .
   </i>
   See
   <b>
    Correlation Risk
   </b>
   .
  </p>
  <p block-type="TextInlineMath">
   g
   <i>
    .
   </i>
   For generating the Clayton copula, it would be sufficient to use
   <i>
    (u
   </i>
   <sup>
    −
   </sup>
   <i>
    <sup>
     θ
    </sup>
   </i>
   − 1
   <i>
    )
   </i>
   instead of
   <i>
    θ
   </i>
   <sup>
    −
   </sup>
   <sup>
    1
   </sup>
   <i>
    (u
   </i>
   <sup>
    −
   </sup>
   <i>
    <sup>
     θ
    </sup>
   </i>
   − 1
   <i>
    )
   </i>
   as generator. However, for
   <i>
    θ &lt;
   </i>
   0, this function is increasing and the above result would not be applicable.
  </p>
  <p block-type="Text">
   h
   <i>
    .
   </i>
   Compare [14], Theorem 5.36.
   <i>
    ρS
   </i>
   and
   <i>
    ρτ
   </i>
   for elliptic distributions are also covered.
  </p>
  <h2>
   <b>
    Acknowledgments
   </b>
  </h2>
  <p block-type="Text">
   The author thanks F. Durante and R. Frey for helpful comments.
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Charpentier, A., Fermanian, J.-D. &amp; Scaillet, O. (2007). The estimation of copulas: from theory to practice, in
     <i>
      Copulas: From Theory to Applications in Finance
     </i>
     , J. Rank, ed, Risk Books, pp. 35–60.
    </li>
    <li block-type="ListItem">
     [2] Cherubini, U., Luciano, E. &amp; Vecchiato, W. (2004).
     <i>
      Copula Methods in Finance
     </i>
     , Wiley, Chichester.
    </li>
    <li block-type="ListItem">
     [3] Cuadras, C.M. &amp; Augee, J. (1981). A continuous general ´ multivariate distribution and its properties,
     <i>
      Communications in Statistics: Theory and Methods
     </i>
     <b>
      10
     </b>
     , 339–353.
    </li>
    <li block-type="ListItem">
     [4] Embrechts, P. Lindskog, F. &amp; McNeil, A.J. (2003). Modeling dependence with copulas and applications to risk management, in
     <i>
      Handbook of Heavy Tailed Distributions in Finance
     </i>
     , S.T. Rachev, ed, Elsevier, pp. 331–385.
    </li>
    <li block-type="ListItem">
     [5] Embrechts, P., McNeil, A.J. &amp; Straumann, D. (2001). Correlation and dependency in risk management: properties and pitfalls, in
     <i>
      Risk Management: Value at Risk and Beyond
     </i>
     , M. Dempster &amp; H.K. Moffatt, eds, Cambridge University Press, pp. 176–223.
    </li>
    <li block-type="ListItem">
     [6] Esary, J.D., Proschan, F. &amp; Walkup, D.W. (1967). Association of random variables, with applications,
     <i>
      Annals of Mathematical Statistics
     </i>
     <b>
      28
     </b>
     , 1466–1474.
    </li>
    <li block-type="ListItem">
     [7] Fisher, N.I. (1995). Copulas, in
     <i>
      Encyclopedia of Statistical Sciences
     </i>
     , S. Kotz, C. Read, N. Balakrishnan, &amp; B. Vidakovic, eds, Wiley, pp. 159–163.
    </li>
    <li block-type="ListItem">
     [8] Genest, C. &amp; Neslehov ˇ a, J. (2007). A primer on copulas ´ for count data,
     <i>
      Astin Bulletin
     </i>
     <b>
      37
     </b>
     , 475–515.
    </li>
    <li block-type="ListItem">
     [9] Genest, C. &amp; Remillard, B. (2006). Diskussion of "cop- ´ ulas: tales and facts", by Thomas Mikosch,
     <i>
      Extremes
     </i>
     <b>
      9
     </b>
     , 27–36.
    </li>
    <li block-type="ListItem">
     [10] Genest, C., Remillard, B. &amp; Beaudoin, D. (2007). ´ Goodness-of-fit tests for copulas: A review and a power study,
     <i>
      Insurance: Mathematics and Economics
     </i>
     in press, http://dx.doi.org/10.1016/j.insmatheco.2007.10.005.
    </li>
    <li block-type="ListItem">
     [11] Joe, H. (1997).
     <i>
      Multivariate Models and Dependence Concepts
     </i>
     , Chapman &amp; Hall, London.
    </li>
    <li block-type="ListItem">
     [12] Kallsen, J. &amp; Tankov, P. (2006). Characterization of dependence of multidimensional Levy processes using ´ Levy copulas, ´
     <i>
      Journal of Multivariate Analysis
     </i>
     <b>
      97
     </b>
     , 1551–1572.
    </li>
    <li block-type="ListItem">
     [13] Malevergne, Y. &amp; Sornette, D. (2006).
     <i>
      Extreme Financial Risks
     </i>
     , Springer Verlag, Berlin.
    </li>
    <li block-type="ListItem">
     [14] McNeil, A.J., Frey, R. &amp; Embrechts, P. (2005).
     <i>
      Quantitative Risk Management: Concepts, Techniques and Tools
     </i>
     , Princeton University Press.
    </li>
    <li block-type="ListItem">
     [15] McNeil, A.J. &amp; Neslehov ˇ a, J. (2008). Multivariate ´ Archimedean copulas,
     <i>
      d
     </i>
     -monotone functions and
     <i>
      l
     </i>
     1 norm symmetric distributions,
     <i>
      Annals of Statistics
     </i>
     , forthcoming.
    </li>
    <li block-type="ListItem">
     [16] Mikosch, T. (2006). Copulas: tales and facts,
     <i>
      Extremes
     </i>
     <b>
      9
     </b>
     , 3–20.
    </li>
    <li block-type="ListItem">
     [17] Muller, A. &amp; Stoyan, D. (2002). ¨
     <i>
      Comparison Methods for Stochastic Models and Risk
     </i>
     , John Wiley &amp; Sons, New York.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    8 Copulas: Estimation
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [18] Nelsen, R.B. (1999).
     <i>
      An Introduction to Copulas
     </i>
     ,
     <i>
      Lecture Notes in Statistics
     </i>
     , Springer Verlag, Berlin, Vol. 139.
    </li>
    <li block-type="ListItem">
     [19] Rank, J. (ed) (2007).
     <i>
      Copulas: from Theory to Applications in Finance
     </i>
     , Risk Books.
    </li>
    <li block-type="ListItem">
     [21] Schmidt, T. (2007). Coping with copulas, in
     <i>
      Copulas: From Theory to Applications in Finance
     </i>
     , J. Rank, ed, Risk Books, pp. 1–31.
    </li>
    <li block-type="ListItem">
     [20] Schmidt, F. &amp; Schmidt, R. (2007). Multivariate conditional versions of Spearmans' rho,
     <i>
      Journal of Multivariate Analysis
     </i>
     <b>
      98
     </b>
     , 1123–1140.
    </li>
    <li block-type="ListItem">
     [22] Sklar, A. (1959). Fonctions de repartition ´ a`
     <i>
      n
     </i>
     dimensions e leurs marges,
     <i>
      Publications de l'Institut de Statistique de l'Univiversit´e de Paris
     </i>
     <b>
      8
     </b>
     , 229–231.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   [23] Taylor, M.D. (2007). Multivariate measures of concordance,
   <i>
    Annals of the Institute of Statistical Mechanics
   </i>
   <b>
    59
   </b>
   , 789–806.
  </p>
  <h2>
   <b>
    Related Articles
   </b>
  </h2>
  <p block-type="Text">
   <b>
    Copulas in Econometrics
   </b>
   ;
   <b>
    Copulas in Insurance
   </b>
   ;
   <b>
    Correlation Risk
   </b>
   ;
   <b>
    Levy Copulas ´
   </b>
   ;
   <b>
    Operational Risk
   </b>
   ;
   <b>
    Risk Exposures
   </b>
   .
  </p>
  <p block-type="Text">
   THORSTEN SCHMIDT
  </p>
 </body>
</html>
