<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Weighted Monte Carlo
   </b>
  </h1>
  <p block-type="Text">
   Weighted Monte Carlo (WMC) is the name given to an algorithm used to build and calibrate asset-pricing models for financial derivatives. The algorithm combines two fixtures of the toolbox of quantitative modeling. One is Monte Carlo simulation to generate "paths" for rates and market prices on which derivatives are written [7, 9]. The other is the maximum entropy (ME) criterion, used to calculate a posteriori statistical weights for the paths. ME is one of the main tools in science for calculating a posteri
   <i>
    ori
   </i>
   probabilities in the presence of known constraints associated with the probability measure (see [8] for classical econometric applications of ME).
  </p>
  <p block-type="TextInlineMath">
   The essence of the method is as follows [3]: let
   <math display="inline">
    X_t(\omega)
   </math>
   ,
   <math display="inline">
    0 &lt; t &lt; T
   </math>
   ,
   <math display="inline">
    \omega \in \Omega
   </math>
   represent a model for the evolution of market variables or factors of interest. One of the most common applications is the case when
   <math display="inline">
    X_t
   </math>
   is a multivariate diffusion or jump diffusion process, for example,
  </p>
  <p block-type="Equation">
   <math display="block">
    dX_{\alpha t} = \sum_{j} \sigma_{\alpha j} dW_{jt} + \mu_{\alpha} dt,
   </math>
   <br/>
   <math display="block">
    1 \le \alpha \le n, \quad 1 \le j \le m \tag{1}
   </math>
  </p>
  <p block-type="Text">
   This process represents an
   <i>
    a priori
   </i>
   model for the joint forward evolution of the market. The parameters of the model,
   <math display="inline">
    \sigma
   </math>
   ,
   <math display="inline">
    \mu
   </math>
   typically correspond to econometrically estimated factors and expected returns. We note that, since the model is used for pricing derivatives. some of the parameters can also be implied from the prices of at-the-money options and forward prices. In the language of financial economics, the measure induced by
   <math display="inline">
    X_t
   </math>
   is either the "physical measure" or a hybrid of the physical measure and a risk-neutral measure with respect to select observable forwards and implied volatilities.
  </p>
  <p block-type="TextInlineMath">
   A Monte Carlo simulation of the ensemble with
   <math display="inline">
    N
   </math>
   paths is generated numerically, where the paths are denoted by
   <math display="inline">
    \omega_k
   </math>
   , that is, they can be viewed as a sampling of the probability space
   <math display="inline">
    \Omega
   </math>
   . The WMC algorithm calibrates the Monte Carlo model so that it fits the current market prices of
   <math display="inline">
    M
   </math>
   benchmarks or reference European-style derivatives, with discounted payoffs
   <math display="inline">
    g_1(\omega), g_2(\omega), \ldots, g_N(\omega)
   </math>
   and prices
   <math display="inline">
    c_1, c_2, \ldots, c_M
   </math>
   . We denote the discounted payoffs along the simulated
  </p>
  <p block-type="Text">
   paths by
  </p>
  <p block-type="Equation">
   <math display="block">
    G_{ik} = g_i(\omega_k), \ i = 1, \ldots, M, \ k = 1, \ldots, N \ (2)
   </math>
  </p>
  <p block-type="TextInlineMath">
   WMC associates a probability
   <math display="inline">
    p_k, k = 1, \dots N
   </math>
   to each path, in such a way that the pricing equations
  </p>
  <p block-type="Equation">
   <math display="block">
    c_i = \sum_{k=1}^{N} G_{ik} p_k \tag{3}
   </math>
  </p>
  <p block-type="TextInlineMath">
   or
   <math display="inline">
    c = Gp
   </math>
   in vector notation, hold for all indices i. Clearly, equation (3) states that the model reprices correctly the
   <math display="inline">
    M
   </math>
   reference instruments. In general, we assume that the number of simulation paths is much larger than the number of benchmarks (options, forwards), which is what happens in practical situations. The choice of the probabilities is done by applying the criterion of ME, that is, by maximizing
  </p>
  <p block-type="Equation">
   <math display="block">
    H(p_1, \ldots, p_N) = -\sum_{k=1}^{N} p_k \log p_k \qquad (4)
   </math>
  </p>
  <p block-type="Text">
   subject to the
   <math display="inline">
    M
   </math>
   constraints in equation (3). A least-squares version of the algorithm least squares weighted Monte Carlo (LSWMC) proposes to solve the problem
  </p>
  <p block-type="Equation">
   <math display="block">
    \min_{\mathbf{p}} \left\{ \sum_{i=1}^{M} \left( \sum_{k=1}^{N} G_{ik} p_{k} - c_{i} \right)^{2} - 2\epsilon H(\mathbf{p}) \right\} \qquad (5)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Here,
   <math display="inline">
    \epsilon &gt; 0
   </math>
   is a tolerance parameter that must be adjusted by the user. If
   <math display="inline">
    \epsilon \ll 1
   </math>
   , LSWMC corresponds to the classical WMC. For finite, relatively small, values of
   <math display="inline">
    \epsilon
   </math>
   the algorithm returns, an approximate solution of equation (3). In practice, the implementation
   <math display="inline">
    (5)
   </math>
   is recommended, since a solution will exist for arbitrary data
   <math display="inline">
    \{G_{ik}, c_i\}
   </math>
   .
  </p>
  <h4>
   <b>
    Dual Formulation
   </b>
  </h4>
  <p block-type="Text">
   The WMC (LSWMC) algorithm is usually solved in its
   <i>
    dual form
   </i>
   . Define the partition function
  </p>
  <p block-type="Equation">
   <math display="block">
    Z(\lambda_1,\ldots,\lambda_M) = \sum_{k=1}^N e^{\sum_{i=1}^M \lambda_i G_{ik}} \qquad (6)
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    \lambda_1, \ldots, \lambda_M
   </math>
   are Lagrange multipliers. The dual problem is
  </p>
  <p block-type="Equation">
   <math display="block">
    \min_{\lambda} \left\{ \log Z(\lambda) - \sum_{i=1}^{M} c_i \lambda_i + \frac{\epsilon}{2} \sum_{i=1}^{M} \lambda_i^2 \right\} \quad (7)
   </math>
  </p>
  <p block-type="TextInlineMath">
   The advantage of solving the dual problem is that the number of variables is
   <math display="inline">
    M
   </math>
   , hence much less than the number of simulated paths. It is well known that the latter problem is convex in
   <math display="inline">
    \lambda
   </math>
   and always admits a solution if
   <math display="inline">
    \epsilon &gt; 0
   </math>
   . Furthermore, the probabilities are given explicitly in terms of the multipliers, which solve the dual problem, namely,
  </p>
  <p block-type="Equation">
   <math display="block">
    p_k = \frac{1}{Z} e^{\sum_{i=1}^{M} \lambda_i G_{ik}} \ k = 1, 2, \dots, N \tag{8}
   </math>
  </p>
  <p block-type="Text">
   In practical implementations, the dual problem can be solved with a gradient-based convex optimization routine such as L-BFGS.
  </p>
  <h2>
   <b>
    Connection with Kullback-Leibler Relative Entropy
   </b>
  </h2>
  <p block-type="TextInlineMath">
   We can view WMC as an algorithm that minimizes, in a discrete setting, the relative entropy, or Kullback-Leibler distance between the prior probability measure induced by the paths (3) (call it
   <math display="inline">
    P_0
   </math>
   ) and the posterior measure induced by the probability vector
   <math display="inline">
    p
   </math>
   (call it
   <math display="inline">
    P
   </math>
   ), in the sense that it provides a solution of
  </p>
  <p block-type="Equation">
   <math display="block">
    \min_{P} \left\{ 2\epsilon D(P||P_0) + \sum_{i=1}^{M} \left[ \mathbb{E}^P(g_i(\omega)) - c_i \right]^2 \right\} \tag{9}
   </math>
  </p>
  <p block-type="Text">
   with
  </p>
  <p block-type="Equation">
   <math display="block">
    D(P||P_0) = \mathbf{E}^P \left( \log \frac{\mathrm{d}P}{\mathrm{d}P_0} \right) \tag{10}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \frac{dP}{dP_0}
   </math>
   is the Radon-Nikodym derivative of
   <math display="inline">
    P
   </math>
   with respect to
   <math display="inline">
    P_0
   </math>
   . The latter interpretation, however, should be taken with a grain of salt since the implementation is always done in the discrete setting, ensuring that the relative entropy between two measures defined on the paths of the MC simulation is always well defined (unlike in the continuous limit, where absolute continuity in Wiener space is often a stringent condition).
  </p>
  <h4>
   <b>
    Connection with Utility Maximization
   </b>
  </h4>
  <p block-type="Text">
   It can be shown,
   <i>
    via
   </i>
   an analysis of the dual problem, that the WMC algorithm gives a pricing measure, which corresponds to optimal investment by a representative investor in the reference instruments when this investor has an exponential utility [6].
  </p>
  <h4>
   Main Known Applications
  </h4>
  <p block-type="Text">
   Some of the most well-known applications of this method have been in the context of multiasset equity derivatives. In this case, the
   <i>
    a priori
   </i>
   measure corresponds to a multidimensional diffusion for stock prices, generated using a factor model (or model for the correlation matrix). The
   <i>
    a posteriori
   </i>
   measure is generated by calibrating to traded options on several underlying assets. For instance, the underlying stocks can be the components of the Nasdaq 100 index and the reference instruments all listed options on the underlying stocks. In the latter case, some care must be taken with the fact that listed options are American-style, but this difficulty can be overcome by generating prices of European options using the implied volatilities of the traded options. This yields a calibrated multiasset pricer for derivatives defined on the components of the Nasdaq 100. As a general rule, it is recommended to calibrate to forward prices (zero-strike calls) in addition to options, to ensure put-call parity in the
   <i>
    a posteriori
   </i>
   measure. The value
   <math display="inline">
    \epsilon = 0.25
   </math>
   seems to give results that are within the bid-ask spread of listed options contracts [1, 2].
  </p>
  <p block-type="Text">
   Another application of WMC is to the calibration of volatility surfaces for foreign-exchange (FX) options, to obtain a volatility surface that matches forward prices, at-the-money options, strangles, and risk-reversals on all available maturities. Owing to the nature of quotes in FX, the recommended value for the tolerance parameter should be of the order of
   <math display="inline">
    10^{-4}
   </math>
   in this case [1, 2].
  </p>
  <p block-type="Text">
   Applications of WMC have been also proposed in the context of credit derivatives, most notably for calibrating so-called top-down models [5].
  </p>
  <h4>
   Dispersion Trading
  </h4>
  <p block-type="Text">
   Dispersion trading corresponds to buying and selling index options and hedging with options on the component stocks. WMC gives a method for obtaining a model price for index options based on a model,
  </p>
  <p block-type="Text">
   which incorporates a view of the correlation between stocks (expressed in the
   <i>
    a priori
   </i>
   probability for
   <math display="inline">
    X_t
   </math>
   ) and is calibrated to all the options on the components of the index. Comparing the model price (or implied volatility) with the implied volatility of index options quoted in the market provides a rational setting for comparing the prices of index options with the prices of options on the components of the index. One of the important features of WMC is that it allows the user to incorporate views on the volatility skew/smile of the components in the valuation process
   <math display="inline">
    [1, 2]
   </math>
   .
  </p>
  <h4>
   Connection to Control Variates
  </h4>
  <p block-type="Text">
   The WMC framework can be generalized to any concave function
   <math display="inline">
    H(\mathbf{p})
   </math>
   . Avellaneda and Gamba [4] suggest, as one practical approach,
  </p>
  <p block-type="Equation">
   <math display="block">
    H(\mathbf{p}) = -\sum_{k=1}^{N} \Psi(p_k) \tag{11}
   </math>
  </p>
  <p block-type="Text">
   with
   <math display="inline">
    \Psi(\cdot)
   </math>
   being any convex function. Obvious choices are
  </p>
  <p block-type="Equation">
   <math display="block">
    \Psi^{(\mathbf{Q})}(p) = \left(p - \frac{1}{N}\right)^2 \quad \text{(Quadratic)} \tag{12}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \Psi^{(S)}(p) = p \log p \qquad \text{(Shannon)} \qquad (13)
   </math>
  </p>
  <p block-type="TextInlineMath">
   The problem of minimization of
   <math display="inline">
    -H(\mathbf{p})
   </math>
   subject to the constraints (probability normalization and calibration)
  </p>
  <p block-type="Equation">
   <math display="block">
    1 = \boldsymbol{p}^\top \boldsymbol{n} \quad \text{and} \quad \boldsymbol{c} = G \boldsymbol{p} \tag{14}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with the diagonal vector
   <math display="inline">
    \mathbf{n} := (1, \cdots, 1)^{\top} \in \mathbb{R}^{N}
   </math>
   (to simplify summation notation), leads to the Lagrange function
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathcal{L}(\boldsymbol{p}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = -H(\boldsymbol{p}) - \boldsymbol{\lambda}^{\top} (G \boldsymbol{p} - \boldsymbol{c}) - \boldsymbol{\mu} \left( \boldsymbol{p}^{\top} \boldsymbol{n} - 1 \right)
   </math>
   (15)
  </p>
  <p block-type="Text">
   Assuming the existence of an extremum, solving
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\! \boldsymbol{p}} \mathcal{L}(\boldsymbol{p}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = 0 \n\tag{16}
   </math>
  </p>
  <p block-type="Text">
   gives
  </p>
  <p block-type="Equation">
   <math display="block">
    p_k = \psi^{-1}(\boldsymbol{\lambda}^\top G \boldsymbol{e}_k + \mu) \tag{17}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with
   <math display="inline">
    \psi(p) = \frac{\mathrm{d}\Psi(p)}{\mathrm{d}p}
   </math>
   and
   <math display="inline">
    e_k
   </math>
   being the unit vector along the
   <i>
    k
   </i>
   -th axis in
   <math display="inline">
    \mathbb{R}^N
   </math>
   . For the specific choices (12) and
   <math display="inline">
    (13)
   </math>
   , this means
  </p>
  <p block-type="Equation">
   <math display="block">
    p_k^{(\mathbf{Q})} = \frac{1}{N} + \frac{1}{2} \boldsymbol{\lambda}^\top G \left( \boldsymbol{e}_k - \frac{1}{N} \boldsymbol{n} \right)
   </math>
   (Quadratic) (18)
  </p>
  <p block-type="Equation">
   <math display="block">
    p_k^{(S)} = \frac{e^{\lambda^\top G e_k}}{Z(\lambda)} \quad \text{(Shannon)} \tag{19}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with
   <math display="inline">
    Z(\lambda) = \sum_{k=1}^{N} e^{\lambda^{\top} G e_k}
   </math>
   , where we have eliminated
   <math display="inline">
    \mu
   </math>
   using the probability normalization condition 1 =
   <math display="inline">
    \boldsymbol{p}^\top \boldsymbol{n}
   </math>
   . Substituting equation (18) and, respectively, equation
   <math display="inline">
    (19)
   </math>
   , back into equation
   <math display="inline">
    (15)
   </math>
   leads to the Lagrange dual functions
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\mathcal{L}}^{(\mathbf{Q})}(\boldsymbol{\lambda}) = \boldsymbol{\lambda}^{\top} \left( \boldsymbol{c} - G \frac{\boldsymbol{n}}{N} \right) \n+ \frac{N}{2} \boldsymbol{\lambda}^{\top} G \left( \frac{\boldsymbol{n} \boldsymbol{n}^{\top}}{N^{2}} - \frac{1}{N} \right) G^{\top} \boldsymbol{\lambda} \qquad (20)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\mathcal{L}}^{(S)}(\lambda) = \lambda^{\top} c - \log Z(\lambda) \tag{21}
   </math>
  </p>
  <p block-type="Text">
   The dual formulation of the original problem is to find
  </p>
  <p block-type="Equation">
   <math display="block">
    \arg\max_{\lambda} \hat{\mathcal{L}}(\lambda) \tag{22}
   </math>
  </p>
  <p block-type="Text">
   For the quadratic case, this is guaranteed to have at least one solution given by the linear system
  </p>
  <p block-type="Equation">
   <math display="block">
    \frac{N}{2}G\left(\frac{1}{N} - \frac{\boldsymbol{n}\boldsymbol{n}^{\top}}{N^{2}}\right)G^{\top}\boldsymbol{\lambda}^{(\mathbf{Q})} = \boldsymbol{c} - G\frac{\boldsymbol{n}}{N} \qquad (23)
   </math>
  </p>
  <p block-type="Text">
   Note that
  </p>
  <p block-type="Equation">
   <math display="block">
    G\left(\frac{1}{N} - \frac{n\boldsymbol{n}^{\top}}{N^{2}}\right)G^{\top} = \langle \boldsymbol{g}\boldsymbol{g}^{\top}\rangle_{N}^{P_{0}} - \langle \boldsymbol{g}\rangle_{N}^{P_{0}}\langle \boldsymbol{g}\rangle_{N}^{P_{0}\top} (24)
   </math>
   <math display="block">
    = \langle \boldsymbol{g}, \boldsymbol{g}\rangle_{N}^{P_{0}} \tag{25}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \langle \cdot \rangle_N^{P_0}
   </math>
   stands for the Monte Carlo estimator of the expectation under the original measure
   <math display="inline">
    P_0
   </math>
   computed as the plain average over the
   <math display="inline">
    N
   </math>
   simulated paths, and
   <math display="inline">
    \langle \cdot, \cdot \rangle_N^{P_0}
   </math>
   for the according covariance (defined such that
   <math display="inline">
    \langle \boldsymbol{a}, \boldsymbol{b} \rangle = \langle \boldsymbol{b}, \boldsymbol{a} \rangle^{\top}
   </math>
   ). In other words,
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{\lambda}^{(\mathrm{Q})} = \frac{2}{N} \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P_0^{-1}} \cdot \left( \boldsymbol{c} - \langle \boldsymbol{g} \rangle_N^{P_0} \right) \tag{26}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{p}^{(\mathrm{Q})} = \frac{\boldsymbol{n}}{N} + \left(\frac{\boldsymbol{n}}{N} - \frac{\boldsymbol{n}\boldsymbol{n}^{\top}}{N^{2}}\right)G^{\top}
   </math>
   <math display="block">
    \cdot \left\langle \boldsymbol{g}, \boldsymbol{g} \right\rangle_{N}^{P_{0}^{-1}} \cdot \left(\boldsymbol{c} - \left\langle \boldsymbol{g} \right\rangle_{N}^{P_{0}}\right) \tag{27}
   </math>
  </p>
  <p block-type="Text">
   Note that the inverse of the autocovariance matrix of the calibration instruments is to be understood in a Moore-Penrose sense to safeguard against the singular case.
  </p>
  <p block-type="TextInlineMath">
   When using these probabilities for the valuation of a payoff v, with
   <math display="inline">
    v_k := v(\omega_k)
   </math>
   , we arrive at
  </p>
  <p block-type="Equation">
   <math display="block">
    \begin{split} \langle v \rangle_N^{P^{(\mathbf{Q})}} &amp;= \boldsymbol{v}^\top \boldsymbol{p}^{(\mathbf{Q})} \\ &amp;= \langle v \rangle_N^{P_0} + \langle v, \boldsymbol{g} \rangle_N^{P_0} \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P_0^{-1}} \end{split} \tag{28}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \cdot \left( \boldsymbol{c} - \langle \boldsymbol{g} \rangle_N^{P_0} \right) \tag{29}
   </math>
  </p>
  <p block-type="Text">
   which is identical to the classic control variate rule [7, 9].
  </p>
  <p block-type="Text">
   For
   <math display="inline">
    \mathcal{L}^{(S)}(\lambda)
   </math>
   , a second-order expansion in
   <math display="inline">
    \lambda
   </math>
   around zero gives
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\mathcal{L}}^{(\mathrm{S})} = -\log N + \boldsymbol{\lambda}^{\top} \left( \boldsymbol{c} - \langle \boldsymbol{g} \rangle_N^{P_0} \right) \n- \frac{1}{2} \boldsymbol{\lambda}^{\top} \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P_0} \boldsymbol{\lambda} + \mathcal{O}(\boldsymbol{\lambda}^3)
   </math>
   (30)
  </p>
  <p block-type="Text">
   and hence we obtain the analytical initial guess
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{\lambda}^{(\mathrm{S}),1} := \frac{N}{2} \boldsymbol{\lambda}^{(\mathrm{Q})} \tag{31}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for any iterative procedure to solve (22). A simple algorithm can be based on a second-order expansion of
   <math display="inline">
    \hat{\mathcal{L}}^{(S)}(\lambda)
   </math>
   around the previous iteration's estimate for
   <math display="inline">
    \lambda^{(S)}
   </math>
   . This gives
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{\lambda}^{(\mathrm{S}),i+1} = \boldsymbol{\lambda}^{(\mathrm{S}),i} + \left(G\boldsymbol{\Pi}^{(\mathrm{S}),i}G^{\top} - G\boldsymbol{p}^{(\mathrm{S}),i}\boldsymbol{p}^{(\mathrm{S}),i^{\top}}G^{\top}\right)^{-1} \times (\boldsymbol{c} - G\boldsymbol{p}^{(\mathrm{S}),i}) \tag{32}
   </math>
  </p>
  <p block-type="Text">
   with
  </p>
  <p block-type="Equation">
   <math display="block">
    \Pi^{(\mathcal{S}),i} := \text{diag}\left(p_1^{(\mathcal{S}),i}, \cdots, p_N^{(\mathcal{S}),i}\right) \in \mathbb{R}^{N \times N} \tag{33}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    p_k^{(\mathbf{S}),i} = p_k^{(\mathbf{S})}(\boldsymbol{\lambda}^{(\mathbf{S}),i}) \tag{34}
   </math>
  </p>
  <p block-type="Text">
   as defined in equation (19). Interestingly, the term
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    G\mathbf{p}^{(\mathrm{S}),i}
   </math>
  </p>
  <p block-type="TextInlineMath">
   in equation
   <math display="inline">
    (32)
   </math>
   is the vector of expectations for the
   <math display="inline">
    M
   </math>
   calibration instruments under the (numerical) measure
   <math display="inline">
    P^{(S),i}
   </math>
   defined by the numerically computed vector of probabilities
   <math display="inline">
    \boldsymbol{p}^{(S),i}
   </math>
   , and
  </p>
  <p block-type="Equation">
   <math display="block">
    \left(G\Pi^{(\mathrm{S}),i}G^{\top} - G\boldsymbol{p}^{(\mathrm{S}),i}\boldsymbol{p}^{(\mathrm{S}),i}^{\top}G^{\top}\right) \qquad (35)
   </math>
  </p>
  <p block-type="TextInlineMath">
   is the associated (numerical) covariance matrix of the calibration instruments. The simple algorithm is thus, in a formal notation, to start with
   <math display="inline">
    \lambda^{(S),0} = 0
   </math>
   (in all entries of the vector), to compute
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{p}^{(\mathrm{S}),i} = \boldsymbol{p}^{(\mathrm{S})} \left( \boldsymbol{\lambda}^{(\mathrm{S}),i} \right) \tag{36}
   </math>
  </p>
  <p block-type="Text">
   using equation
   <math display="inline">
    (19)
   </math>
   , to proceed to
  </p>
  <p block-type="Equation">
   <math display="block">
    \boldsymbol{\lambda}^{(\mathrm{S}),i+1} = \boldsymbol{\lambda}^{(\mathrm{S}),i} + \langle \boldsymbol{g}, \boldsymbol{g} \rangle_{N}^{P^{(\mathrm{S}),i}}
   </math>
   <math display="block">
    \cdot \left( \boldsymbol{c} - \langle \boldsymbol{g} \rangle_{N}^{P^{(\mathrm{S}),i}} \right) \tag{37}
   </math>
  </p>
  <p block-type="TextInlineMath">
   and to
   <math display="inline">
    \boldsymbol{p}^{(S),i+1}
   </math>
   , and so on.
  </p>
  <p block-type="TextInlineMath">
   It is, in general, possible that a solution to equation (22) may not exist for
   <math display="inline">
    \hat{\mathcal{L}}^{(S)}
   </math>
   if the model's initial calibration implies prices for the calibration instruments that are too far away from
   <math display="inline">
    c
   </math>
   . When this happens, any iterative procedure will experience that
   <math display="inline">
    \hat{\mathcal{L}}^{(S)}(\lambda)
   </math>
   grows at an ever decreasing rate in some direction in
   <math display="inline">
    \mathbb{R}^M
   </math>
   , and, eventually, the solver will terminate when it hits an internal minimum-progress criterion. A numerical approximation for
   <math display="inline">
    \lambda^{(S)}
   </math>
   computed in this way will represent an ME best-possible fit, and is still usable in a vein similar to that obtained by the least squares approach mentioned in the beginning. An inexpensive warning indication for this situation is given when any of the
   <math display="inline">
    p_k^{(Q)}
   </math>
   are negative. Note that this then also signals that the classic control variate method implicitly uses a (numerical) measure that is not equivalent to the original model's measure, which in turn may result in arbitrageable prices.
  </p>
  <h4>
   Hedge Ratios
  </h4>
  <p block-type="Text">
   The fact that the fine tuning of the pricing measure
   <math display="inline">
    P
   </math>
   is achieved by varying the probabilities of the paths such that hedge instruments are correctly repriced allows for the calculation of hedge ratios without recalibration of the original model, and without resimulation. This can be seen as follows. We seek to compute the sensitivity of
   <math display="inline">
    \langle v \rangle_N^P
   </math>
   with respect to the calibration prices c. Since the probability vector
   <math display="inline">
    p(\lambda)
   </math>
   is computed as an analytical function of the Lagrange multipliers, which in turn are computed numerically from
   <math display="inline">
    c
   </math>
   , we have
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\!c} \langle v \rangle_N^P = \nabla_{\!c} (\boldsymbol{p}^\top \boldsymbol{v}) = J \cdot \nabla_{\!a} \boldsymbol{p}^\top \boldsymbol{v} \tag{38}
   </math>
  </p>
  <p block-type="Text">
   with the elements of the Jacobian matrix
   <math display="inline">
    J
   </math>
   given by
  </p>
  <p block-type="Equation">
   <math display="block">
    J_{lm} = \partial_{c_l} \lambda_m \tag{39}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Given any
   <math display="inline">
    \Psi
   </math>
   , which, together with the calibration constraints, ultimately defines our desired pricing measure
   <math display="inline">
    P
   </math>
   , we can combine equation (17) and the probability normalization condition
   <math display="inline">
    1 = \mathbf{p}^\top \mathbf{n}
   </math>
   to arrive at
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\!c} \langle v \rangle_N^P = s^{P_{\rm H}} \cdot J \cdot \langle \boldsymbol{g}, v \rangle_N^{P_{\rm H}} \tag{40}
   </math>
  </p>
  <p block-type="Text">
   where we have defined the hedge measure
   <math display="inline">
    P_{\rm H}
   </math>
   in terms of the (numerical) probabilities
   <math display="inline">
    \boldsymbol{p}^{P_{\rm H}}
   </math>
   whose elements are given by
  </p>
  <p block-type="Equation">
   <math display="block">
    s^{P_{\rm H}} := \sum_{k=1}^{N} 1/\psi'\left(p_{k}^{P}\right) \quad \text{with}
   </math>
   <math display="block">
    p_{k}^{P_{\rm H}} := \frac{1/\psi'\left(p_{k}^{P}\right)}{s^{P_{\rm H}}} \tag{41}
   </math>
  </p>
  <p block-type="Text">
   What remains to be calculated is the Jacobian
   <math display="inline">
    J
   </math>
   . This can be done in one of three ways, depending on the choice of
   <math display="inline">
    \Psi
   </math>
   :
  </p>
  <p block-type="Text">
   1. Analytically (explicitly). For instance, for
   <math display="inline">
    \Psi^{(Q)}
   </math>
   , we obtain
   <math display="inline">
    s^{P_{\rm H}^{(Q)}} = 1
   </math>
   ,
   <math display="inline">
    p_k^{P_{\rm H}^{(Q)}} = \frac{1}{N}
   </math>
   ,
   <math display="inline">
    P_{\rm H}^{(Q)} = P_0
   </math>
   , and therefore
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\!\!c} \langle v \rangle_N^{P^{(Q)}} = \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P_0^{-1}} \cdot \langle \boldsymbol{g}, v \rangle_N^{P_0} \tag{42}
   </math>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     2. Numerically. If
     <math display="inline">
      \lambda^P
     </math>
     is computed by an iterative procedure that starts with no information other than the simulated paths and
     <math display="inline">
      c
     </math>
     itself as indicated in equation (32), the chain rule propagation can be derived and implemented as part of the iterative procedure. This approach may have to be chosen if no solution to equation
     <math display="inline">
      (22)
     </math>
     exists. An alternative would be to precalibrate the original model better such that a Monte Carlo weighting scheme can be found that reprices the calibration instruments exactly.
    </li>
    <li block-type="ListItem">
     3. Analytically (implicitly). As long as a solution to equation
     <math display="inline">
      (22)
     </math>
     exists, that is, as long as the WMC scheme reprices the calibration instruments correctly, we can use the fact that
     <math display="inline">
      \nabla_{\!c} \langle g \rangle_N^P
     </math>
     must be the
     <math display="inline">
      M \times M
     </math>
     identity matrix. This gives the generic result
    </li>
   </ul>
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\!\!c} \langle v \rangle_N^P = \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P_{\rm H} - 1} \cdot \langle \boldsymbol{g}, v \rangle_N^{P_{\rm H}} \qquad (43)
   </math>
  </p>
  <p block-type="TextInlineMath">
   which means that for any
   <math display="inline">
    P_0
   </math>
   and
   <math display="inline">
    P
   </math>
   , that is,
   <math display="inline">
    \Psi
   </math>
   , that permit perfect repricing of the hedges (calibration instruments) under
   <math display="inline">
    P
   </math>
   , the hedge ratios for any payoff
   <math display="inline">
    v
   </math>
   can be seen as a regression of the covariances between
   <math display="inline">
    v
   </math>
   and the hedge instruments against the autocovariances of the hedge instruments under the calibration-adjusted measure
   <math display="inline">
    P_{\rm H}
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   It is worth mentioning that for
   <math display="inline">
    \Psi^{(S)}(p) = p \log p
   </math>
   , we obtain
   <math display="inline">
    s^{P_{\rm H}^{(\rm S)}} = 1, \ p_k^{P_{\rm H}^{(\rm S)}} = p_k^{P_{\rm H}^{(\rm S)}}, \ P_{\rm H}^{(\rm S)} = P^{(\rm S)}, \text{ and}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \nabla_{\!\!c} \langle v \rangle_N^{P^{(\mathrm{S})}} = \langle \boldsymbol{g}, \boldsymbol{g} \rangle_N^{P^{(\mathrm{S})}-1} \cdot \langle \boldsymbol{g}, v \rangle_N^{P^{(\mathrm{S})}} \tag{44}
   </math>
  </p>
  <p block-type="Text">
   In other words, the calibration-adjusted measure is the same as the pricing measure. This is a special property of the Shannon entropy pricing measure
   <math display="inline">
    p(S)
   </math>
  </p>
  <p block-type="Text">
   As a final note on hedge ratio calculations with WMC, it should be noted that unlike most of the other sensitivity calculation schemes used with Monte Carlo methods, the above shown analysis results directly in
   <i>
    hedge ratios
   </i>
   , bypassing the otherwise common intermediate stage of
   <i>
    model parameter sen
   </i>
   sitivities, which require remapping to hedge ratios for tradable instruments. This feature greatly reduces the noise often observed on risk figures that are computed by numerically fitting model parameters to market observable prices since the noise-compounding effects of recalibration and numerical calculation of sensitivities of hedge instrument prices to model parameters are avoided.
  </p>
  <h3>
   References
  </h3>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Avellaneda, M. (2002). Empirical Aspects of Dispersion Trading in the US Equities Markets. Powerpoint presentation, Courant Institute of Mathematical Sciences, New York University, November 2002. www.math.nyu.edu/ faculty/avellane/ParisFirstTalkSlides.pdf
    </li>
    <li block-type="ListItem">
     [2] Avellaneda, M. (2002). Weighted-Monte Carlo Methods for Equity Derivatives: Theory and Practice, Powerpoint presentation, Courant Institute of Mathematical Sciences, New York University, November 2002. www.math.nyu. edu/faculty/avellane/ParisTalk2.pdf
    </li>
    <li block-type="ListItem">
     Avellaneda, M., Buff, R., Friedman, C., Grandchamp, N., [3] Kruk, L. &amp; Newman, J. (2001). Weighted Monte Carlo: a new approach for calibrating asset-pricing models, International Journal of Theoretical and Applied Finance,
     <math display="inline">
      4(1), 91-119.
     </math>
    </li>
    <li block-type="ListItem">
     [4] Avellaneda, M. &amp; Gamba, R. (2000). Conquering the Greeks in Monte Carlo: efficient calculation of the market
    </li>
   </ul>
  </p>
  <p block-type="Text">
   sensitivities and Hedge-Ratios of financial assets by direct numerical simulation,
   <i>
    Mathematical Finance–Bachelier Congress 2000
   </i>
   , Monte Carlo, pp. 93–109, 2000/2002. www.math.nyu.edu/faculty/avellane/ConqueringThe Greeks.pdf
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [5] Cont, R. &amp; Minca, A. (2008).
     <i>
      Recovering Portfolio Default Intensities Implied by CDO Quotes
     </i>
     , Financial engineering report no. 2008-01, Columbia University Center for Financial Engineering, ssrn.com/ abstract=1104855.
    </li>
    <li block-type="ListItem">
     [6] Delbaen, F., Grandits, P., Rheinlander, T., Samperi, D., Schweizer, M. &amp; Stricker, C. (2002). Exponential hedging
    </li>
   </ul>
  </p>
  <p block-type="Text">
   and entropic penalties,
   <i>
    Mathematical Finance
   </i>
   <b>
    12
   </b>
   , 99–123, ssrn.com/abstract=312802.
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [7] Glasserman, P. (2003).
     <i>
      Monte Carlo Methods in Financial Engineering
     </i>
     , Springer.
    </li>
    <li block-type="ListItem">
     [8] Golan, A., Judge, G. &amp; Miller, D. (1996).
     <i>
      Maximum Entropy Econometrics
     </i>
     , John Wiley &amp; Sons.
    </li>
    <li block-type="ListItem">
     [9] Jackel, P. (2002). ¨
     <i>
      Monte Carlo Methods in Finance
     </i>
     , John Wiley &amp; Sons.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   MARCO AVELLANEDA &amp; PETER JACKEL ¨
  </p>
 </body>
</html>
