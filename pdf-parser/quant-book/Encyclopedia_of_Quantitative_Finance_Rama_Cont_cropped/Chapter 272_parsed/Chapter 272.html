<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Quasi-Monte Carlo Methods
   </b>
  </h1>
  <p block-type="Text">
   Many typical problems of modern computational finance can be rephrased mathematically as problems of calculating integrals with high-dimensional integration domains (see the section Applications to Computational Finance for several examples). In fact, the dimensions may very well be in the hundreds or even in the thousands. Very often in such finance problems the integrand will be quite complicated, so that the integral cannot be evaluated analytically and precisely. In such cases, one has to resort to
   <i>
    numerical integration
   </i>
   , that is, to a numerical scheme for the approximation of integrals.
  </p>
  <p block-type="Text">
   High-dimensional numerical integration is a challenging problem. Classical methods for multidimensional numerical integration, namely Cartesian products of one-dimensional integration rules such as the trapezoidal rule and Simpson's rule (see [15]), work well only for dimensions up to three or four, or if the given high-dimensional integral can be reduced to a low-dimensional integral by analytic tricks. For most high-dimensional integrals arising in finance, the classical methods fail.
  </p>
  <p block-type="Text">
   A more powerful approach to multidimensional numerical integration employs Monte Carlo methods. In a nutshell, a
   <i>
    Monte Carlo method
   </i>
   is a numerical method based on random sampling. A comprehensive treatment of Monte Carlo methods can be found in [20].
  </p>
  <p block-type="TextInlineMath">
   Monte Carlo methods for numerical integration can be explained in a straightforward manner. In many cases, by using suitable transformations, we can assume that the integration domain is an
   <i>
    s
   </i>
   -dimensional unit cube
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   := [0
   <i>
    ,
   </i>
   1]
   <i>
    <sup>
     s
    </sup>
   </i>
   , so this is the situation on which we focus. We assume also that the integrand
   <i>
    f
   </i>
   is square integrable over
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   . Then the
   <i>
    Monte Carlo approximation
   </i>
   for the integral is
  </p>
  <p block-type="Equation">
   <math display="block">
    \int_{I^s} f(\mathbf{u}) \, \mathrm{d} \mathbf{u} \approx \frac{1}{N} \sum_{n=1}^N f(\mathbf{x}_n) \tag{1}
   </math>
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   where
   <b>
    x
   </b>
   1
   <i>
    ,...,
   </i>
   <b>
    x
   </b>
   <i>
    <sup>
     N
    </sup>
   </i>
   are independent random samples drawn from the uniform distribution on
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   . In statistics, we approximate the expected value of a random variable by sample means. The law of large numbers
  </p>
  <p block-type="Text">
   guarantees that with probability 1 (i.e., for "almost all" sequences of sample points) we have
  </p>
  <p block-type="Equation">
   <math display="block">
    \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_n) = \int_{I^s} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} \tag{2}
   </math>
  </p>
  <p block-type="Text">
   and so the Monte Carlo method for numerical integration converges almost surely.
  </p>
  <p block-type="TextInlineMath">
   We can, in fact, be more precise about the error committed in the Monte Carlo approximation (1). It can be verified quite easily that the square of the error in equation (1) is, on the average over all samples of size
   <i>
    N
   </i>
   , equal to
   <i>
    σ
   </i>
   <sup>
    2
   </sup>
   <i>
    (f )N
   </i>
   <sup>
    −
   </sup>
   1, where
   <i>
    σ
   </i>
   <sup>
    2
   </sup>
   <i>
    (f )
   </i>
   is the variance of
   <i>
    f
   </i>
   . Thus, with overwhelming probability we have
  </p>
  <p block-type="Equation">
   <math display="block">
    \int_{I^{s}} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} - \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_{n}) = O(N^{-1/2}) \quad (3)
   </math>
  </p>
  <p block-type="TextInlineMath">
   This means, very roughly, that if we want to compute the given integral with an error tolerance of the order 10
   <sup>
    −
   </sup>
   2, say, then we need about 104 sample points, and to reduce the error tolerance by one order of magnitude, we need to increase the sample size by a factor of about 100. An important fact here is that the convergence rate in equation (3) is independent of the dimension
   <i>
    s
   </i>
   , and this makes Monte Carlo methods attractive for high-dimensional problems.
  </p>
  <p block-type="Text">
   Despite the initial appeal of Monte Carlo methods for numerical integration, there are several drawbacks of these methods: (i) it is difficult to generate truly random samples; (ii) Monte Carlo methods for numerical integration provide only probabilistic error bounds; and (iii) in many applications the convergence rate in equation (3) is considered too slow. Quasi-Monte Carlo (QMC) methods were introduced to address these concerns.
  </p>
  <h1>
   <b>
    General Background on QMC Methods
   </b>
  </h1>
  <p block-type="Text" class="has-continuation">
   A
   <i>
    QMC method
   </i>
   is a deterministic version of a Monte Carlo method, in the sense that the random samples used in the implementation of a Monte Carlo method are replaced by
   <i>
    quasi-random points
   </i>
   , which are judiciously chosen deterministic points with good distribution properties. The general idea is that the Monte Carlo error bound (3) describes the average performance of integration points
   <b>
    x
   </b>
   1
   <i>
    ,...,
   </i>
   <b>
    x
   </b>
   <i>
    <sup>
     N
    </sup>
   </i>
   , and there should exist points that perform better than
  </p>
  <p block-type="Text">
   average. These are the quasi-random points we are seeking.
  </p>
  <p block-type="TextInlineMath">
   We again consider these methods in the context of numerical integration over an
   <math display="inline">
    s
   </math>
   -dimensional unit cube
   <math display="inline">
    I^s = [0, 1]^s
   </math>
   . The approximation scheme is the same as for the Monte Carlo method, namely
  </p>
  <p block-type="Equation">
   <math display="block">
    \int_{I^{s}} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} \approx \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_{n}) \tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   but now
   <math display="inline">
    \mathbf{x}_1, \ldots, \mathbf{x}_N
   </math>
   are deterministic points in
   <math display="inline">
    I^s
   </math>
   . For such a deterministic numerical integration scheme we expect a deterministic error bound, and this is indeed provided by the Koksma–Hlawka inequality. It depends on the star discrepancy, a measure for the irregularity of distribution of a point set
   <math display="inline">
    P
   </math>
   consisting of
   <math display="inline">
    \mathbf{x}_1, \ldots, \mathbf{x}_N \in I^s
   </math>
   . For any Borel set
   <math display="inline">
    M \subseteq I^s
   </math>
   , let
   <math display="inline">
    A(M; P)
   </math>
   be the number of integers n, with
   <math display="inline">
    1 \le n \le
   </math>
   <i>
    N
   </i>
   such that
   <math display="inline">
    \mathbf{x}_n
   </math>
   ∈
   <i>
    M
   </i>
   . We put
  </p>
  <p block-type="Equation">
   <math display="block">
    R(M;P) = \frac{A(M;P)}{N} - \lambda_s(M) \tag{5}
   </math>
  </p>
  <p block-type="TextInlineMath">
   which is the difference between the relative frequency of the points of
   <math display="inline">
    P
   </math>
   in
   <math display="inline">
    M
   </math>
   and the s-dimensional Lebesgue measure
   <math display="inline">
    \lambda_s(M)
   </math>
   of M. If the points of P have a very uniform distribution over
   <math display="inline">
    I^s
   </math>
   , then the values of
   <math display="inline">
    R(M; P)
   </math>
   will be close to 0 for a reasonable collection of Borel sets, such as for all subintervals of
   <math display="inline">
    I^s
   </math>
   .
  </p>
  <p block-type="Text">
   <b>
    Definition 1
   </b>
   The star discrepancy of the point set
   <math display="inline">
    P
   </math>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    D_N^* = D_N^*(P) = \sup_J |R(J;P)| \tag{6}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where the supremum is extended over all intervals
   <math display="inline">
    J = \prod_{i=1}^{s} [0, u_i]
   </math>
   with
   <math display="inline">
    0 &lt; u_i \le 1
   </math>
   for
   <math display="inline">
    1 \le i \le s
   </math>
   .
  </p>
  <h4>
   Koksma-Hlawka Inequality
  </h4>
  <p block-type="TextInlineMath">
   For any function
   <math display="inline">
    f
   </math>
   of bounded variation
   <math display="inline">
    V(f)
   </math>
   on
   <math display="inline">
    I
    <sup>
     s
    </sup>
   </math>
   in the sense of Hardy and Krause and any points
   <math display="inline">
    \mathbf{x}_1,\ldots,\mathbf{x}_N\in[0,1)^s
   </math>
   , we have
  </p>
  <p block-type="Equation">
   <math display="block">
    \left| \int_{I^s} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} - \frac{1}{N} \sum_{n=1}^N f(\mathbf{x}_n) \right| \le V(f) D_N^* \qquad (7)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    D_N^*
   </math>
   is the star discrepancy of
   <math display="inline">
    \mathbf{x}_1, \ldots, \mathbf{x}_N
   </math>
   .
  </p>
  <p block-type="Text">
   Note that
   <math display="inline">
    V(f)
   </math>
   is a measure for the oscillation of the function
   <math display="inline">
    f
   </math>
   . The precise definition of the variation
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    V(f)
   </math>
   can be found in [49, p. 19]. For
   <math display="inline">
    f(\mathbf{u}) =
   </math>
   <math display="inline">
    f(u_1,\ldots,u_s)
   </math>
   , a sufficient condition for
   <math display="inline">
    V(f)
   </math>
   <math display="inline">
    \infty
   </math>
   is that the partial derivative
   <math display="inline">
    \partial^s f/\partial u_1 \cdots \partial u_s
   </math>
   be continuous on
   <math display="inline">
    I^s
   </math>
   . A detailed proof of the Koksma-Hlawka inequality is by Kuipers and Niederreiter [37, Section 2.5]. There are all types of variants of this inequality; see [46, 49, Section 2.2, 24].
  </p>
  <p block-type="Text">
   A different kind of error bound for OMC integration was shown by Niederreiter [50]. It relies on the following concept.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Definition 2
   </b>
   Let
   <math display="inline">
    M
   </math>
   be a nonempty collection of Borel sets in
   <math display="inline">
    I^s
   </math>
   . Then a point set P of elements of
   <math display="inline">
    I^s
   </math>
   is called
   <math display="inline">
    (\mathcal{M}, \lambda_s)
   </math>
   -uniform if
   <math display="inline">
    R(M; P) = 0
   </math>
   for all
   <math display="inline">
    M \in \mathcal{M}
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   Now let
   <math display="inline">
    \mathcal{M} = \{M_1, \ldots, M_k\}
   </math>
   be a partition of
   <math display="inline">
    I^s
   </math>
   into nonempty Borel subsets of
   <math display="inline">
    I^s
   </math>
   . For a bounded Lebesgue-integrable function
   <math display="inline">
    f
   </math>
   on
   <math display="inline">
    I^s
   </math>
   and for
   <math display="inline">
    1 &lt;
   </math>
   <math display="inline">
    j \leq k
   </math>
   , we put
  </p>
  <p block-type="Equation">
   <math display="block">
    G_j(f) = \sup_{\mathbf{u} \in M_j} f(\mathbf{u}), \quad g_j(f) = \inf_{\mathbf{u} \in M_j} f(\mathbf{u}) \quad (8)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Then for any
   <math display="inline">
    (\mathcal{M}, \lambda_s)
   </math>
   -uniform point set consisting of
   <math display="inline">
    \mathbf{x}_1,\ldots,\mathbf{x}_N\in I^s
   </math>
   we have the error bound
  </p>
  <p block-type="Equation">
   <math display="block">
    \left| \int_{I^{s}} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} - \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_{n}) \right|
   </math>
   <br/>
   <math display="block">
    \leq \sum_{j=1}^{k} \lambda_{s}(M_{j})(G_{j}(f) - g_{j}(f)) \tag{9}
   </math>
  </p>
  <p block-type="Text">
   An analog of the bound (9) holds, in fact, for numerical integration over any probability space (see [50]).
  </p>
  <p block-type="TextInlineMath">
   A family of QMC methods that is particularly suited for periodic integrands is formed by lattice rules. For a given dimension
   <math display="inline">
    s &gt; 1
   </math>
   , consider the factor group
   <math display="inline">
    \mathbb{R}^s/\mathbb{Z}^s
   </math>
   , which is an abelian group under addition of residue classes. Let
   <math display="inline">
    L/\mathbb{Z}^s
   </math>
   be an arbitrary finite subgroup of
   <math display="inline">
    \mathbb{R}^s/\mathbb{Z}^s
   </math>
   and let
   <math display="inline">
    \mathbf{x}_n + \mathbb{Z}^s
   </math>
   with
   <math display="inline">
    \mathbf{x}_n \in
   </math>
   <math display="inline">
    [0, 1)^s
   </math>
   for
   <math display="inline">
    1 \le n \le N
   </math>
   be the distinct residue classes making up the group
   <math display="inline">
    L/\mathbb{Z}^s
   </math>
   . The points
   <math display="inline">
    \mathbf{x}_1,\ldots,\mathbf{x}_N
   </math>
   form the integration points of an
   <math display="inline">
    N
   </math>
   -point lattice rule. This terminology stems from the fact that the subset
   <math display="inline">
    L = \bigcup_{n=1}^{N} (\mathbf{x}_{n} + \mathbb{Z}^{s})
   </math>
   of
   <math display="inline">
    \mathbb{R}^{s}
   </math>
   is an
   <i>
    s
   </i>
   -dimensional lattice. The dual lattice
   <math display="inline">
    L^{\perp}
   </math>
   of L is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    L^{\perp} = \{ \mathbf{h} \in \mathbb{Z}^s : \mathbf{h} \cdot \mathbf{x} \in \mathbb{Z} \text{ for all } \mathbf{x} \in L \} \tag{10}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{h} \cdot \mathbf{x}
   </math>
   denotes the standard inner product of
   <math display="inline">
    \mathbf{h}
   </math>
   and
   <b>
    x
   </b>
   . For real numbers,
   <math display="inline">
    \alpha &gt; 1
   </math>
   and
   <math display="inline">
    C &gt; 0
   </math>
   , let
   <math display="inline">
    \mathcal{E}^s_{\alpha}(C)
   </math>
   be the class of all continuous periodic functions f on
   <math display="inline">
    \mathbb{R}^s
   </math>
   with period interval
   <math display="inline">
    I^s
   </math>
   and with Fourier coefficients
   <math display="inline">
    \hat{f}(\mathbf{h})
   </math>
   satisfying
  </p>
  <p block-type="Equation">
   <math display="block">
    |\hat{f}(\mathbf{h})| \leq Cr(\mathbf{h})^{-\alpha}
   </math>
   for all nonzero
   <math>
    \mathbf{h} \in \mathbb{Z}^s
   </math>
   (11)
  </p>
  <p block-type="TextInlineMath">
   where for
   <math display="inline">
    \mathbf{h} = (h_1, \ldots, h_s) \in \mathbb{Z}^s
   </math>
   , we put
  </p>
  <p block-type="Equation">
   <math display="block">
    r(\mathbf{h}) = \prod_{i=1}^{s} \max(1, |h_i|) \tag{12}
   </math>
  </p>
  <p block-type="Text">
   Then it can be shown that with the notation above.
  </p>
  <p block-type="Equation">
   <math display="block">
    \max_{f \in \mathcal{E}_{\alpha}^{s}(C)} \left| \int_{I^{s}} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} - \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_{n}) \right|
   </math>
   <math display="block">
    = C \sum_{\mathbf{h} \in L^{\perp} \setminus \{\mathbf{0}\}} r(\mathbf{h})^{-\alpha} \tag{13}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Further analysis leads to the result that for any
   <math display="inline">
    s \ge 2
   </math>
   and
   <math display="inline">
    N \ge 2
   </math>
   there exists an s-dimensional Npoint lattice rule with an error bound of order
   <math display="inline">
    O(N^{-\alpha}(\log N)^{c(\alpha,s)})
   </math>
   for all
   <math display="inline">
    f \in \mathcal{E}^s_\alpha(C)
   </math>
   , where the exponent
   <math display="inline">
    c(\alpha, s) &gt; 0
   </math>
   depends only on
   <math display="inline">
    \alpha
   </math>
   and s. Expository accounts of the theory of lattice rules are given in [49, Chapters 5, 71]. A more recent detailed discussion of lattice rules can be found in [25]. Algorithms for the construction of efficient lattice rules are presented, for example, in [16, 58, 72].
  </p>
  <p block-type="Text">
   This article can present only a rough overview of OMC methods. For a full treatment of OMC methods, we refer to [49]. Developments from the invention of QMC methods in the early 1950s up to 1978 are covered in detail in the survey article [46].
  </p>
  <h3>
   <b>
    Low-discrepancy Sequences
   </b>
  </h3>
  <p block-type="TextInlineMath">
   The Koksma-Hlawka inequality leads to the conclusion that point sets with small star discrepancy guarantee small errors in QMC integration over
   <math display="inline">
    I^s
   </math>
   . This raises the question of how small we can make the star discrepancy of N points in
   <math display="inline">
    I^s
   </math>
   for fixed N and s. For any
   <math display="inline">
    N \ge 2
   </math>
   and
   <math display="inline">
    s \ge 1
   </math>
   , the least order of magnitude that can be achieved at present is
  </p>
  <p block-type="Equation">
   <math display="block">
    D_N^*(P) = O(N^{-1}(\log N)^{s-1}) \tag{14}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where the implied constant is independent of
   <math display="inline">
    N
   </math>
   . (Strictly speaking, one has to consider infinitely many values of
   <math display="inline">
    N
   </math>
   , that is, an infinite collection of point sets of increasing size, for this
   <math display="inline">
    O
   </math>
   -bound to make sense in a rigorous fashion, but this technicality is often ignored.) A point set
   <math display="inline">
    P
   </math>
   achieving equation (14) is called a
   <i>
    low-discrepancy point set
   </i>
   . The points in a low-discrepancy point set are an ideal form of quasirandom points. It is conjectured that the order of magnitude in equation
   <math display="inline">
    (14)
   </math>
   is best possible, that is, the star discrepancy of any
   <math display="inline">
    N \ge 2
   </math>
   points in
   <math display="inline">
    I^s
   </math>
   is at least of the order of magnitude
   <math display="inline">
    N^{-1}(\log N)^{s-1}
   </math>
   . This conjecture is proved for
   <math display="inline">
    s = 1
   </math>
   and
   <math display="inline">
    s = 2
   </math>
   (see [37, Sections
   <math display="inline">
    2.1
   </math>
   and
   <math display="inline">
    2.21
   </math>
   ).
  </p>
  <p block-type="TextInlineMath">
   A very useful concept is that of a
   <i>
    low-discrepancy sequence
   </i>
   , which is an infinite sequence
   <math display="inline">
    S
   </math>
   of points in
   <math display="inline">
    I^s
   </math>
   such that for all
   <math display="inline">
    N &gt; 2
   </math>
   the star discrepancy
   <math display="inline">
    D^*_N(S)
   </math>
   of the first
   <math display="inline">
    N
   </math>
   terms of
   <math display="inline">
    S
   </math>
   satisfies
  </p>
  <p block-type="Equation">
   <math display="block">
    D_N^*(S) = O(N^{-1}(\log N)^s)
   </math>
   (15)
  </p>
  <p block-type="TextInlineMath">
   with an implied constant independent of
   <math display="inline">
    N
   </math>
   . It is conjectured that the order of magnitude in equation
   <math display="inline">
    (15)
   </math>
   is best possible, but in this case the conjecture has been verified only for
   <math display="inline">
    s = 1
   </math>
   (see [37, Section 2.2]).
  </p>
  <p block-type="TextInlineMath">
   Low-discrepancy sequences have several practical advantages. In the first place, if
   <math display="inline">
    \mathbf{x}_1, \mathbf{x}_2, \ldots \in I^s
   </math>
   is a low-discrepancy sequence and
   <math display="inline">
    N \ge 2
   </math>
   is an integer, then it is easily seen that the points
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{y}_n = \left(\frac{n-1}{N}, \mathbf{x}_n\right) \in I^{s+1}, \quad n = 1, \dots, N \quad (16)
   </math>
  </p>
  <p block-type="Text">
   form a low-discrepancy point set. Thus, if a lowdiscrepancy sequence has been constructed, then we immediately obtain arbitrarily large low-discrepancy point sets. Hence, in the following, we concentrate on the construction of low-discrepancy sequences. Furthermore, given a low-discrepancy sequence S and a budget of
   <math display="inline">
    N
   </math>
   integration points, we can simply use the first
   <math display="inline">
    N
   </math>
   terms of the sequence
   <math display="inline">
    S
   </math>
   to get a good QMC method. If later on we want to increase
   <math display="inline">
    N
   </math>
   to achieve a higher accuracy, we can do so while retaining the results of the earlier computation. This is an advantage of low-discrepancy sequences over low-discrepancy point sets.
  </p>
  <p block-type="Text" class="has-continuation">
   It is clear from the Koksma-Hlawka inequality and equation
   <math display="inline">
    (15)
   </math>
   that if we apply QMC integration with an integrand
   <math display="inline">
    f
   </math>
   of bounded variation on
   <math display="inline">
    I^s
   </math>
   in the sense of Hardy and Krause and with the first
   <math display="inline">
    N
   </math>
   terms
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    \mathbf{x}_1, \ldots, \mathbf{x}_N \in [0, 1)^s
   </math>
   of a low-discrepancy sequence, then
  </p>
  <p block-type="Equation">
   <math display="block">
    \int_{I^{s}} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} - \frac{1}{N} \sum_{n=1}^{N} f(\mathbf{x}_{n}) = O(N^{-1} (\log N)^{s}) \tag{17}
   </math>
  </p>
  <p block-type="TextInlineMath">
   This yields a significantly faster convergence rate than the convergence rate
   <math display="inline">
    O(N^{-1/2})
   </math>
   in equation (3). Thus, for many types of integrals, the OMC method will outperform the Monte Carlo method.
  </p>
  <p block-type="TextInlineMath">
   Over the years, various constructions of lowdiscrepancy sequences have been obtained. Historically, the first one was designed by Halton [23]. For integers
   <math display="inline">
    b \ge 2
   </math>
   and
   <math display="inline">
    n \ge 1
   </math>
   , let
  </p>
  <p block-type="Equation">
   <math display="block">
    n-1 = \sum_{j=0}^{\infty} a_j(n)b^j, \quad a_j(n) \in \{0, 1, \dots, b-1\}
   </math>
   (18)
  </p>
  <p block-type="Text">
   be the digit expansion of
   <math display="inline">
    n-1
   </math>
   in base b. Then put
  </p>
  <p block-type="Equation">
   <math display="block">
    \phi_b(n) = \sum_{j=0}^{\infty} a_j(n) b^{-j-1} \tag{19}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Now let
   <math display="inline">
    p_1 = 2, p_2 = 3, \ldots, p_s
   </math>
   be the first s prime numbers. Then
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{x}_n = (\phi_{p_1}(n), \dots, \phi_{p_s}(n)) \in I^s, \quad n = 1, 2, \dots
   </math>
   (20)
  </p>
  <p block-type="Text">
   is the
   <i>
    Halton sequence
   </i>
   in the bases
   <math display="inline">
    p_1, \ldots, p_s
   </math>
   . This sequence
   <math display="inline">
    S
   </math>
   satisfies
  </p>
  <p block-type="Equation">
   <math display="block">
    D_N^*(S) = O(N^{-1}(\log N)^s)
   </math>
   (21)
  </p>
  <p block-type="TextInlineMath">
   for all
   <math display="inline">
    N \geq 2
   </math>
   , with an implied constant depending only on
   <math display="inline">
    s
   </math>
   . The standard software implementation of Halton sequences is that of Fox [21]. More sophisticated constructions of better low-discrepancy sequences are described in the following section.
  </p>
  <h4>
   Nets and
   <math display="inline">
    (T, s)
   </math>
   -Sequences
  </h4>
  <p block-type="Text">
   Current methods of constructing low-discrepancy sequences rely on the following definition, which is a special case of Definition 2.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Definition 3
   </b>
   Let
   <math display="inline">
    s \ge 1
   </math>
   ,
   <math display="inline">
    b \ge 2
   </math>
   , and
   <math display="inline">
    0 \le t \le m
   </math>
   be integers and let
   <math display="inline">
    \mathcal{M}_{b,m,t}^{(s)}
   </math>
   be the collection of all subintervals
   <math display="inline">
    J
   </math>
   of
   <math display="inline">
    I^s
   </math>
   of the form
  </p>
  <p block-type="Equation">
   <math display="block">
    J = \prod_{i=1}^{s} \left[ a_i b^{-d_i}, \quad (a_i + 1)b^{-d_i} \right) \tag{22}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with integers
   <math display="inline">
    d_i \ge 0
   </math>
   and
   <math display="inline">
    0 \le a_i &lt; b^{d_i}
   </math>
   for
   <math display="inline">
    1 \le i \le s
   </math>
   and with
   <math display="inline">
    \lambda_s(J) = b^{t-m}
   </math>
   . Then an
   <math display="inline">
    (\mathcal{M}_{b,m,t}^{(s)}, \lambda_s)
   </math>
   -uniform point set consisting of
   <math display="inline">
    b^m
   </math>
   points in
   <math display="inline">
    I^s
   </math>
   is called a
   <math display="inline">
    (t, m, s)
   </math>
   -net in base b.
  </p>
  <p block-type="TextInlineMath">
   It is important to note that the smaller the value of
   <math display="inline">
    t
   </math>
   for given
   <math display="inline">
    b
   </math>
   ,
   <math display="inline">
    m
   </math>
   , and
   <math display="inline">
    s
   </math>
   , the larger the collection
   <math display="inline">
    \mathcal{M}_{b,m,t}^{(s)}
   </math>
   of intervals in Definition 3, and so the stronger the uniform point set property in Definition 2. Thus, the primary interest is in
   <math display="inline">
    (t, m, s)
   </math>
   -nets in base
   <math display="inline">
    b
   </math>
   with a small value of
   <math display="inline">
    t
   </math>
   .
  </p>
  <p block-type="Text">
   There is an important sequence analog of Definition 3. Given a real number
   <math display="inline">
    x \in [0, 1]
   </math>
   , let
  </p>
  <p block-type="Equation">
   <math display="block">
    x = \sum_{j=1}^{\infty} z_j b^{-j}, \quad z_j \in \{0, 1, \dots, b-1\}
   </math>
   (23)
  </p>
  <p block-type="TextInlineMath">
   be a
   <i>
    b
   </i>
   -adic expansion of
   <i>
    x
   </i>
   , where the case
   <math display="inline">
    z_i = b - 1
   </math>
   for all but finitely many
   <math display="inline">
    j
   </math>
   is allowed. For an integer
   <math display="inline">
    m \ge 1
   </math>
   , we define the truncation
  </p>
  <p block-type="Equation">
   <math display="block">
    [x]_{b,m} = \sum_{j=1}^{m} z_j b^{-j} \tag{24}
   </math>
  </p>
  <p block-type="TextInlineMath">
   If
   <math display="inline">
    \mathbf{x} = (x^{(1)}, \dots, x^{(s)}) \in I^s
   </math>
   and the
   <math display="inline">
    x^{(i)}, 1 &lt; i &lt; s
   </math>
   , are given by prescribed
   <math display="inline">
    b
   </math>
   -adic expansions, then we define
  </p>
  <p block-type="Equation">
   <math display="block">
    [\mathbf{x}]_{b,m} = \left( \left[ x^{(1)} \right]_{b,m}, \dots, \left[ x^{(s)} \right]_{b,m} \right) \tag{25}
   </math>
  </p>
  <p block-type="Text">
   We write
   <math display="inline">
    \mathbb{N}
   </math>
   for the set of positive integers and
   <math display="inline">
    \mathbb{N}_0
   </math>
   for the set of nonnegative integers.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Definition 4
   </b>
   Let
   <math display="inline">
    s \ge 1
   </math>
   and
   <math display="inline">
    b \ge 2
   </math>
   be integers and let
   <math display="inline">
    \mathbf{T}: \mathbb{N} \to \mathbb{N}_0
   </math>
   be a function with
   <math display="inline">
    \mathbf{T}(m) \leq m
   </math>
   for all
   <math display="inline">
    m \in \mathbb{N}
   </math>
   . Then a sequence
   <math display="inline">
    \mathbf{x}_1, \mathbf{x}_2, \ldots
   </math>
   of points in
   <math display="inline">
    I^s
   </math>
   is a
   <math display="inline">
    (\mathbf{T}, s)
   </math>
   -sequence in base b if for all
   <math display="inline">
    k \in \mathbb{N}_0
   </math>
   and
   <math display="inline">
    m \in \mathbb{N}
   </math>
   , the points
   <math display="inline">
    [\mathbf{x}_n]_{b,m}
   </math>
   with
   <math display="inline">
    kb^m &lt; n \leq (k+1)b^m
   </math>
   form a
   <math display="inline">
    (\mathbf{T}(m), m, s)
   </math>
   -net in base b. If for some integer
   <math display="inline">
    t &gt; 0
   </math>
   , we have
   <math display="inline">
    \mathbf{T}(m) = m
   </math>
   for
   <math display="inline">
    m &lt; t
   </math>
   and
   <math display="inline">
    \mathbf{T}(m) = t
   </math>
   for
   <math display="inline">
    m &gt; t
   </math>
   , then we speak of a
   <math display="inline">
    (t, s)
   </math>
   -sequence in base b.
  </p>
  <p block-type="Text" class="has-continuation">
   A general theory of
   <math display="inline">
    (t, m, s)
   </math>
   -nets and
   <math display="inline">
    (t, s)
   </math>
   sequences was developed by Niederreiter [47]. The
  </p>
  <p block-type="TextInlineMath">
   concept of a
   <math display="inline">
    (\mathbf{T}, s)
   </math>
   -sequence was introduced by Larcher and Niederreiter [40], with the variant in Definition 4 being due to Niederreiter and Özbudak [53]. Recent surveys of this topic are presented in [51, 52]. For a
   <math display="inline">
    (t, s)
   </math>
   -sequence in base b we have
  </p>
  <p block-type="Equation">
   <math display="block">
    D_{N}^{*}(S) = O(b^{t}N^{-1}(\log N)^{s}) \tag{26}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for all
   <math display="inline">
    N &gt; 2
   </math>
   , where the implied constant depends only on b and s. Thus, any
   <math display="inline">
    (t, s)
   </math>
   -sequence is a lowdiscrepancy sequence.
  </p>
  <p block-type="TextInlineMath">
   The standard technique of constructing
   <math display="inline">
    (T, s)
   </math>
   sequences is the
   <i>
    digital method
   </i>
   . Fix a dimension
   <math display="inline">
    s &gt; 1
   </math>
   and a base
   <math display="inline">
    b &gt; 2
   </math>
   . Let R be a finite commutative ring with identity and of order
   <math display="inline">
    b
   </math>
   . Set up a map
   <math display="inline">
    \rho: R^{\infty} \to [0, 1]
   </math>
   by selecting a bijection
   <math display="inline">
    \eta: R \to
   </math>
   <math display="inline">
    \mathbb{Z}_b := \{0, 1, \ldots, b-1\}
   </math>
   and putting
  </p>
  <p block-type="Equation">
   <math display="block">
    \rho(r_1, r_2, \ldots) = \sum_{j=1}^{\infty} \eta(r_j) b^{-j} \quad \text{for } (r_1, r_2, \ldots) \in R^{\infty}
   </math>
   (27)
  </p>
  <p block-type="TextInlineMath">
   Furthermore, choose
   <math display="inline">
    \infty \times \infty
   </math>
   matrices
   <math display="inline">
    C^{(1)}, \ldots, C^{(s)}
   </math>
   over
   <math display="inline">
    R
   </math>
   which are called
   <i>
    generating matrices
   </i>
   . For
   <math display="inline">
    n = 1, 2, \dots
   </math>
   let
  </p>
  <p block-type="Equation">
   <math display="block">
    n-1 = \sum_{j=0}^{\infty} a_j(n) b^j, \quad a_j(n) \in \mathbb{Z}_b \qquad (28)
   </math>
  </p>
  <p block-type="TextInlineMath">
   be the digit expansion of
   <math display="inline">
    n-1
   </math>
   in base b. Choose a bijection
   <math display="inline">
    \psi: \mathbb{Z}_b \to R
   </math>
   with
   <math display="inline">
    \psi(0) = 0
   </math>
   and associate with
   <math display="inline">
    n
   </math>
   the sequence
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{n} = (\psi(a_0(n)), \psi(a_1(n)), \ldots) \in R^{\infty} \tag{29}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Then the sequence
   <math display="inline">
    \mathbf{x}_1, \mathbf{x}_2, \ldots
   </math>
   of points in
   <math display="inline">
    I^s
   </math>
   is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{x}_n = (\rho(\mathbf{n}C^{(1)}), \dots, \rho(\mathbf{n}C^{(s)})) \quad \text{for } n = 1, 2, \dots
   </math>
   (30)
  </p>
  <p block-type="TextInlineMath">
   Note that the products
   <math display="inline">
    \mathbf{n}C^{(i)}
   </math>
   are well defined since
   <math display="inline">
    \mathbf{n}
   </math>
   contains only finitely many nonzero terms. In practice, the ring
   <math display="inline">
    R
   </math>
   is usually chosen to be a finite field
   <math display="inline">
    \mathbb{F}_q
   </math>
   of order q, where q is a prime power. The success of the digital method depends on a careful choice of the generating matrices
   <math display="inline">
    C^{(1)}, \ldots, C^{(s)}
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The first application of the digital method occurred in the construction of Sobol' sequences in [76]. This construction uses primitive polynomials over
   <math display="inline">
    \mathbb{F}_2
   </math>
   to set up the generating matrices
   <math display="inline">
    C^{(1)}, \ldots, C^{(s)}
   </math>
   and leads
  </p>
  <p block-type="TextInlineMath">
   to
   <math display="inline">
    (t, s)
   </math>
   -sequences in base 2. The wider family of irreducible polynomials was used in the construction of Niederreiter sequences in [48], and this construction works for arbitrary prime-power bases
   <math display="inline">
    q
   </math>
   . Let
   <math display="inline">
    p_1, \ldots, p_s
   </math>
   be the first s monic irreducible polynomials over
   <math display="inline">
    \mathbb{F}_q
   </math>
   , ordered according to nondecreasing degrees and put,
  </p>
  <p block-type="Equation">
   <math display="block">
    T_q(s) = \sum_{i=1}^{s} (\deg(p_i) - 1) \tag{31}
   </math>
  </p>
  <p block-type="TextInlineMath">
   The construction of Niederreiter sequences yields
   <math display="inline">
    (t, s)
   </math>
   -sequences in base q with
   <math display="inline">
    t = T
    <sub>
     q
    </sub>
    (s)
   </math>
   . Let
   <math display="inline">
    U(s)
   </math>
   denote the least value of
   <math display="inline">
    t
   </math>
   that can be achieved by Sobol' sequences for given s. Then
   <math display="inline">
    T_2(s) =
   </math>
   <math display="inline">
    U(s)
   </math>
   for
   <math display="inline">
    1 \le s \le 7
   </math>
   and
   <math display="inline">
    T_2(s) &lt; U(s)
   </math>
   for all
   <math display="inline">
    s \ge 8
   </math>
   . Thus, according to equation (26), for all dimensions
   <math display="inline">
    s \ge 8
   </math>
   Niederreiter sequences in base 2 lead to a smaller upper bound on the star discrepancy than Sobol' sequences. Convenient software implementations of Sobol' and Niederreiter sequences are described in
   <math display="inline">
    [8-10]
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The potentially smallest, and thus best,
   <math display="inline">
    t
   </math>
   -value for any
   <math display="inline">
    (t, s)
   </math>
   -sequence in base b would be
   <math display="inline">
    t =
   </math>
   0. However, according to [49, Corollary 4.24], a necessary condition for the existence of a
   <math display="inline">
    (0, s)
   </math>
   sequence in base b is
   <math display="inline">
    s \leq b
   </math>
   . For primes p, a construction of
   <math display="inline">
    (0, s)
   </math>
   -sequences in base p for
   <math display="inline">
    s \leq p
   </math>
   was given by Faure [18]. For prime powers
   <math display="inline">
    q
   </math>
   , a construction of
   <math display="inline">
    (0, s)
   </math>
   -sequences in base
   <math display="inline">
    q
   </math>
   for
   <math display="inline">
    s \le q
   </math>
   was given by Niederreiter [47]. Since
   <math display="inline">
    T_a(s) = 0
   </math>
   for
   <math display="inline">
    s \leq q
   </math>
   by equation (31), the Niederreiter sequences in [48] also yield
   <math display="inline">
    (0, s)
   </math>
   -sequences in base q for
   <math display="inline">
    s &lt; q
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   An important advance in the construction of lowdiscrepancy sequences was made in the mid-1990s with the design of
   <i>
    Niederreiter-Xing
   </i>
   sequences, which utilizes powerful tools from algebraic geometry and the theory of algebraic function fields and generalizes the construction of Niederreiter sequences. The basic articles here are [54, 83], and expository accounts of this work and further results are given in [55, 56, 57, Chapter 8]. Niederreiter-Xing sequences are
   <math display="inline">
    (t, s)
   </math>
   -sequences in a primepower base q with
   <math display="inline">
    t = V_q(s)
   </math>
   . Here,
   <math display="inline">
    V_q(s)
   </math>
   is a number determined by algebraic curves (or equivalently algebraic function fields) over
   <math display="inline">
    \mathbb{F}_q
   </math>
   , and we have
   <math display="inline">
    V_q(s) \leq
   </math>
   <math display="inline">
    T_q(s)
   </math>
   for all
   <math display="inline">
    s \ge 1
   </math>
   . In fact, much more is true. If we fix q and consider
   <math display="inline">
    V_q(s)
   </math>
   and
   <math display="inline">
    T_q(s)
   </math>
   as functions of s, then
   <math display="inline">
    V_q(s)
   </math>
   is of the order of magnitude s, whereas
   <math display="inline">
    T_q(s)
   </math>
   is of the order of magnitude
   <math display="inline">
    s \log s
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   This yields an enormous improvement on the bound for the star discrepancy in equation (7). It is known that for any
   <i>
    (t, s)
   </i>
   -sequences in base
   <i>
    b
   </i>
   the parameter
   <i>
    t
   </i>
   must grow at least linearly with
   <i>
    s
   </i>
   for fixed
   <i>
    b
   </i>
   (see [55, Section 10]), and so Niederreiter–Xing sequences yield
   <i>
    t
   </i>
   -values of the optimal order of magnitude as a function of
   <i>
    s
   </i>
   . A software implementation of Niederreiter–Xing sequences is described in [68] and available at http://math.iit.edu/˜mcqmc/Software.html by following the appropriate links.
  </p>
  <p block-type="TextInlineMath">
   To illustrate the comparative quality of the above constructions of
   <i>
    (t, s)
   </i>
   -sequences, we consider the case of the most convenient base 2 and tabulate some values of
   <i>
    U (s)
   </i>
   for Sobol' sequences, of
   <i>
    T
   </i>
   2
   <i>
    (s)
   </i>
   for Niederreiter sequences, and of
   <i>
    V
   </i>
   2
   <i>
    (s)
   </i>
   for Niederreiter–Xing sequences in Table 1. Note that the values of
   <i>
    V
   </i>
   2
   <i>
    (s)
   </i>
   in Table 1 for 2 ≤
   <i>
    s
   </i>
   ≤ 7 are the least values of
   <i>
    t
   </i>
   for which a
   <i>
    (t, s)
   </i>
   -sequence in base 2 can exist.
  </p>
  <p block-type="Text">
   The approach by algebraic function fields was followed up recently by Mayor and Niederreiter [45] who gave an alternative construction of Niederreiter–Xing sequences using differentials of global function fields. Niederreiter and Ozbudak [53] obtain- ¨ ed the first improvement on Niederreiter–Xing sequences for some special pairs
   <i>
    (q, s)
   </i>
   of prime-power bases
   <i>
    q
   </i>
   and dimensions
   <i>
    s
   </i>
   . For instance, consider the case where
   <i>
    q
   </i>
   is an arbitrary prime power and
   <i>
    s
   </i>
   =
   <i>
    q
   </i>
   + 1. Then
   <i>
    Tq (q
   </i>
   + 1
   <i>
    )
   </i>
   = 1 by equation (8) and this is the least possible
   <i>
    t
   </i>
   -value for a
   <i>
    (t, q
   </i>
   + 1
   <i>
    )
   </i>
   sequence in base
   <i>
    q
   </i>
   . However, the construction in [53] yields a
   <i>
    (
   </i>
   <b>
    T
   </b>
   <i>
    , q
   </i>
   + 1
   <i>
    )
   </i>
   -sequence in base
   <i>
    q
   </i>
   with
   <b>
    T
   </b>
   <i>
    (m)
   </i>
   = 0 for even
   <i>
    m
   </i>
   and
   <b>
    T
   </b>
   <i>
    (m)
   </i>
   = 1 for odd
   <i>
    m
   </i>
   , which is even better.
  </p>
  <p block-type="TextInlineMath">
   We remark that all constructions mentioned in this section are based on the digital method. We note also that the extensive database at http://mint.sbg.ac.at is devoted to
   <i>
    (t, m, s)
   </i>
   -nets and
   <i>
    (t, s)
   </i>
   -sequences. In summary, for a given prime-power base
   <i>
    q
   </i>
   , the currently best low-discrepancy sequences are as follows: (i) the Faure or Niederreiter sequences (depending on whether
   <i>
    q
   </i>
   is prime or not) for all dimensions
  </p>
  <p>
   <b>
    Table 1
   </b>
   Values of
   <i>
    U (s)
   </i>
   ,
   <i>
    T
   </i>
   2
   <i>
    (s)
   </i>
   , and
   <i>
    V
   </i>
   2
   <i>
    (s)
   </i>
  </p>
  <table>
   <tbody>
    <tr>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
     <th>
     </th>
    </tr>
    <tr>
     <th>
      s
     </th>
     <th>
      2
     </th>
     <th>
      3
     </th>
     <th>
      4
     </th>
     <th>
      5
     </th>
     <th>
      6
     </th>
     <th>
      7
     </th>
     <th>
      8
     </th>
     <th>
      9
     </th>
     <th>
      10
     </th>
     <th>
      15
     </th>
     <th>
      20
     </th>
    </tr>
    <tr>
     <th>
      U (s)
     </th>
     <td>
      0
     </td>
     <td>
      1
     </td>
     <td>
      3
     </td>
     <td>
      5
     </td>
     <td>
      8
     </td>
     <td>
      11
     </td>
     <td>
      15
     </td>
     <td>
      19
     </td>
     <td>
      23
     </td>
     <td>
      45
     </td>
     <td>
      71
     </td>
    </tr>
    <tr>
     <th>
      T2(s)
     </th>
     <td>
      0
     </td>
     <td>
      1
     </td>
     <td>
      3
     </td>
     <td>
      5
     </td>
     <td>
      8
     </td>
     <td>
      11
     </td>
     <td>
      14
     </td>
     <td>
      18
     </td>
     <td>
      22
     </td>
     <td>
      43
     </td>
     <td>
      68
     </td>
    </tr>
    <tr>
     <th>
      V2(s)
     </th>
     <td>
      0
     </td>
     <td>
      1
     </td>
     <td>
      1
     </td>
     <td>
      2
     </td>
     <td>
      3
     </td>
     <td>
      4
     </td>
     <td>
      5
     </td>
     <td>
      6
     </td>
     <td>
      8
     </td>
     <td>
      15
     </td>
     <td>
      21
     </td>
    </tr>
   </tbody>
  </table>
  <p block-type="TextInlineMath">
   <i>
    s
   </i>
   ≤
   <i>
    q
   </i>
   and (ii) the Niederreiter–Xing sequences for all dimensions
   <i>
    s&gt;q
   </i>
   , except for some special values of
   <i>
    s&gt;q
   </i>
   where the Niederreiter–Ozbudak sequences ¨ are better. We emphasize that the bound (7) on the star discrepancy of
   <i>
    (t, s)
   </i>
   -sequences is completely explicit; see [49, Section 4.1] and a recent improvement in [36]. For the best
   <i>
    (t, s)
   </i>
   -sequences, the coefficient of the leading term
   <i>
    N
   </i>
   <sup>
    −
   </sup>
   <sup>
    1
   </sup>
   <i>
    (
   </i>
   log
   <i>
    N )s
   </i>
   in the bound on the star discrepancy tends to 0 at a superexponential rate as
   <i>
    s
   </i>
   → ∞.
  </p>
  <h1>
   <b>
    Effective Dimension
   </b>
  </h1>
  <p block-type="Text">
   In view of equation (6), the QMC method for numerical integration performs asymptotically better than the Monte Carlo method for any dimension
   <i>
    s
   </i>
   . However, in practical terms, the number
   <i>
    N
   </i>
   of integration points cannot be taken too large, and then already for moderate values of
   <i>
    s
   </i>
   the size of the factor
   <i>
    (
   </i>
   log
   <i>
    N )s
   </i>
   may wipe out the advantage over the Monte Carlo method. On the other hand, numerical experiments with many types of integrands have shown that even for large dimensions
   <i>
    s
   </i>
   the QMC method will often lead to a convergence rate
   <i>
    O(N
   </i>
   <sup>
    −
   </sup>
   <sup>
    1
   </sup>
   <i>
    )
   </i>
   rather than
   <i>
    O(N
   </i>
   <sup>
    −
   </sup>
   <sup>
    1
   </sup>
   <i>
    (
   </i>
   log
   <i>
    N )s )
   </i>
   as predicted by the theory, thus beating the Monte Carlo method by a wide margin. One reason may be that the Koksma–Hlawka inequality is in general overly pessimistic. Another explanation can sometimes be given by means of the nature of the integrand
   <i>
    f
   </i>
   . Even though
   <i>
    f
   </i>
   is a function of
   <i>
    s
   </i>
   variables, the influence of these variables could differ greatly. For numerical purposes,
   <i>
    f
   </i>
   may behave like a function of much fewer variables, so that the numerical integration problem is in essence a lowdimensional one with a faster convergence rate. This idea is captured by the notion of effective dimension.
  </p>
  <p block-type="TextInlineMath">
   We start with the ANOVA decomposition of a random variable
   <i>
    f (
   </i>
   <b>
    u
   </b>
   <i>
    )
   </i>
   =
   <i>
    f (u
   </i>
   1
   <i>
    ,...,us)
   </i>
   on
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   of finite variance. This decomposition amounts to writing
   <i>
    f
   </i>
   in the form
  </p>
  <p block-type="Equation">
   <math display="block">
    f(\mathbf{u}) = \sum_{K \subseteq \{1,\ldots,s\}} f_K(\mathbf{u}) \tag{32}
   </math>
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   where
   <i>
    f
   </i>
   <sup>
    ∅
   </sup>
   is the expected value of
   <i>
    f
   </i>
   and each
   <i>
    fK(
   </i>
   <b>
    u
   </b>
   <i>
    )
   </i>
   with
   <i>
    K
   </i>
   = ∅ depends only on the variables
   <i>
    ui
   </i>
   with
   <i>
    i
   </i>
   ∈
   <i>
    K
   </i>
   and has expected value 0. Furthermore,
   <i>
    fK
   </i>
   <sup>
    1
   </sup>
   and
   <i>
    fK
   </i>
   <sup>
    2
   </sup>
   are orthogonal whenever
   <i>
    K
   </i>
   <sup>
    1
   </sup>
   =
   <i>
    K
   </i>
   2. Then the
  </p>
  <p block-type="TextInlineMath">
   variance
   <math display="inline">
    \sigma^2(f)
   </math>
   of f decomposes as
  </p>
  <p block-type="Equation">
   <math display="block">
    \sigma^{2}(f) = \sum_{K \subseteq \{1,\dots,s\}} \sigma^{2}(f_{K}) \tag{33}
   </math>
  </p>
  <p block-type="Text">
   The following definition relates to this decomposition.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Definition 5
   </b>
   Let d be an integer with
   <math display="inline">
    1 &lt; d &lt; s
   </math>
   and r a real number with
   <math display="inline">
    0 &lt; r &lt; 1
   </math>
   . Then the function
   <math display="inline">
    f
   </math>
   has effective dimension
   <math display="inline">
    d
   </math>
   at the rate
   <math display="inline">
    r
   </math>
   in the superposition sense if
  </p>
  <p block-type="Equation">
   <math display="block">
    \sum_{|K| \le d} \sigma^2(f_K) \ge r\sigma^2(f) \tag{34}
   </math>
  </p>
  <p block-type="Text">
   The function
   <math display="inline">
    f
   </math>
   has effective dimension
   <math display="inline">
    d
   </math>
   at the rate
   <math display="inline">
    r
   </math>
   in the truncation sense if
  </p>
  <p block-type="Equation">
   <math display="block">
    \sum_{K \subseteq \{1,\dots,d\}} \sigma^2(f_K) \ge r\sigma^2(f) \tag{35}
   </math>
  </p>
  <p block-type="Text">
   Values of r of practical interest are
   <math display="inline">
    r = 0.95
   </math>
   and
   <math display="inline">
    r = 0.99
   </math>
   , for instance. The formalization of the idea of effective dimension goes back to the articles of Caflisch et al. [13] and Hickernell [25]. There are many problems of high-dimensional numerical integration arising in computational finance for which the integrands have a relatively small effective dimension, one possible reason being discount factors, which render variables corresponding to distant time horizons essentially negligible. The classical example here is that of the valuation of mortgage-backed securities (see
   <math display="inline">
    [13, 66]
   </math>
   ). For further interesting work on the ANOVA decomposition and effective dimension, with applications to the pricing of Asian and barrier options, we refer to [32, 42, 43, 81].
  </p>
  <p block-type="Text">
   A natural way of capturing the relative importance of variables is to attach weights to them. More generally, one may attach weights to any collection of variables, thus measuring the relative importance of all projections—and not just of the one-dimensional projections—of the given integrand. This leads then to a weighted version of the theory of QMC methods, an approach that was pioneered by Sloan and Woźniakowski [75].
  </p>
  <p block-type="TextInlineMath">
   Given a dimension s we consider the set
   <math display="inline">
    \{1, \ldots, s\}
   </math>
   of coordinate indices. To any nonempty subset
   <math display="inline">
    K
   </math>
   of
   <math display="inline">
    \{1,\ldots,s\}
   </math>
   we attach a weight
   <math display="inline">
    \gamma_K \geq 0
   </math>
   . To avoid a trivial case, we assume that not all weights are 0. Let
   <math display="inline">
    \gamma
   </math>
   denote the collection of all these weights
   <math display="inline">
    \gamma_K
   </math>
   . Then we
  </p>
  <p block-type="TextInlineMath">
   introduce the
   <i>
    weighted star discrepancy
   </i>
   <math display="inline">
    D_{N,\nu}^*
   </math>
   , which generalizes Definition 1. For
   <math display="inline">
    \mathbf{u} = (u_1, \ldots, u_s) \in I^s
   </math>
   , we abbreviate the interval
   <math display="inline">
    \prod_{i=1}^{s} [0, u_i]
   </math>
   by [0, u). For any nonempty
   <math display="inline">
    K \subseteq \{1, \ldots, s\}
   </math>
   , let
   <math display="inline">
    \mathbf{u}_K
   </math>
   denote the point in
   <math display="inline">
    I^s
   </math>
   with all coordinates whose indices are not in
   <math display="inline">
    K
   </math>
   replaced by 1. Now for a point set
   <math display="inline">
    P
   </math>
   consisting of N points from
   <math display="inline">
    I^s
   </math>
   , we define
  </p>
  <p block-type="Equation">
   <math display="block">
    D_{N,\gamma}^* = \sup_{\mathbf{u}\in I^s} \max_K \gamma_K |R([\mathbf{0},\mathbf{u}_K);P)| \qquad (36)
   </math>
  </p>
  <p block-type="TextInlineMath">
   We recover the classical star discrepancy if we choose
   <math display="inline">
    \gamma_{\{1,\dots,s\}} = 1
   </math>
   and
   <math display="inline">
    \gamma_K = 0
   </math>
   for all nonempty proper subsets
   <math display="inline">
    K
   </math>
   of
   <math display="inline">
    \{1, \ldots, s\}
   </math>
   . With this weighted star discrepancy, one can then prove a weighted analog of the Koksma-Hlawka inequality (see [75]).
  </p>
  <p block-type="TextInlineMath">
   There are some special kinds of weights that are of great practical interest. In the case of
   <i>
    product weights
   </i>
   , one attaches a weight
   <math display="inline">
    \eta_i
   </math>
   to each
   <math display="inline">
    i \in \{1, \ldots, s\}
   </math>
   and puts
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_K = \prod_{i \in K} \eta_i \quad \text{for all } K \subseteq \{1, \dots, s\}, \ K \neq \emptyset \quad (37)
   </math>
  </p>
  <p block-type="TextInlineMath">
   In the case of
   <i>
    finite-order weights
   </i>
   , one chooses a threshold k and puts
   <math display="inline">
    \gamma_K = 0
   </math>
   for all K of cardinality larger than
   <math display="inline">
    k
   </math>
   .
  </p>
  <p block-type="Text">
   The theoretical analysis of the performance of weighted QMC methods requires the introduction of weighted function spaces in which the integrands live. These can, for instance, be weighted Sobolev spaces or weighted Korobov spaces. In this context again, the weights reflect the relative importance of variables or collections of variables. The articles [38, 70, 75] are representative for this approach.
  </p>
  <p block-type="Text">
   The analysis of the integration error utilizing weighted function spaces also leads to powerful results on tractability, a concept stemming from the theory of information-based complexity. The emphasis here is on the performance of multidimensional numerical integration schemes as a function not only of the number
   <math display="inline">
    N
   </math>
   of integration points but also of the dimension s as
   <math display="inline">
    s \to \infty
   </math>
   . Let
   <math display="inline">
    \mathcal{F}_s
   </math>
   be a Banach space of integrands f on
   <math display="inline">
    I^s
   </math>
   with norm
   <math display="inline">
    || f ||
   </math>
   . Write
  </p>
  <p block-type="Equation">
   <math display="block">
    L_s(f) = \int_{I^s} f(\mathbf{u}) \, \mathrm{d}\mathbf{u} \qquad \text{for } f \in \mathcal{F}_s \tag{38}
   </math>
  </p>
  <p block-type="Text">
   Consider numerical integration schemes of the form
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathcal{A}(f) = \sum_{n=1}^{N} a_n f(\mathbf{x}_n) \tag{39}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with real numbers
   <i>
    a
   </i>
   1
   <i>
    ,...,aN
   </i>
   and points
   <b>
    x
   </b>
   1
   <i>
    ,...,
   </i>
   <b>
    x
   </b>
   <i>
    <sup>
     N
    </sup>
   </i>
   ∈
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   . The QMC method is of course a special case of such a scheme. For A as in equation (39), we write card
   <i>
    (
   </i>
   A
   <i>
    )
   </i>
   =
   <i>
    N
   </i>
   . Furthermore, we put
  </p>
  <p block-type="Equation">
   <math display="block">
    \operatorname{err}(\mathcal{A}, \mathcal{F}_s) = \sup_{\|f\| \le 1} |L_s(f) - \mathcal{A}(f)| \qquad (40)
   </math>
  </p>
  <p block-type="TextInlineMath">
   For any
   <i>
    N
   </i>
   ≥ 1 and
   <i>
    s
   </i>
   ≥ 1, the
   <i>
    N
   </i>
   th minimal error of the
   <i>
    s
   </i>
   -dimensional numerical integration problem is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \operatorname{err}(N, \mathcal{F}_s) = \inf \{ \operatorname{err}(\mathcal{A}, \mathcal{F}_s) : \mathcal{A} \text{ with } \operatorname{card}(\mathcal{A}) = N \}
   </math>
   (41)
  </p>
  <p block-type="TextInlineMath">
   The numerical integration problem is called
   <i>
    tractable
   </i>
   if there exist constants
   <i>
    C
   </i>
   ≥ 0,
   <i>
    e
   </i>
   <sup>
    1
   </sup>
   ≥ 0, and
   <i>
    e
   </i>
   <sup>
    2
   </sup>
   <i>
    &gt;
   </i>
   0 such that
  </p>
  <p block-type="Equation">
   <math display="block">
    \operatorname{err}(N, \mathcal{F}_s) \leq C s^{e_1} N^{-e_2} \|L_s\|_{\operatorname{op}}
   </math>
   <br/>
   for all
   <math>
    N \geq 1, s \geq 1
   </math>
   (42)
  </p>
  <p block-type="TextInlineMath">
   where
   <i>
    Ls
   </i>
   op is the operator norm of
   <i>
    Ls
   </i>
   . If, in addition, the exponent
   <i>
    e
   </i>
   <sup>
    1
   </sup>
   may be chosen to be 0, then the problem is said to be
   <i>
    strongly tractable
   </i>
   .
  </p>
  <p block-type="TextInlineMath">
   Tractability and strong tractability depend very much on the choice of the spaces F
   <i>
    s
   </i>
   . Weighted function spaces using product weights have proved particularly effective in this connection. Since the interest is in
   <i>
    s
   </i>
   → ∞, product weights are set up by choosing a sequence
   <i>
    η
   </i>
   1
   <i>
    , η
   </i>
   2
   <i>
    ,...
   </i>
   of positive numbers and then, for fixed
   <i>
    s
   </i>
   ≥ 1, defining appropriate weights
   <i>
    γK
   </i>
   by equation (37). If the
   <i>
    ηi
   </i>
   tend to 0 sufficiently quickly as
   <i>
    i
   </i>
   → ∞, then in a Hilbert-space setting strong tractability can be achieved by QMC methods based on Halton, Sobol', or Niederreiter sequences (see [29, 79]). Further results on (strong) tractability as it relates to QMC methods can be found, for example, in [27, 28, 73, 74, 80, 82].
  </p>
  <h2>
   <b>
    Randomized QMC
   </b>
  </h2>
  <p block-type="Text" class="has-continuation">
   Conventional QMC methods are fully deterministic and thus do not allow statistical error estimation as in Monte Carlo methods. However, one may introduce an element of randomness into a QMC method by randomizing (or "scrambling") the deterministic integration points used in the method. In this way,
  </p>
  <p block-type="Text">
   one can combine the advantages of QMC methods, namely faster convergence rates, and those of Monte Carlo methods, namely the possibility of error estimation.
  </p>
  <p block-type="TextInlineMath">
   Historically, the first scrambling scheme is
   <i>
    Cranley–Patterson rotation
   </i>
   , which was introduced in [14]. This scheme can be applied to any point set in
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   . Let
   <b>
    x
   </b>
   1
   <i>
    ,...,
   </i>
   <b>
    x
   </b>
   <i>
    <sup>
     N
    </sup>
   </i>
   ∈
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   be given and put
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{y}_n = \{\mathbf{x}_n + \mathbf{r}\} \quad \text{for } n = 1, \dots, N \tag{43}
   </math>
  </p>
  <p block-type="Text">
   where
   <b>
    r
   </b>
   is a random vector uniformly distributed over
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   and {·} denotes reduction modulo 1 in each coordinate of a point in
   <i>
    <sup>
     s
    </sup>
   </i>
   . This scheme transforms low-discrepancy point sets into low-discrepancy point sets.
  </p>
  <p block-type="TextInlineMath">
   A sophisticated randomization of
   <i>
    (t, m, s)
   </i>
   -nets and
   <i>
    (t, s)
   </i>
   -sequences is provided by
   <i>
    Owen scrambling
   </i>
   (see [60]). This scrambling scheme works with mutually independent random permutations of the digits in the
   <i>
    b
   </i>
   -adic expansions of the coordinates of all points in a
   <i>
    (t, m, s)
   </i>
   -net in base
   <i>
    b
   </i>
   or a
   <i>
    (t, s)
   </i>
   -sequence in base
   <i>
    b
   </i>
   . The scheme is set up in such a way that the scrambled version of a
   <i>
    (t, m, s)
   </i>
   -net, respectively
   <i>
    (t, s)
   </i>
   -sequence, in base
   <i>
    b
   </i>
   is a
   <i>
    (t, m, s)
   </i>
   -net, respectively
   <i>
    (t, s)
   </i>
   -sequence, in base
   <i>
    b
   </i>
   with probability 1. Further investigations of this scheme, particularly regarding the resulting mean square discrepancy and variance, were carried out, for example, by Hickernell and Hong [26], Hickernell and Yue [30], and Owen [61–63].
  </p>
  <p block-type="TextInlineMath">
   Since Owen scrambling is quite time consuming, various faster special versions have been proposed, such as a method of Matousek [44] and the method ˇ of
   <i>
    digital shifts
   </i>
   in which the permutations in Owen scrambling are additive shifts modulo
   <i>
    b
   </i>
   and the shift parameters may depend on the coordinate index
   <i>
    i
   </i>
   ∈ {1
   <i>
    ,...,s
   </i>
   } and on the position of the digit in the digit expansion of the coordinate. In the binary case
   <i>
    b
   </i>
   = 2, digital shifting amounts to choosing
   <i>
    s
   </i>
   infinite bit strings
   <b>
    B
   </b>
   1
   <i>
    ,...,
   </i>
   <b>
    B
   </b>
   <i>
    <sup>
     s
    </sup>
   </i>
   and then taking each point
   <b>
    x
   </b>
   <i>
    <sup>
     n
    </sup>
   </i>
   of the given
   <i>
    (t, m, s)
   </i>
   -net or
   <i>
    (t, s)
   </i>
   -sequence in base 2 and bitwise XORing the binary expansion of the
   <i>
    i
   </i>
   th coordinate of
   <b>
    x
   </b>
   <i>
    <sup>
     n
    </sup>
   </i>
   with
   <b>
    B
   </b>
   <i>
    <sup>
     i
    </sup>
   </i>
   for 1 ≤
   <i>
    i
   </i>
   ≤
   <i>
    s
   </i>
   . Digital shifts and their applications are discussed, for example, in [17, 41]. The latter article presents also a general survey of randomized QMC methods and stresses the interpretation of these methods as variance reduction techniques.
  </p>
  <p block-type="Text">
   Convenient scrambling schemes are also obtained by operating on the generating matrices of
   <i>
    (t, m, s)
   </i>
   nets and
   <i>
    (t, s)
   </i>
   -sequences constructed by the digital method. The idea is to multiply the generating matrices by suitable random matrices from the left or from the right in such a way that the value of the parameter
   <i>
    t
   </i>
   is preserved. We refer to [19, 64] for such scrambling schemes. Software implementations of randomized low-discrepancy sequences are described in [22, 31] and are integrated into the Java library SSJ available at http://www.iro.umontreal.ca/∼simardr/ssj, which contains also many other simulation tools.
  </p>
  <h2>
   <b>
    Applications to Computational Finance
   </b>
  </h2>
  <p block-type="Text">
   The application of Monte Carlo methods to challenging problems in computational finance was pioneered by Boyle [3] in 1977. Although QMC methods were already known at that time, they were not applied to computational finance because it was thought that they would be inefficient for problems involving the high dimensions occurring in this area.
  </p>
  <p block-type="Text">
   A breakthrough came in the early 1990s when Paskov and Traub applied QMC integration to the problem of pricing a 30-year collateralized mortgage obligation provided by Goldman Sachs; see [67] for a report on this work. This problem required the computation of 10 integrals of dimension 360 each, and the results were astounding. For the hardest of the 10 integrals, the QMC method achieved accuracy 10
   <sup>
    −
   </sup>
   <sup>
    2
   </sup>
   with just 170 points, whereas the Monte Carlo method needed 2700 points for the same accuracy. When higher accuracy is desired, the QMC method can be about 1000 times faster than the Monte Carlo method. For further work on the pricing of mortgagebacked securities, we refer to [13, 66, 78].
  </p>
  <p block-type="Text">
   Applications of QMC methods to option pricing were first considered in the technical report of Birge [2] and the article of Joy
   <i>
    et al.
   </i>
   [35]. These works concentrated on European and Asian options. In the case of path-dependent options, if the security's terminal value depends only on the prices at
   <i>
    s
   </i>
   intermediate times, then after discretization the expected discounted payoff under the risk-neutral measure can be converted into an integral over the
   <i>
    s
   </i>
   -dimensional unit cube
   <i>
    I
    <sup>
     s
    </sup>
   </i>
   . For instance, in [35] an Asian option with 53 time steps is studied numerically.
  </p>
  <p block-type="Text" class="has-continuation">
   A related problem in which an
   <i>
    s
   </i>
   -dimensional integral arises is the pricing of a multiasset option
  </p>
  <p block-type="Text">
   with
   <i>
    s
   </i>
   assets; see [1] in which numerical experiments comparing Monte Carlo and QMC methods are reported for dimensions up to
   <i>
    s
   </i>
   = 100. This article also discusses Brownian bridge constructions for option pricing. Related work on the pricing of multiasset European-style options using QMC and randomized QMC methods was carried out in [39, 69, 77], and comparative numerical experiments for Asian options can be found in [4, 59]. Jiang [33] gave a detailed error analysis of the pricing of Europeanstyle options using QMC methods, which is based on a variant of the bound (3) and requires only minimal smoothness assumptions.
  </p>
  <p block-type="Text">
   Owing to its inherent difficulty, it took much longer for Monte Carlo and QMC methods to be applied to the problem of pricing American options. An excellent survey of early work on Monte Carlo methods for pricing American options is presented in [4]. The first important idea in this context was the
   <i>
    bundling algorithm
   </i>
   in which paths in state space for which the stock prices behave in a similar way are grouped together in the simulation. Initially, the bundling algorithm was applicable only to single-asset American options. Jin
   <i>
    et al.
   </i>
   [34] recently extended the bundling algorithm in order to price high-dimensional American-style options, and they also showed that computing representative states by a QMC method improves the performance of the algorithm.
  </p>
  <p block-type="Text">
   Another approach to pricing American options by simulation is the
   <i>
    stochastic mesh method
   </i>
   . The choice of mesh density functions at each discrete time step is crucial for the success of this method. The standard mesh density functions are mixture densities, and so in a Monte Carlo approach one can use known techniques for generating random samples from mixture densities. In a QMC approach, these random samples are replaced by deterministic points whose empirical distribution function is close to the target distribution function. Work on the latter approach was carried out by Boyle, Kolkiewicz, and Tan [5–7] and Broadie
   <i>
    et al.
   </i>
   [11]. Another application of QMC methods to the pricing of American options occurs in
   <i>
    regression-based methods
   </i>
   , which are typically leastsquares Monte Carlo methods. Here Caflisch and Chaudhary [12] have shown that QMC versions improve the performance of such methods.
  </p>
  <p block-type="Text" class="has-continuation">
   We conclude by mentioning two more applications of QMC methods to computational finance, namely by Papageorgiou and Paskov [65] to value-at-risk
  </p>
  <p block-type="Text">
   computations and by Jiang [33] to the pricing of interest-rate derivatives in a LIBOR market model.
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Acworth, P., Broadie, M. &amp; Glasserman, P. (1998). A comparison of some Monte Carlo and quasi Monte Carlo techniques for option pricing, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 1996
     </i>
     , H. Niederreiter, P. Hellekalek, G. Larcher &amp; P. Zinterhof, eds, Springer, New York, pp. 1–18.
    </li>
    <li block-type="ListItem">
     [2] Birge, J.R. (1994). Quasi-Monte Carlo approaches to option pricing, Technical report 94–19, Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, MI.
    </li>
    <li block-type="ListItem">
     [3] Boyle, P.P. (1977). Options: a Monte Carlo approach,
     <i>
      Journal of Financial Economics
     </i>
     <b>
      4
     </b>
     , 323–338.
    </li>
    <li block-type="ListItem">
     [4] Boyle, P., Broadie, M. &amp; Glasserman, P. (1997). Monte Carlo methods for security pricing,
     <i>
      Journal of Economic Dynamics and Control
     </i>
     <b>
      21
     </b>
     , 1267–1321.
    </li>
    <li block-type="ListItem">
     [5] Boyle, P.P., Kolkiewicz, A.W. &amp; Tan, K.S. (2001). Valuation of the reset options embedded in some equitylinked insurance products,
     <i>
      North American Actuarial Journal
     </i>
     <b>
      5
     </b>
     (3), 1–18.
    </li>
    <li block-type="ListItem">
     [6] Boyle, P.P., Kolkiewicz, A.W. &amp; Tan, K.S. (2002). Pricing American derivatives using simulation: a biased low approach, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 181–200.
    </li>
    <li block-type="ListItem">
     [7] Boyle, P.P., Kolkiewicz, A.W. &amp; Tan, K.S. (2003). An improved simulation method for pricing highdimensional American derivatives,
     <i>
      Mathematics and Computers in Simulation
     </i>
     <b>
      62
     </b>
     , 315–322.
    </li>
    <li block-type="ListItem">
     [8] Bratley, P. &amp; Fox, B.L. (1988). Algorithm 659: implementing Sobol's quasirandom sequence generator,
     <i>
      ACM Transactions on Mathematical Software
     </i>
     <b>
      14
     </b>
     , 88–100.
    </li>
    <li block-type="ListItem">
     [9] Bratley, P., Fox, B.L. &amp; Niederreiter, H. (1992). Implementation and tests of low-discrepancy sequences,
     <i>
      ACM Transactions on Modeling and Computer Simulation
     </i>
     <b>
      2
     </b>
     , 195–213.
    </li>
    <li block-type="ListItem">
     [10] Bratley, P., Fox, B.L. &amp; Niederreiter, H. (1994). Algorithm 738: programs to generate Niederreiter's lowdiscrepancy sequences,
     <i>
      ACM Transactions on Mathematical Software
     </i>
     <b>
      20
     </b>
     , 494–495.
    </li>
    <li block-type="ListItem">
     [11] Broadie, M., Glasserman, P. &amp; Ha, Z. (2000). Pricing American options by simulation using a stochastic mesh with optimized weights, in
     <i>
      Probabilistic Constrained Optimization: Methodology and Applications
     </i>
     , S.P. Uryasev, ed, Kluwer Academic Publishers, Dordrecht, pp. 26–44.
    </li>
    <li block-type="ListItem">
     [12] Caflisch, R.E. &amp; Chaudhary, S. (2004). Monte Carlo simulation for American options, in
     <i>
      A Celebration of Mathematical Modeling
     </i>
     , D. Givoli, M.J. Grote &amp; G.C. Papanicolaou, eds, Kluwer Academic Publishers, Dordrecht, pp. 1–16.
    </li>
    <li block-type="ListItem">
     [13] Caflisch, R.E., Morokoff, M. &amp; Owen, A. (1997). Valuation of mortgage-backed securities using Brownian
    </li>
   </ul>
  </p>
  <p block-type="Text">
   bridges to reduce effective dimension,
   <i>
    The Journal of Computational Finance
   </i>
   <b>
    1
   </b>
   , 27–46.
  </p>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [14] Cranley, R. &amp; Patterson, T.N.L. (1976). Randomization of number theoretic methods for multiple integration,
     <i>
      SIAM Journal on Numerical Analysis
     </i>
     <b>
      13
     </b>
     , 904–914.
    </li>
    <li block-type="ListItem">
     [15] Davis, P.J. &amp; Rabinowitz, P. (1984).
     <i>
      Methods of Numerical Integration
     </i>
     , 2nd Edition, Academic Press, New York.
    </li>
    <li block-type="ListItem">
     [16] Dick, J. &amp; Kuo, F.Y. (2004). Constructing good lattice rules with millions of points, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2002
     </i>
     , H. Niederreiter, ed, Springer, Berlin, pp. 181–197.
    </li>
    <li block-type="ListItem">
     [17] Dick, J. &amp; Pillichshammer, F. (2005). Multivariate integration in weighted Hilbert spaces based on Walsh functions and weighted Sobolev spaces,
     <i>
      Journal of Complexity
     </i>
     <b>
      21
     </b>
     , 149–195.
    </li>
    <li block-type="ListItem">
     [18] Faure, H. (1982). Discrepance de suites associ ´ ees ´ a` un systeme de num ` eration (en dimension ´
     <i>
      s
     </i>
     ),
     <i>
      Acta Arithmetica
     </i>
     <b>
      41
     </b>
     , 337–351.
    </li>
    <li block-type="ListItem">
     [19] Faure, H. &amp; Tezuka, S. (2002). Another random scrambling of digital
     <i>
      (t, s)
     </i>
     -sequences, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 242–256.
    </li>
    <li block-type="ListItem">
     [20] Fishman, G.S. (1996).
     <i>
      Monte Carlo: Concepts, Algorithms, and Applications
     </i>
     , Springer, New York.
    </li>
    <li block-type="ListItem">
     [21] Fox, B.L. (1986). Algorithm 647: implementation and relative efficiency of quasirandom sequence generators,
     <i>
      ACM Transactions on Mathematical Software
     </i>
     <b>
      12
     </b>
     , 362–376.
    </li>
    <li block-type="ListItem">
     [22] Friedel, I. &amp; Keller, A. (2002). Fast generation of randomized low-discrepancy point sets, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 257–273.
    </li>
    <li block-type="ListItem">
     [23] Halton, J.H. (1960). On the efficiency of certain quasi-random sequences of points in evaluating multidimensional integrals,
     <i>
      Numerische Mathematik
     </i>
     <b>
      2
     </b>
     , 84–90, 196.
    </li>
    <li block-type="ListItem">
     [24] Hickernell, F.J. (1998). A generalized discrepancy and quadrature error bound,
     <i>
      Mathematics of Computation
     </i>
     <b>
      67
     </b>
     , 299–322.
    </li>
    <li block-type="ListItem">
     [25] Hickernell, F.J. (1998). Lattice rules: how well do they measure up? in
     <i>
      Random and Quasi-Random Point Sets
     </i>
     , P. Hellekalek &amp; G. Larcher, eds, Springer, New York, pp. 109–166.
    </li>
    <li block-type="ListItem">
     [26] Hickernell, F.J. &amp; Hong, H.S. (1999). The asymptotic efficiency of randomized nets for quadrature,
     <i>
      Mathematics of Computation
     </i>
     <b>
      68
     </b>
     , 767–791.
    </li>
    <li block-type="ListItem">
     [27] Hickernell, F.J., Sloan, I.H. &amp; Wasilkowski, G.W. (2004). On tractability of weighted integration for certain Banach spaces of functions, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2002
     </i>
     , H. Niederreiter, ed, Springer, Berlin, pp. 51–71.
    </li>
    <li block-type="ListItem">
     [28] Hickernell, F.J., Sloan, I.H. &amp; Wasilkowski, G.W. (2004). The strong tractability of multivariate integration using lattice rules, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2002
     </i>
     , H. Niederreiter, ed, Springer, Berlin, pp. 259–273.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [29] Hickernell, F.J. &amp; Wang, X.Q. (2002). The error bounds and tractability of quasi-Monte Carlo algorithms in infinite dimension,
     <i>
      Mathematics of Computation
     </i>
     <b>
      71
     </b>
     , 1641–1661.
    </li>
    <li block-type="ListItem">
     [30] Hickernell, F.J. &amp; Yue, R.-X. (2001). The mean square discrepancy of scrambled
     <i>
      (t, s)
     </i>
     -sequences,
     <i>
      SIAM Journal on Numerical Analysis
     </i>
     <b>
      38
     </b>
     , 1089–1112.
    </li>
    <li block-type="ListItem">
     [31] Hong, H.S. &amp; Hickernell, F.J. (2003). Algorithm 823: implementing scrambled digital sequences,
     <i>
      ACM Transactions on Mathematical Software
     </i>
     <b>
      29
     </b>
     , 95–109.
    </li>
    <li block-type="ListItem">
     [32] Imai, J. &amp; Tan, K.S. (2004). Minimizing effective dimension using linear transformation, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2002
     </i>
     , H. Niederreiter, ed, Springer, Berlin, pp. 275–292.
    </li>
    <li block-type="ListItem">
     [33] Jiang, X.F. (2007).
     <i>
      Quasi-Monte Carlo methods in finance
     </i>
     . Ph.D. dissertation, Northwestern University, Evanston, IL.
    </li>
    <li block-type="ListItem">
     [34] Jin, X., Tan, H.H. &amp; Sun, J.H. (2007). A state-space partitioning method for pricing high-dimensional Americanstyle options,
     <i>
      Mathematical Finance
     </i>
     <b>
      17
     </b>
     , 399–426.
    </li>
    <li block-type="ListItem">
     [35] Joy, C., Boyle, P.P. &amp; Tan, K.S. (1996). Quasi-Monte Carlo methods in numerical finance,
     <i>
      Management Science
     </i>
     <b>
      42
     </b>
     , 926–938.
    </li>
    <li block-type="ListItem">
     [36] Kritzer, P. (2006). Improved upper bounds on the star discrepancy of
     <i>
      (t, m, s)
     </i>
     -nets and
     <i>
      (t, s)
     </i>
     -sequences,
     <i>
      Journal of Complexity
     </i>
     <b>
      22
     </b>
     , 336–347.
    </li>
    <li block-type="ListItem">
     [37] Kuipers, L. &amp; Niederreiter, H. (1974).
     <i>
      Uniform Distribution of Sequences
     </i>
     , Wiley, New York. Reprint by Dover Publications, Mineola, NY, 2006.
    </li>
    <li block-type="ListItem">
     [38] Kuo, F.Y. (2003). Component-by-component constructions achieve the optimal rate of convergence for multivariate integration in weighted Korobov and Sobolev spaces,
     <i>
      Journal of Complexity
     </i>
     <b>
      19
     </b>
     , 301–320.
    </li>
    <li block-type="ListItem">
     [39] Lai, Y.Z. &amp; Spanier, J. (2000). Applications of Monte Carlo/quasi-Monte Carlo methods in finance: option pricing, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 1998
     </i>
     , H. Niederreiter &amp; J. Spanier, eds, Springer, Berlin, pp. 284–295.
    </li>
    <li block-type="ListItem">
     [40] Larcher, G. &amp; Niederreiter, H. (1995). Generalized
     <i>
      (t, s)
     </i>
     -sequences, Kronecker-type sequences, and diophantine approximations of formal Laurent series,
     <i>
      Transactions of the American Mathematical Society
     </i>
     <b>
      347
     </b>
     , 2051–2073.
    </li>
    <li block-type="ListItem">
     [41] L'Ecuyer, P. &amp; Lemieux, C. (2002). Recent advances in randomized quasi-Monte Carlo methods, in
     <i>
      Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and Applications
     </i>
     , M. Dror, P. L'Ecuyer &amp; F. Szidarovszky, eds, Kluwer Academic Publishers, Boston, pp. 419–474.
    </li>
    <li block-type="ListItem">
     [42] Lemieux, C. &amp; Owen, A.B. (2002). Quasi-regression and the relative importance of the ANOVA components of a function, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 331–344.
    </li>
    <li block-type="ListItem">
     [43] Liu, R.X. &amp; Owen, A.B. (2006). Estimating mean dimensionality of analysis of variance decompositions,
     <i>
      Journal of the American Statistical Association
     </i>
     <b>
      101
     </b>
     , 712–721.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [44] Matousek, J. (1998). On the ˇ
     <i>
      L
     </i>
     2-discrepancy for anchored boxes,
     <i>
      Journal of Complexity
     </i>
     <b>
      14
     </b>
     , 527–556.
    </li>
    <li block-type="ListItem">
     [45] Mayor, D.J.S. &amp; Niederreiter, H. (2007). A new construction of
     <i>
      (t, s)
     </i>
     -sequences and some improved bounds on their quality parameter,
     <i>
      Acta Arithmetica
     </i>
     <b>
      128
     </b>
     , 177–191.
    </li>
    <li block-type="ListItem">
     [46] Niederreiter, H. (1978). Quasi-Monte Carlo methods and pseudo-random numbers,
     <i>
      Bulletin of the American Mathematical Society
     </i>
     <b>
      84
     </b>
     , 957–1041.
    </li>
    <li block-type="ListItem">
     [47] Niederreiter, H. (1987). Point sets and sequences with small discrepancy,
     <i>
      Monatshefte f¨ur Mathematik
     </i>
     <b>
      104
     </b>
     , 273–337.
    </li>
    <li block-type="ListItem">
     [48] Niederreiter, H. (1988). Low-discrepancy and lowdispersion sequences,
     <i>
      Journal of Number Theory
     </i>
     <b>
      30
     </b>
     , 51–70.
    </li>
    <li block-type="ListItem">
     [49] Niederreiter, H. (1992).
     <i>
      Random Number Generation and Quasi-Monte Carlo Methods
     </i>
     , SIAM, Philadelphia.
    </li>
    <li block-type="ListItem">
     [50] Niederreiter, H. (2003). Error bounds for quasi-Monte Carlo integration with uniform point sets,
     <i>
      Journal of Computational and Applied Mathematics
     </i>
     <b>
      150
     </b>
     , 283–292.
    </li>
    <li block-type="ListItem">
     [51] Niederreiter, H. (2005). Constructions of
     <i>
      (t, m, s)
     </i>
     -nets and
     <i>
      (t, s)
     </i>
     -sequences,
     <i>
      Finite Fields and Their Applications
     </i>
     <b>
      11
     </b>
     , 578–600.
    </li>
    <li block-type="ListItem">
     [52] Niederreiter, H. (2008). Nets,
     <i>
      (t, s)
     </i>
     -sequences, and codes, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2006
     </i>
     , A. Keller, S. Heinrich &amp; H. Niederreiter, eds, Springer, Berlin, pp. 83–100.
    </li>
    <li block-type="ListItem">
     [53] Niederreiter, H. &amp; Ozbudak, F. (2007). Low-discrepancy ¨ sequences using duality and global function fields,
     <i>
      Acta Arithmetica
     </i>
     <b>
      130
     </b>
     , 79–97.
    </li>
    <li block-type="ListItem">
     [54] Niederreiter, H. &amp; Xing, C.P. (1996). Low-discrepancy sequences and global function fields with many rational places,
     <i>
      Finite Fields and Their Applications
     </i>
     <b>
      2
     </b>
     , 241–273.
    </li>
    <li block-type="ListItem">
     [55] Niederreiter, H. &amp; Xing, C.P. (1996). Quasirandom points and global function fields, in
     <i>
      Finite Fields and Applications
     </i>
     , S. Cohen &amp; H. Niederreiter, eds, Cambridge University Press, Cambridge, pp. 269–296.
    </li>
    <li block-type="ListItem">
     [56] Niederreiter, H. &amp; Xing, C.P. (1998). Nets,
     <i>
      (t, s)
     </i>
     sequences, and algebraic geometry, in
     <i>
      Random and Quasi-Random Point Sets
     </i>
     , P. Hellekalek &amp; G. Larcher, eds, Springer, New York, pp. 267–302.
    </li>
    <li block-type="ListItem">
     [57] Niederreiter, H. &amp; Xing, C.P. (2001).
     <i>
      Rational Points on Curves over Finite Fields: Theory and Applications
     </i>
     , Cambridge University Press, Cambridge.
    </li>
    <li block-type="ListItem">
     [58] Nuyens, D. &amp; Cools, R. (2006). Fast algorithms for component-by-component construction of rank-1 lattice rules in shift-invariant reproducing kernel Hilbert spaces,
     <i>
      Mathematics of Computation
     </i>
     <b>
      75
     </b>
     , 903–920.
    </li>
    <li block-type="ListItem">
     [59] Okten, G. &amp; Eastman, W. (2004). Randomized quasi- ¨ Monte Carlo methods in pricing securities,
     <i>
      Journal of Economic Dynamics and Control
     </i>
     <b>
      28
     </b>
     , 2399–2426.
    </li>
    <li block-type="ListItem">
     [60] Owen, A.B. (1995). Randomly permuted
     <i>
      (t, m, s)
     </i>
     -nets and
     <i>
      (t, s)
     </i>
     -sequences, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing
     </i>
     , H. Niederreiter &amp; P.J.-S. Shiue, eds, Springer, New York, pp. 299–317.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [61] Owen, A.B. (1997). Monte Carlo variance of scrambled net quadrature,
     <i>
      SIAM Journal on Numerical Analysis
     </i>
     <b>
      34
     </b>
     , 1884–1910.
    </li>
    <li block-type="ListItem">
     [62] Owen, A.B. (1997). Scrambled net variance for integrals of smooth functions,
     <i>
      The Annals of Statistics
     </i>
     <b>
      25
     </b>
     , 1541–1562.
    </li>
    <li block-type="ListItem">
     [63] Owen, A.B. (1998). Scrambling Sobol' and Niederreiter-Xing points,
     <i>
      Journal of Complexity
     </i>
     <b>
      14
     </b>
     , 466–489.
    </li>
    <li block-type="ListItem">
     [64] Owen, A.B. (2003). Variance with alternative scramblings of digital nets,
     <i>
      ACM Transactions on Modeling and Computer Simulation
     </i>
     <b>
      13
     </b>
     , 363–378.
    </li>
    <li block-type="ListItem">
     [65] Papageorgiou, A. &amp; Paskov, S. (1999). Deterministic simulation for risk management,
     <i>
      Journal of Portfolio Management
     </i>
     <b>
      25
     </b>
     (5), 122–127.
    </li>
    <li block-type="ListItem">
     [66] Paskov, S.H. (1997). New methodologies for valuing derivatives, in
     <i>
      Mathematics of Derivative Securities
     </i>
     , M.A.H. Dempster &amp; S.R. Pliska, eds, Cambridge University Press, Cambridge, pp. 545–582.
    </li>
    <li block-type="ListItem">
     [67] Paskov, S.H. &amp; Traub, J.F. (1995). Faster valuation of financial derivatives,
     <i>
      Journal of Portfolio Management
     </i>
     <b>
      22
     </b>
     (1), 113–120.
    </li>
    <li block-type="ListItem">
     [68] Pirsic, G. (2002). A software implementation of Niederreiter-Xing sequences, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 434–445.
    </li>
    <li block-type="ListItem">
     [69] Ross, R. (1998). Good point methods for computing prices and sensitivities of multi-asset European style options,
     <i>
      Applied Mathematical Finance
     </i>
     <b>
      5
     </b>
     , 83–106.
    </li>
    <li block-type="ListItem">
     [70] Sloan, I.H. (2002). QMC integration—beating intractability by weighting the coordinate directions, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 2000
     </i>
     , K.T. Fang, F.J. Hickernell &amp; H. Niederreiter, eds, Springer, Berlin, pp. 103–123.
    </li>
    <li block-type="ListItem">
     [71] Sloan, I.H. &amp; Joe, S. (1994).
     <i>
      Lattice Methods for Multiple Integration
     </i>
     , Oxford University Press, Oxford.
    </li>
    <li block-type="ListItem">
     [72] Sloan, I.H., Kuo, F.Y. &amp; Joe, S. (2002). Constructing randomly shifted lattice rules in weighted Sobolev spaces,
     <i>
      SIAM Journal on Numerical Analysis
     </i>
     <b>
      40
     </b>
     , 1650–1665.
    </li>
    <li block-type="ListItem">
     [73] Sloan, I.H., Kuo, F.Y. &amp; Joe, S. (2002). On the stepby-step construction of quasi-Monte Carlo integration rules that achieve strong tractability error bounds in
    </li>
   </ul>
  </p>
  <p block-type="Text">
   weighted Sobolev spaces,
   <i>
    Mathematics of Computation
   </i>
   <b>
    71
   </b>
   , 1609–1640.
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [74] Sloan, I.H., Wang, X.Q. &amp; Wozniakowski, H. (2004). ´ Finite-order weights imply tractability of multivariate integration,
     <i>
      Journal of Complexity
     </i>
     <b>
      20
     </b>
     , 46–74.
    </li>
    <li block-type="ListItem">
     [75] Sloan, I.H. &amp; Wozniakowski, H. (1998). When are quasi- ´ Monte Carlo algorithms efficient for high dimensional integrals?
     <i>
      Journal of Complexity
     </i>
     <b>
      14
     </b>
     , 1–33.
    </li>
    <li block-type="ListItem">
     [76] Sobol', I.M. (1967). Distribution of points in a cube and approximate evaluation of integrals,
     <i>
      USSR Computational Mathematics and Mathematical Physics
     </i>
     <b>
      7
     </b>
     (4), 86–112.
    </li>
    <li block-type="ListItem">
     [77] Tan, K.S. &amp; Boyle, P.P. (2000). Applications of randomized low discrepancy sequences to the valuation of complex securities,
     <i>
      Journal of Economic Dynamics and Control
     </i>
     <b>
      24
     </b>
     , 1747–1782.
    </li>
    <li block-type="ListItem">
     [78] Tezuka, S. (1998). Financial applications of Monte Carlo and quasi-Monte Carlo methods, in
     <i>
      Random and Quasi-Random Point Sets
     </i>
     , P. Hellekalek &amp; G. Larcher, eds, Springer, New York, pp. 303–332.
    </li>
    <li block-type="ListItem">
     [79] Wang, X.Q. (2002). A constructive approach to strong tractability using quasi-Monte Carlo algorithms,
     <i>
      Journal of Complexity
     </i>
     <b>
      18
     </b>
     , 683–701.
    </li>
    <li block-type="ListItem">
     [80] Wang, X.Q. (2003). Strong tractability of multivariate integration using quasi-Monte Carlo algorithms,
     <i>
      Mathematics of Computation
     </i>
     <b>
      72
     </b>
     , 823–838.
    </li>
    <li block-type="ListItem">
     [81] Wang, X.Q. &amp; Sloan, I.H. (2005). Why are highdimensional finance problems often of low effective dimension?
     <i>
      SIAM Journal on Scientific Computing
     </i>
     <b>
      27
     </b>
     , 159–183.
    </li>
    <li block-type="ListItem">
     [82] Wozniakowski, H. (2000). Efficiency of quasi-Monte ´ Carlo algorithms for high dimensional integrals, in
     <i>
      Monte Carlo and Quasi-Monte Carlo Methods 1998
     </i>
     , H. Niederreiter &amp; J. Spanier, eds, Springer, Berlin, pp. 114–136.
    </li>
    <li block-type="ListItem">
     [83] Xing, C.P. &amp; Niederreiter, H. (1995). A construction of low-discrepancy sequences using global function fields,
     <i>
      Acta Arithmetica
     </i>
     <b>
      73
     </b>
     , 87–102.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   HARALD NIEDERREITER
  </p>
 </body>
</html>
