<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Heavy Tails
   </b>
  </h1>
  <p block-type="Text">
   The three most cited stylized properties attributed to log-returns of financial assets or stocks are (i) a kurtosis much larger than 3, the kurtosis of a normal distribution; (ii) serial dependence without correlation; and (iii) volatility clustering. Any realistic and useful model for log-returns must account for all three of these characteristics. In this article, the focus is on the large kurtosis property, which is indicative of heavy tails in the returns. Although this stylized fact may not draw the same level of attention as the other two, it can have a serious impact on modeling and inference questions related to financial time series. One such application is the estimation of the Value at Risk, which is an important entity in the finance industry. For example, financial institutions would like to estimate large quantiles of the absolute returns, that is, the level at which the probability that an absolute return exceeds this value is small such as 0.01 or less. The estimation of these large quantities is extremely sensitive to the shape assumed for the tail of the marginal distribution. A light-tailed assumption for the tails can severely underestimate the actual quantiles of the marginal distribution. In addition to Value at Risk, heavy tails can impact the estimation of key measures of dependencies in financial time series. This includes the sample autocorrelation of the time series and of functions of the time series such as absolute values and squares. Standard central limit theory for mixing sequences generally directly applies to the sample autocorrelation functions (ACFs) of a financial time series and its squares, provided the fourth and eight moments, respectively, are finite. If these moments are infinite, as well may be the case for financial time series, then the asymptotic behavior of the sample ACFs is often nonstandard. As it turns out, GARCH processes and stochastic volatility (SV) processes, which are the primary modeling engines for financial returns, exhibit heavy tails in the marginal distribution. We focus on heavy tails and how the concept of regular variation plays a vital role in both these processes.
  </p>
  <p block-type="Text" class="has-continuation">
   It is often a misconception to associate heavytailed distributions with a very large variance. Rather, the term is used to describe data that exhibit bursts of outlying observations. These outlying observations could be orders of magnitude larger than the median
  </p>
  <p block-type="Text">
   of the observations. In the early 1960s, Mandelbrot (
   <i>
    see
   </i>
   <b>
    Mandelbrot, Benoit
   </b>
   ) [31], Mandelbrot and Taylor [32], and Fama [21] realized that the marginal distribution of returns appeared to be heavy tailed. To cope with heavy tails, they considered non-Gaussian stable distributions for the marginals. Since this class of distributions has infinite variance, it was a slightly controversial approach. On the other hand, for many financial time series, there is evidence that the marginal distribution may have a finite variance but an infinite fourth moment. Figure 1 contains two financial time series that exhibit heavy tails. Figure 1(a) consists of the daily pound/US dollar exchange rate from October 1, 1981 to June 28, 1985, while Figure 1(b) displays the log-returns of the daily closing price of Merck stock from January 2, 2003 through April 28, 2006. One can certainly detect the occasional bursts of outlying observations in both series that are representative of heavy tails. As described in the second section (see Figure 3c and d), there is statistical evidence that the tail behavior of the marginal distribution is heavy with possibly infinite fourth moments.
  </p>
  <p block-type="Text">
   Regular variation is a natural and often used concept to describe and model heavy-tailed phenomena. Many processes that are designed to model financial time series, such as the GARCH and heavytailed SV processes, have the property that all finitedimensional distributions are regularly varying. For such processes, one can apply standard results from extreme value theory for establishing limiting behavior of the extremes of the process, the sample ACF of the process and its squares, and a host of other statistics. The regular variation condition and its properties are described in the second section. In the third section, some of the main results on regular variation for GARCH and SV processes, respectively, are described. The fourth section describes some of the applications of the regular variation conditions mentioned in the third section, with emphasis on extreme values, point processes, and sample autocorrelations.
  </p>
  <h2>
   <b>
    Regular Variation
   </b>
  </h2>
  <p block-type="Text">
   Multivariate regular variation plays an indispensable role in extreme value theory and often serves as the starting point for modeling multivariate extremes. In some respect, one can regard a random vector that is regularly varying as the heavy-tailed analog
  </p>
  <p>
   <img src="_page_1_Figure_1.jpeg"/>
  </p>
  <p>
   Figure 1 Log-returns for US/pound exchange rate, October 1, 1981 to June 28, 1985 (a) and log-returns for closing price of Merck stock, January 2, 2003 to April 28, 2006 (b)
  </p>
  <p block-type="TextInlineMath">
   of a Gaussian random vector. Unlike a Gaussian random vector, which is characterized by the mean vector and all pairwise covariances, a regular varying random vector in
   <math display="inline">
    d
   </math>
   dimensions is characterized by two components, an index
   <math display="inline">
    \alpha &gt; 0
   </math>
   and a random vector
   <math display="inline">
    \boldsymbol{\Theta}
   </math>
   with values in
   <math display="inline">
    \mathbb{S}^{d-1}
   </math>
   , where
   <math display="inline">
    \mathbb{S}^{d-1}
   </math>
   denotes the unit sphere in
   <math display="inline">
    \mathbb{R}^d
   </math>
   with respect to the norm
   <math display="inline">
    |\cdot|
   </math>
   . The random vector
   <math display="inline">
    \mathbf{X}
   </math>
   is said to be
   <i>
    regularly varying
   </i>
   with index
   <math display="inline">
    -\alpha
   </math>
   if for all
   <math display="inline">
    t &gt; 0
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    \frac{P(|\mathbf{X}| &gt; tu, \mathbf{X}/|\mathbf{X}| \in \cdot)}{P(|\mathbf{X}| &gt; u)} \stackrel{v}{\to} t^{-\alpha} P(\mathbf{\Theta} \in \cdot)
   </math>
   <br/>
   as
   <math>
    u \to \infty
   </math>
   (1)
  </p>
  <p block-type="TextInlineMath">
   The symbol
   <math display="inline">
    \stackrel{v}{\rightarrow}
   </math>
   stands for vague convergence on
   <math display="inline">
    \mathbb{S}^{d-1}
   </math>
   ; vague convergence of measures is treated in detail in [27]. See [24, 36, 37] for background on multivariate regular variation. In this context, the convergence in equation (1) holds for all continuity sets
   <math display="inline">
    A \in \mathcal{B}(\mathbb{S}^{d-1})
   </math>
   of
   <math display="inline">
    \Theta
   </math>
   . In particular, equation (1) implies that the modulus of the random vector
   <math display="inline">
    |X|
   </math>
   is regularly varying, that is,
  </p>
  <p block-type="Equation">
   <math display="block">
    \lim_{u \to \infty} \frac{P(|\mathbf{X}| &gt; t \, u)}{P(|\mathbf{X}| &gt; u)} = t^{-\alpha} \tag{2}
   </math>
  </p>
  <p block-type="Text">
   Hence, roughly speaking, from the defining equation
   <math display="inline">
    (1)
   </math>
   , the modulus and angular parts of the random vector,
   <math display="inline">
    |X|
   </math>
   and
   <math display="inline">
    X/|X|
   </math>
   , are
   <i>
    independent
   </i>
   in the limit,
  </p>
  <p block-type="Text">
   that is,
  </p>
  <p block-type="Equation">
   <math display="block">
    P(|\mathbf{X}/|\mathbf{X}| \in A||\mathbf{X}| &gt; u) \to P(\mathbf{\Theta} \in A)
   </math>
   <br/>
   as
   <math>
    u \to \infty
   </math>
   (3)
  </p>
  <p block-type="Text">
   The distribution of
   <math display="inline">
    \Theta
   </math>
   is often called the
   <i>
    spectral measure
   </i>
   of the regularly varying random vector. The modulus has power-law-like tails in the sense that
  </p>
  <p block-type="Equation">
   <math display="block">
    P(|\mathbf{X}| &gt; x) = L(x)x^{-\alpha} \tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    L(x)
   </math>
   is a slowly varying function, that is, for any
   <math display="inline">
    t &gt; 0
   </math>
   ,
   <math display="inline">
    L(tx)/L(x) \to 1
   </math>
   as
   <math display="inline">
    x \to \infty
   </math>
   . This property implies that the rth moments of
   <math display="inline">
    |X|
   </math>
   are infinite for
   <math display="inline">
    r &gt; \alpha
   </math>
   and finite for
   <math display="inline">
    r &lt; \alpha
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   There is a second characterization of regular variation that is often useful in applications. Replacing u in equation (1) by the sequence
   <math display="inline">
    a_n &gt; 0
   </math>
   satisfying,
   <math display="inline">
    nP(|\mathbf{X}| &gt; a_n) \rightarrow 1
   </math>
   (i.e., we may take
   <math display="inline">
    a_n
   </math>
   to be the
   <math display="inline">
    1 - n^{-1}
   </math>
   quantile of
   <math display="inline">
    |\mathbf{X}|
   </math>
   ), we obtain
  </p>
  <p block-type="Equation">
   <math display="block">
    nP(|\mathbf{X}| &gt; t \, a_n \, , \, \mathbf{X}/|\mathbf{X}| \in \cdot \, ) \xrightarrow{v} t^{-\alpha} P(\mathbf{\Theta} \in \cdot \, )
   </math>
   <br/>
   as
   <math>
    n \to \infty
   </math>
   (5)
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   As expected, the multivariate regular variation condition collapses to the standard condition in the one-dimensional case
   <math display="inline">
    d = 1
   </math>
   . In this case,
   <math display="inline">
    \mathbb{S}^0 =
   </math>
   <math display="inline">
    \{-1, 1\}
   </math>
   , so that the random variable X is regular
  </p>
  <p block-type="Text">
   varying if and only if
   <math display="inline">
    |X|
   </math>
   is regularly varying
  </p>
  <p block-type="Equation">
   <math display="block">
    \lim_{u \to \infty} \frac{P(|X| &gt; t u)}{P(|X| &gt; u)} = t^{-\alpha} \tag{6}
   </math>
  </p>
  <p block-type="Text">
   and the tail balancing condition,
  </p>
  <p block-type="Equation">
   <math display="block">
    \lim_{u \to \infty} \frac{P(X &gt; u)}{P(|X| &gt; u)} = p \quad \text{and}
   </math>
   <br/>
   <math display="block">
    \lim_{u \to \infty} \frac{P(X &lt; -u)}{P(|X| &gt; u)} = q \tag{7}
   </math>
  </p>
  <p block-type="TextInlineMath">
   holds, where
   <math display="inline">
    p
   </math>
   and
   <math display="inline">
    q
   </math>
   are nonnegative constants with
   <math display="inline">
    p + q = 1
   </math>
   . The Pareto distribution,
   <i>
    t
   </i>
   -distribution, and nonnormal stable distributions are all examples of one-dimensional distributions that are regularly varying.
  </p>
  <p block-type="Text" class="has-continuation">
   <b>
    Example 1
   </b>
   (Independent components). Suppose that
   <math display="inline">
    \mathbf{X} = (X_1, X_2)'
   </math>
   consists of two independent and identically distributed (i.i.d.) components, where
   <math display="inline">
    X_1
   </math>
   is regularly varying random variable. The scatter plot of 10 000 replicates of these pairs, where
   <math display="inline">
    X_1
   </math>
   has a
   <math display="inline">
    t
   </math>
   -distribution with 3 degrees of freedom, is displayed in Figure 2(a). The
   <math display="inline">
    t
   </math>
   -distribution is regularly varying, with index
   <math display="inline">
    \alpha
   </math>
   being equal to the degrees of freedom. In this case, the spectral measure is a discrete distribution, which places equal mass at the intersection of
  </p>
  <p block-type="Text">
   the unit circle and the coordinate axes. That is,
  </p>
  <p block-type="Equation">
   <math display="block">
    P\left(\Theta = \frac{\pi k}{2}\right) = \frac{1}{4} \quad \text{for } k = -1, 0, 1, 2
   </math>
   (8)
  </p>
  <p block-type="Text">
   The scatter plot in Figure 2 reflects the form of the spectral distribution. The points that are far from the origin occur only near the coordinate axes. The interpretation is that the probability that both components of the random vector are large at the same time is quite small.
  </p>
  <p block-type="TextInlineMath">
   Example 2 (Totally Dependent Components). In contrast to the independent case of Example 1, suppose that both components of the vector are identical, that is,
   <math display="inline">
    \mathbf{X} = (X, X)
   </math>
   , with X regularly varying in one dimension. Independent replicates of this random vector would just produce points lying on a
   <math display="inline">
    45^{\circ}
   </math>
   line through the origin. Here, it is easy to see that the vector is regularly varying with spectral measure given by
  </p>
  <p block-type="Equation">
   <math display="block">
    P\left(\Theta = \frac{\pi}{4}\right) = p \quad \text{and} \quad P\left(\Theta = \frac{-\pi}{4}\right) = q \quad (9)
   </math>
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 3
   </b>
   (AR(1) Process). Let
   <math display="inline">
    \{X_t\}
   </math>
   be the AR(1) process defined by the recursion:
  </p>
  <p block-type="Equation">
   <math display="block">
    X_t = 0.9X_{t-1} + Z_t \tag{10}
   </math>
  </p>
  <p>
   <img src="_page_2_Figure_14.jpeg"/>
  </p>
  <p>
   <b>
    Figure 2
   </b>
   Scatter plot of 10 000 pairs of observations with i.i.d. components having a
   <math display="inline">
    t
   </math>
   -distribution with 3 degrees of freedom (a) and 10 000 observations of
   <math display="inline">
    (X_t, X_{t+1})
   </math>
   from an AR(1) process (b)
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \{Z_t\}
   </math>
   is an i.i.d. sequence of random variables that have a symmetric stable distribution with exponent 1.8. This stable distribution is regularly varying with index
   <math display="inline">
    \alpha = 1.8
   </math>
   . Since
   <math display="inline">
    X_t = \sum_{i=0}^{\infty} 0.9^i Z_{t-i}
   </math>
   is a linear process, it follows [14, 15] that
   <math display="inline">
    X_t
   </math>
   is also symmetric and regularly varying with index 1.8. In fact,
   <math display="inline">
    X_t
   </math>
   has a symmetric stable distribution with exponent 1.8 and scale parameter
   <math display="inline">
    (1 - 0.9^{1.8})^{-1/1.8}
   </math>
   . The scatter plot of consecutive observations
   <math display="inline">
    (X_t, X_{t+1})
   </math>
   based on 10 000 observations generated from an AR(1) process is displayed in Figure
   <math display="inline">
    2(b)
   </math>
   . It can be shown that all finite-dimensional distributions of this time series are regularly varying. The spectral distribution of the vector consisting of two consecutive observations
   <math display="inline">
    \mathbf{X} = (X_t, X_{t+1})
   </math>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    P(\Theta = \pm \arctan(0.9)) = 0.9898
   </math>
   and
   <br/>
   <math>
    P(\Theta = \pm \pi/2) = 0.0102
   </math>
   (11)
  </p>
  <p block-type="Text">
   As seen in Figure 2, one can see that most of the points in the scatter plot, especially those far from the origin, cluster tightly around the line through the origin with slope 0.9. This corresponds to the large mass at
   <math display="inline">
    arctan(0.9)
   </math>
   of the distribution of
   <math display="inline">
    \Theta
   </math>
   . One can also detect a smattering of extreme points clustered around the vertical axis.
  </p>
  <h4>
   <i>
    Estimation of
   </i>
   <math display="inline">
    \alpha
   </math>
  </h4>
  <p block-type="TextInlineMath">
   A great deal of attention in the extreme value theory community has been devoted to the estimation of
   <math display="inline">
    \alpha
   </math>
   in the regular variation condition (1). The generic Hill estimate is often a good starting point for this task. There are more sophisticated versions of Hill estimates, see [23] for a nice treatment of Hill estimators, but for illustration we stick with the standard version. For observations
   <math display="inline">
    X_1, \ldots, X_n
   </math>
   from a nonnegative-valued time series, let
   <math display="inline">
    X_{n:1} &gt; \cdots &gt; X_{n:n}
   </math>
   be the corresponding descending order statistics. If the data were in fact i.i.d. from a Pareto distribution, then the maximum likelihood estimator of
   <math display="inline">
    \alpha^{-1}
   </math>
   based on the largest
   <math display="inline">
    m + 1
   </math>
   order statistics is
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\alpha}^{-1} = \frac{1}{m} \sum_{j=1}^{m} \left( \ln X_{n:j} - \ln X_{n:m+1} \right) \tag{12}
   </math>
  </p>
  <p block-type="Text" class="has-continuation">
   Different values of m produce an array of
   <math display="inline">
    \alpha
   </math>
   estimates. The typical operating procedure is to plot the estimate of
   <math display="inline">
    \alpha
   </math>
   versus
   <i>
    m
   </i>
   and choose a value
  </p>
  <p block-type="TextInlineMath">
   of
   <math display="inline">
    m
   </math>
   where the plot appears horizontal for an extended segment. See [7, 37] for other procedures for selecting
   <math display="inline">
    m
   </math>
   . There is the typical bias
   <i>
    versus
   </i>
   variance trade-off, with larger
   <math display="inline">
    m
   </math>
   producing smaller variance but larger bias. Figure 3 contains graphs of the Hill estimate of
   <math display="inline">
    \alpha
   </math>
   as a function of
   <i>
    m
   </i>
   for the two simulated series in Figure 2 and the exchange rate and log-return data of Figure 1. In all cases, one can see a range of m for which the graph of
   <math display="inline">
    \hat{\alpha}
   </math>
   is relatively flat. Using this segment as an estimate of
   <math display="inline">
    \alpha
   </math>
   , we would estimate the index as approximately 3 for the two simulated series, approximately 3 for the exchange rate data, and around 3.5 for the stock price data. (The value of
   <math display="inline">
    \alpha
   </math>
   for the two simulated series is indeed 3.) Also displayed on the plots are
   <math display="inline">
    95\%
   </math>
   confidence intervals for
   <math display="inline">
    \alpha
   </math>
   , assuming the data are i.i.d. As suggested by these plots, the return data appear to have quite heavy tails.
  </p>
  <h4>
   <i>
    Estimation of the Spectral Distribution
   </i>
  </h4>
  <p block-type="TextInlineMath">
   Using property (3), a naive estimate of the distribution of
   <math display="inline">
    \Theta
   </math>
   is based on the angular components
   <math display="inline">
    \mathbf{X}_{t}/|\mathbf{X}_{t}|
   </math>
   in the sample. One simply uses the empirical distribution of these angular pieces for which the modulus
   <math display="inline">
    |X_t|
   </math>
   exceeds some large threshold. More details can be found in [37]. For the scatter plots in Figure 2, we produced in Figure 4 kernel density estimates of the spectral density function for the random variable
   <math display="inline">
    \Theta
   </math>
   on
   <math display="inline">
    (-\pi, \pi]
   </math>
   . One can see in the graph of the i.i.d. data, the large spikes at values of
   <math display="inline">
    \theta = -\pi, -\pi/2, 0, \pi/2, \pi
   </math>
   corresponding to the coordinate axes (the values at
   <math display="inline">
    -\pi
   </math>
   and
   <math display="inline">
    \pi
   </math>
   should be grouped together). On the other hand for the
   <math display="inline">
    AR(1)
   </math>
   process, the density estimate puts large mass at
   <math display="inline">
    \theta = \arctan(0.9)
   </math>
   and
   <math display="inline">
    \theta = \arctan(0.9) - \pi
   </math>
   corresponding to the line with slope 0.9 in the first and third quadrants, respectively. Since there are only a few points on the vertical axis, the density estimate does not register much mass at 0 and
   <math display="inline">
    \pi
   </math>
   .
  </p>
  <h2>
   Regular Variation for GARCH and SV Processes
  </h2>
  <h3>
   GARCH Processes
  </h3>
  <p block-type="Text">
   The autoregressive conditional heterscedastic (ARCH) process developed by Engle [19] and its generalized version, GARCH, developed by Engle
  </p>
  <p>
   <img src="_page_4_Figure_1.jpeg"/>
  </p>
  <p>
   Figure 3 Hill plots for tail index: (a) i.i.d. data in Figure 2; (b) AR(1) process in Figure 2; (c) log-returns for US/pound exchange rate; and (d) log-returns for Merck stock, January 2, 2003 to April 28, 2006
  </p>
  <p block-type="TextInlineMath">
   and Bollerslev [20] are perhaps the most popular models for financial time series (see GARCH Models). Although there are many variations of the GARCH process, we focus on the traditional version. We say that
   <math display="inline">
    \{X_t\}
   </math>
   is a GARCH
   <math display="inline">
    (p, q)
   </math>
   process if it is a strictly stationary solution of the equations:
  </p>
  <p block-type="Equation">
   <math display="block">
    X_{t} = \sigma_{t} Z_{t}
   </math>
   <br/>
   <math display="block">
    \sigma_{t}^{2} = \alpha_{0} + \sum_{i=1}^{p} \alpha_{i} X_{t-i}^{2}
   </math>
   <br/>
   <math display="block">
    + \sum_{j=1}^{q} \beta_{j} \sigma_{t-j}^{2}, \quad t \in \mathbb{Z}
   </math>
   (13)
  </p>
  <p block-type="TextInlineMath">
   where the
   <i>
    noise
   </i>
   or
   <i>
    innovations
   </i>
   sequence
   <math display="inline">
    (Z_t)_{t \in \mathbb{Z}}
   </math>
   is an i.i.d. sequence with mean zero and unit variance. It is usually assumed that all coefficients
   <math display="inline">
    \alpha_i
   </math>
   and
   <math display="inline">
    \beta_i
   </math>
   are nonnegative, with
   <math display="inline">
    \alpha_0 &gt; 0
   </math>
   . For identification purposes, the variance of the noise is assumed to be 1 since otherwise its standard deviation can be absorbed into
   <math display="inline">
    \sigma_t
   </math>
   .
   <math display="inline">
    (\sigma_t)
   </math>
   is referred to as the
   <i>
    volatility
   </i>
   sequence of the GARCH process.
  </p>
  <p block-type="TextInlineMath">
   The parameters are typically chosen to ensure that a causal and strictly stationary solution to the equations (13) exists. This means that
   <math display="inline">
    X_t
   </math>
   has a representation as a measurable function of the past and present noise values
   <math display="inline">
    Z_s
   </math>
   ,
   <math display="inline">
    s \le t
   </math>
   . The necessary and sufficient conditions for the existence and uniqueness of a stationary ergodic solution to equation
   <math display="inline">
    (13)
   </math>
   are
  </p>
  <p>
   <img src="_page_5_Figure_1.jpeg"/>
  </p>
  <p>
   Figure 4 The estimation of the spectral density function for i.i.d. components (a) and for the
   <math display="inline">
    AR(1)
   </math>
   process (b) from Figure 2
  </p>
  <p block-type="Text">
   given in [35] for the
   <math display="inline">
    GARCH(1, 1)
   </math>
   case and for the general GARCH
   <math display="inline">
    (p, q)
   </math>
   case in [4]; see [30] for a summary of the key properties of a GARCH process. In some cases, one only assumes weak stationarity, in which case the conditions on the parameters reduce substantially. A GARCH process is weakly stationary if and only if
  </p>
  <p block-type="Equation">
   <math display="block">
    \alpha_0 &gt; 0 \quad \text{and} \quad \sum_{j=1}^p \alpha_j + \sum_{j=1}^q \beta_j &lt; 1 \tag{14}
   </math>
  </p>
  <p block-type="Text">
   To derive properties of the tail of the finitedimensional distributions of a GARCH process, including the marginal distribution, it is convenient to embed the squares
   <math display="inline">
    X_t^2
   </math>
   and
   <math display="inline">
    \sigma_t^2
   </math>
   in a stochastic recurrence equation (SRE). This embedding can be used to derive other key properties of the process beyond the finite-dimensional distributions. For example, conditions for stationarity and
   <math display="inline">
    \beta
   </math>
   -mixing can be established from the properties of SREs and general theory of Markov chains. Here, we focus on the tail behavior.
  </p>
  <p block-type="Text">
   One builds an SRE by including the volatility process in the state vector. An SRE takes the form
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{Y}_t = \mathbf{A}_t \mathbf{Y}_{t-1} + \mathbf{B}_t \tag{15}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{Y}_t
   </math>
   is an
   <i>
    m
   </i>
   -dimensional random vector,
   <math display="inline">
    \mathbf{A}_t
   </math>
   is an
   <math display="inline">
    m \times m
   </math>
   random matrix,
   <b>
    B
   </b>
   &lt;sub&gt;t&lt;/sub&gt; is a random vector, and
   <math display="inline">
    \{(\mathbf{A}_t, \mathbf{B}_t)\}
   </math>
   is an i.i.d. sequence. Under suitable conditions on the coefficient matrices and error matrices, one can derive various properties about the Markov chain
   <math display="inline">
    \mathbf{Y}_t
   </math>
   . For example, iteration of equation (15) yields a unique stationary and causal solution:
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{Y}_{t} = \mathbf{B}_{t} + \sum_{i=1}^{\infty} \mathbf{A}_{t} \cdots \mathbf{A}_{t-i+1} \mathbf{B}_{t-i} , \quad t \in \mathbb{Z} \tag{16}
   </math>
  </p>
  <p block-type="Text">
   To ensure almost surely (a.s.) convergence of the infinite series in equation
   <math display="inline">
    (16)
   </math>
   , and hence the existence of a unique a strictly stationary solution to equation
   <math display="inline">
    (15)
   </math>
   , it is assumed that the
   <i>
    top Lyapunov exponent
   </i>
   given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma = \inf_{n \ge 1} n^{-1} E \log \|\mathbf{A}_n \cdots \mathbf{A}_1\| \tag{17}
   </math>
  </p>
  <p block-type="Text">
   is negative, where
   <math display="inline">
    \|\cdot\|
   </math>
   is the operator norm corresponding to a given norm in
   <math display="inline">
    \mathbb{R}^m
   </math>
   .
  </p>
  <p block-type="Text">
   Now, the GARCH process, at least its squares, can be embedded into an SRE by choosing
  </p>
  <p block-type="Text">
   <math display="inline">
    (18)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{Y}_{t} = \begin{pmatrix} \sigma_{t+1}^{2} \\ \vdots \\ \sigma_{t-q+2}^{2} \\ X_{t}^{2} \\ \vdots \\ X_{t-p+1}^{2} \end{pmatrix}, \quad \mathbf{A}_{t} = \begin{pmatrix} \alpha_{1}Z_{t}^{2} + \beta_{1} &amp; \beta_{2} &amp; \cdots &amp; \beta_{q-1} &amp; \beta_{q} &amp; \alpha_{2} &amp; \alpha_{3} &amp; \cdots &amp; \alpha_{p} \\ 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ Z_{t}^{2} &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0 \end{pmatrix}
   </math>
   <math display="block">
    \mathbf{B}_{t} = (\alpha_{0}, 0, \ldots, 0)'
   </math>
  </p>
  <p block-type="TextInlineMath">
   where, as required,
   <math display="inline">
    \{(\mathbf{A}_t, \mathbf{B}_t)\}\
   </math>
   is an i.i.d. sequence. The top row in the SRE for the GARCH specification follows directly from the definition of the squared volatility process
   <math display="inline">
    \sigma_{t+1}^2
   </math>
   and the property that
   <math display="inline">
    X_t =
   </math>
   <math display="inline">
    \sigma_t Z_t
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   In general, the top Lyapunov coefficient
   <math display="inline">
    \gamma
   </math>
   for the GARCH SRE cannot be calculated explicitly. However, a sufficient condition for
   <math display="inline">
    \gamma &lt; 0
   </math>
   is given as
  </p>
  <p block-type="Equation">
   <math display="block">
    \sum_{i=1}^{p} \alpha_i + \sum_{j=1}^{q} \beta_j &lt; 1 \tag{19}
   </math>
  </p>
  <p block-type="TextInlineMath">
   see p. 122 [4]. It turns out that this condition is also necessary and sufficient for the existence of a weakly stationary solution to the GARCH recursions. The solution will also be strictly stationary in this case.
  </p>
  <p block-type="Text">
   It has been noted that for many financial time series, the
   <math display="inline">
    GARCH(1,1)
   </math>
   often provides an adequate model or is at least a good starter model. This is one of the few models where the Lyapunov coefficient can be computed explicitly. In this case, the SRE equation essentially collapses to the one-dimensional SRE given as
  </p>
  <p block-type="Equation">
   <math display="block">
    \sigma_{t+1}^2 = \alpha_0 + (\alpha_1 Z_t^2 + \beta_1) \sigma_t^2 = A_t \sigma_t^2 + \alpha_0 \tag{20}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    A_t = \alpha_1 Z_t^2 + \beta_1
   </math>
   . The elements in the second row in the vector and matrix components of equation
   <math display="inline">
    (18)
   </math>
   play no role in this case. Hence,
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma = n^{-1} E \log \left( A_n \cdots A_1 \right) = E \log A_1
   </math>
   <math display="block">
    = E \log \left( \alpha_1 Z^2 + \beta_1 \right) \tag{21}
   </math>
  </p>
  <p block-type="TextInlineMath">
   The conditions [35],
   <math display="inline">
    E \log(\alpha_1 Z^2 + \beta_1) &lt; 0
   </math>
   and
   <math display="inline">
    \alpha_0 &gt; 0
   </math>
   , are necessary and sufficient for the existence of a stationary causal nondegenerate solution to the
   <math display="inline">
    GARCH(1,1)
   </math>
   equations.
  </p>
  <p block-type="TextInlineMath">
   Once the squares and volatility sequence,
   <math display="inline">
    X_t^2
   </math>
   and
   <math display="inline">
    \sigma_t^2
   </math>
   , respectively, are embedded in an SRE, then one can apply classical theory for SREs as developed by Kesten [28], (see also [22]), and extended by Basrak et al. [2], to establish regular variation of the tails of
   <math display="inline">
    X_t^2
   </math>
   and
   <math display="inline">
    \sigma_t^2
   </math>
   . The following result by Basrak
   <i>
    et al.
   </i>
   [1] summarizes the key results applied to a GARCH process.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Theorem 1
   </b>
   Consider the process
   <math display="inline">
    (Y_t)
   </math>
   in equation (18) obtained from embedding a stationary GARCH process into the SRE
   <math display="inline">
    (18)
   </math>
   . Assume that Z has a positive density on
   <math display="inline">
    \mathbb{R}
   </math>
   such that
   <math display="inline">
    E(|Z|^h) &lt; \infty
   </math>
   for
   <math display="inline">
    h &lt; h_0
   </math>
   and
   <math display="inline">
    E(|Z|^{h_0}) = \infty
   </math>
   for some
   <math display="inline">
    h_0 \in (0, \infty]
   </math>
   . Then with
   <math display="inline">
    \mathbf{Y} = \mathbf{Y}_1
   </math>
   , there exist
   <math display="inline">
    \alpha &gt; 0
   </math>
   , a constant
   <math display="inline">
    c &gt; 0
   </math>
   0. and a random vector
   <math display="inline">
    \boldsymbol{\Theta}
   </math>
   on the unit sphere
   <math display="inline">
    \mathbb{S}^{p+q-2}
   </math>
   such that
  </p>
  <p block-type="Equation">
   <math display="block">
    x^{\alpha/2}P(|\mathbf{Y}| &gt; x) \to c \quad \text{as } x \to \infty \tag{22}
   </math>
  </p>
  <p block-type="TextInlineMath">
   and for every
   <math display="inline">
    t &gt; 0
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \frac{P(|\mathbf{Y}| &gt; tx, \mathbf{Y}/|\mathbf{Y}| \in \cdot)}{P(|\mathbf{Y}| &gt; x)} \xrightarrow{w} t^{-\alpha/2} P(\mathbf{\Theta} \in \cdot)
   </math>
   <br/>
   as
   <math>
    x \to \infty
   </math>
   (23)
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \stackrel{w}{\rightarrow}
   </math>
   denotes weak convergence on the Borel
   <math display="inline">
    \sigma
   </math>
   -(i) field of
   <math display="inline">
    \mathbb{S}^{p+q-2}
   </math>
   a
  </p>
  <p block-type="Text">
   It follows that the components of the vector of
   <math display="inline">
    \mathbf{Y}
   </math>
   are also regularly varying so that
  </p>
  <p block-type="Equation">
   <math display="block">
    P(|X_1| &gt; x) \sim c_1 x^{-\alpha} \quad \text{and}
   </math>
   <br/>
   <math display="block">
    P(\sigma_1 &gt; x) \sim c_2 x^{-\alpha} \tag{24}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for some positive constants
   <math display="inline">
    c_1
   </math>
   and
   <math display="inline">
    c_2
   </math>
   . A straightforward application of Breiman's lemma [6], (cf. [13], Section 4), allows us to remove the absolute values in
   <math display="inline">
    X_1
   </math>
   to obtain
  </p>
  <p block-type="Equation">
   <math display="block">
    P(X_1 &gt; x) = P(\sigma_1 Z_1^+ &gt; x)
   </math>
   <br/>
   <math display="block">
    \sim E((Z_1^+)^{\alpha}) P(\sigma_1 &gt; x) \quad (25)
   </math>
   <br/>
   <math display="block">
    P(X_1 \leq -x) = P(-\sigma_1 Z_1^- \leq -x)
   </math>
   <br/>
   <math display="block">
    \sim E((Z_1^-)^{\alpha}) P(\sigma_1 &gt; x) \quad (26)
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    Z_1^{\pm}
   </math>
   are the respective positive and negative parts of
   <math display="inline">
    Z_1
   </math>
   . With the exception of simple models such as the
   <math display="inline">
    GARCH(1,1)
   </math>
   , there is no explicit formula for the index
   <math display="inline">
    \alpha
   </math>
   of regular variation of the marginal distribution. In principle,
   <math display="inline">
    \alpha
   </math>
   could be estimated from the data using a Hill style estimator, but an enormous sample size would be required in order to obtain a precise estimate of the index.
  </p>
  <p block-type="Text">
   In the GARCH(1,1) case,
   <math display="inline">
    \alpha
   </math>
   is found by solving the following equation:
  </p>
  <p block-type="Equation">
   <math display="block">
    E[(\alpha_1 Z^2 + \beta_1)^{\alpha/2}] = 1 \tag{27}
   </math>
  </p>
  <p block-type="TextInlineMath">
   This equation can be solved for
   <math display="inline">
    \alpha
   </math>
   by numerical and/or simulation methods for fixed values of
   <math display="inline">
    \alpha_1
   </math>
   and
   <math display="inline">
    \beta_1
   </math>
   from the stationarity region of a GARCH(1,1) process and assuming a concrete density for Z. (See [12] for a table of values of
   <math display="inline">
    \alpha
   </math>
   for various choices of
   <math display="inline">
    \alpha_1
   </math>
   and
   <math display="inline">
    \beta_1
   </math>
   .) Note that in the case of an integrated GARCH (IGARCH) process where
   <math display="inline">
    \alpha_1 + \beta_1 = 1
   </math>
   , then we have
   <math display="inline">
    \alpha = 2
   </math>
   . This holds regardless of the distribution of
   <math display="inline">
    Z_1
   </math>
   , provided it has a finite variance. Since the marginal distribution of an IGARCH process has Pareto-like tails with index 2, the variance is infinite.
  </p>
  <p block-type="TextInlineMath">
   While equations
   <math display="inline">
    (25)
   </math>
   and
   <math display="inline">
    (26)
   </math>
   describe only the regular variation of the marginal distribution, it is also true that the finite-dimensional distributions are regularly varying. To see this in the
   <math display="inline">
    GARCH(1,1)
   </math>
   case, we note that the volatility process is given as
  </p>
  <p block-type="Equation">
   <math display="block">
    \sigma_{t+1}^2 = (\alpha_1 Z_t^2 + \beta_1)\sigma_t^2 + \beta_0 \tag{28}
   </math>
  </p>
  <p block-type="Text">
   so that
  </p>
  <p block-type="Equation">
   <math display="block">
    (\sigma_1^2, \dots, \sigma_m^2) = (1, \alpha_1 Z_1^2 + \beta_1, (\alpha_1 Z_2^2 + \beta_1) \times (\alpha_1 Z_1^2 + \beta_1), \dots, \alpha_1 Z_{m-1}^2 + \beta_1) \dots \times (\alpha_1 Z_1^2 + \beta_1) \sigma_1^2 + \mathbf{R}_m
   </math>
   <br/>
   =
   <math>
    \mathbf{D}_m \sigma_1^2 + \mathbf{R}_m
   </math>
   (29)
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{R}_m
   </math>
   has tails that are lighter than those for
   <math display="inline">
    \sigma_1^2
   </math>
   . Now since
   <math display="inline">
    \mathbf{D}_m = (D_1, \ldots, D_m)
   </math>
   is independent of
   <math display="inline">
    \sigma_1^2
   </math>
   and has a
   <math display="inline">
    \alpha/2 + \delta
   </math>
   moment for some
   <math display="inline">
    \delta &gt; 0
   </math>
   , it follows by a generalization of Breiman's lemma [1] that
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{U}_m := (X_1^2, \dots, X_m^2) = \mathbf{F}_m \sigma_1^2 + \mathbf{R}_m \tag{30}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{F}_m = (Z_1^2 D_1, \ldots, Z_m^2 D_m)
   </math>
   is regularly varying with
  </p>
  <p block-type="Equation">
   <math display="block">
    \lim_{x \to \infty} \frac{P(|\mathbf{U}_m| &gt; x, \mathbf{U}_m/|\mathbf{U}_m| \in A)}{P(|\mathbf{U}_m| &gt; x)}
   </math>
   <math display="block">
    = \lim_{x \to \infty} \frac{P(|\mathbf{F}_m|\sigma_1^2 &gt; x, \mathbf{F}_m/|\mathbf{F}_m| \in A)}{P(|\mathbf{F}_m|\sigma_1^2 &gt; x)}
   </math>
   <math display="block">
    = \frac{E\left(|\mathbf{F}_m|^{\alpha/2}I_A(\mathbf{F}_m/|\mathbf{F}_m|)\right)}{E|\mathbf{F}_m|^{\alpha/2}} \tag{31}
   </math>
  </p>
  <p block-type="Text">
   It follows that the finite-dimensional distributions of a GARCH process are regularly varying.
  </p>
  <h2>
   Stochastic Volatility Processes
  </h2>
  <p block-type="Text">
   The SV process also starts with the multiplicative
   <math display="inline">
    model(13)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    X_t = \sigma_t Z_t \tag{32}
   </math>
  </p>
  <p block-type="TextInlineMath">
   with
   <math display="inline">
    (Z_t)
   </math>
   being an i.i.d. sequence of random variables. If
   <math display="inline">
    \text{var}(Z_t) &lt; \infty
   </math>
   , then it is conventional to assume that
   <math display="inline">
    Z_t
   </math>
   has mean 0 and variance 1. Unlike the GARCH process, the volatility process
   <math display="inline">
    (\sigma_t)
   </math>
   for SV processes is assumed to be independent of the sequence
   <math display="inline">
    (Z_t)
   </math>
   . Often, one assumes that
   <math display="inline">
    \log \sigma_t^2
   </math>
   is a linear Gaussian process given by
  </p>
  <p block-type="Equation">
   <math display="block">
    \log \sigma_t^2 = Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \eta_{t-j} \tag{33}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    (\psi_i)
   </math>
   is a sequence of square summable coefficients and
   <math display="inline">
    (\eta_t)
   </math>
   is a sequence of i.i.d. N(0,
   <math display="inline">
    \sigma^2
   </math>
   ) random variables independent of
   <math display="inline">
    (Z_t)
   </math>
   . If
   <math display="inline">
    \text{var}(Z_t)
   </math>
   is
  </p>
  <p block-type="TextInlineMath">
   finite and equal to 1, then the SV process
   <math display="inline">
    X_t =
   </math>
   <math display="inline">
    \sigma_t Z_t = \exp^{Y_t/2} Z_t
   </math>
   is white noise with mean 0 and variance
   <math display="inline">
    \exp{\{\mu + \sigma^2 \sum_{j=0}^{\infty} \psi_j^2/2\}}
   </math>
   . One advantage of such processes is that one can explicitly compute the autocovariance function (ACVF) of any power of
   <math display="inline">
    X_t
   </math>
   and its absolute values. For example, the ACVF of the squares of
   <math display="inline">
    (X_t)
   </math>
   is, for
   <math display="inline">
    h &gt; 0
   </math>
   , given as
  </p>
  <p block-type="Equation">
   <math display="block">
    \begin{split} \gamma_{|X|^2}(h) &amp;= E(\exp\{Y_0 + Y_h\}) - (E\exp\{Y_0\})^2 \\ &amp;= \exp\left\{2\mu + \sigma^2 \sum_{i=0}^{\infty} \psi_i^2\right\} \\ &amp;\times \left[\exp\left\{\sigma^2 \sum_{i=0}^{\infty} \psi_i \psi_{i+h}\right\} - 1\right] \\ &amp;= e^{2\mu} e^{\gamma_Y(0)} \Big[e^{\gamma_Y(h)} - 1\Big] \end{split} \tag{34}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Note that as
   <math display="inline">
    h \to \infty
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_{|X|^2}(h) \sim e^{2\mu} e^{\gamma_Y(0)} \left[ e^{\gamma_Y(h)} - 1 \right] \sim e^{2\mu} e^{\gamma_Y(0)} \gamma_Y(h)
   </math>
   (35)
  </p>
  <p block-type="Text">
   so that the ACVF of the SV for the squares converges to zero at the same rate as the log-volatility process.
  </p>
  <p block-type="TextInlineMath">
   If
   <math display="inline">
    Z_t
   </math>
   has a Gaussian distribution, then the tail of
   <math display="inline">
    X_t
   </math>
   remains light although a bit heavier than a Gaussian [3]. This is in contrast to the GARCH case where an i.i.d. Gaussian input leads to heavytailed marginals of the process. On the other hand, for SV processes, if the
   <math display="inline">
    Z_t
   </math>
   have heavy tails, for example, if
   <math display="inline">
    Z_t
   </math>
   has a
   <i>
    t
   </i>
   -distribution, then Davis and Mikosch [10] show that
   <math display="inline">
    X_t
   </math>
   is regularly varying. Furthermore, in this case, any finite collection of
   <math display="inline">
    X_t
   </math>
   's has the same limiting joint tail behavior as an i.i.d. sequence with regularly varying marginals. Specifically, the two random vectors,
   <math display="inline">
    (X_1, \ldots, X_k)'
   </math>
   and
   <math display="inline">
    (E|\sigma_1|^{\alpha})^{1/\alpha}(Z_1,\ldots,Z_k)'
   </math>
   have the same joint tail behavior.
  </p>
  <h3>
   <b>
    Limit Theory GARCH and SV Processes
   </b>
  </h3>
  <h4>
   Convergence of Maxima
  </h4>
  <p block-type="TextInlineMath">
   If
   <math display="inline">
    (X_t)
   </math>
   is a stationary sequence of random variables with common distribution function
   <math display="inline">
    F
   </math>
   , then often one can directly relate the limiting distribution of the maxima,
   <math display="inline">
    M_n = \max\{X_1, \ldots, X_n\}
   </math>
   to F. Assuming
  </p>
  <p block-type="TextInlineMath">
   that
   <math display="inline">
    X_1
   </math>
   is regularly varying with index
   <math display="inline">
    -\alpha
   </math>
   and choosing the sequence
   <math display="inline">
    (a_n)
   </math>
   such that
   <math display="inline">
    n(1 - F(a_n)) \rightarrow 1
   </math>
   , then
  </p>
  <p block-type="Equation">
   <math display="block">
    F^{n}(a_{n}x) \to G(x) = \begin{cases} 0, &amp; x \le 0 \\ e^{-x^{-\alpha}}, &amp; x &gt; 0 \end{cases}
   </math>
   (36)
  </p>
  <p block-type="TextInlineMath">
   This relation is equivalent to convergence in distribution of the maxima of the associated independent sequence
   <math display="inline">
    (\hat{X}_t)
   </math>
   (i.e., the sequence
   <math display="inline">
    (\hat{X}_t)
   </math>
   is i.i.d. with common distribution function
   <math display="inline">
    F
   </math>
   ) normalized by
   <math display="inline">
    a_n
   </math>
   to the Fréchet distribution G. Specifically, if
   <math display="inline">
    \hat{M}_n =
   </math>
   <math display="inline">
    \max\{\hat{X}_1,\ldots,\hat{X}_n\}
   </math>
   , then
  </p>
  <p block-type="Equation">
   <math display="block">
    P(a_n^{-1}M_n \le x) \to G(x) \tag{37}
   </math>
  </p>
  <p block-type="Text">
   Under mild mixing conditions on the sequence
   <math display="inline">
    (X_t)
   </math>
   <math display="inline">
    [29]
   </math>
   , we have
  </p>
  <p block-type="Equation">
   <math display="block">
    P(a_n^{-1}M_n \le x) \to H(x) \tag{38}
   </math>
  </p>
  <p block-type="Text">
   with
   <math display="inline">
    H
   </math>
   a nondegenerate distribution function if and only if
  </p>
  <p block-type="Equation">
   <math display="block">
    H(x) = G^{\theta}(x) \tag{39}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for some
   <math display="inline">
    \theta \in (0, 1]
   </math>
   . The parameter
   <math display="inline">
    \theta
   </math>
   is called the
   <i>
    extremal index
   </i>
   and can be viewed as a sample size adjustment for the maxima of the dependent sequence due to clustering of the extremes. The case
   <math display="inline">
    \theta = 1
   </math>
   corresponds to no clustering, in which case the limiting behavior of
   <math display="inline">
    M_n
   </math>
   and
   <math display="inline">
    \hat{M}_n
   </math>
   are identical. In case
   <math display="inline">
    \theta &lt; 1
   </math>
   ,
   <math display="inline">
    M_n
   </math>
   behaves asymptotically like the maximum of
   <math display="inline">
    n\theta
   </math>
   independent observations. The reciprocal of the extremal index
   <math display="inline">
    1/\theta
   </math>
   of a stationary sequence
   <math display="inline">
    (X_t)
   </math>
   also has the interpretation as the expected size of clusters of high-level exceedances in the sequence.
  </p>
  <p block-type="TextInlineMath">
   There are various sufficient conditions for ensuring that
   <math display="inline">
    \theta = 1
   </math>
   . Perhaps the most common anticlustering condition is
   <math display="inline">
    D'
   </math>
   [28], which has the following form:
  </p>
  <p block-type="Equation">
   <math display="block">
    \limsup_{n \to \infty} n \sum_{t=2}^{[n/k]} P(X_1 &gt; a_n x, X_t &gt; a_n x) = O(1/k)
   </math>
   (40)
  </p>
  <p block-type="TextInlineMath">
   as
   <math display="inline">
    k \to \infty
   </math>
   . Hence, if the stationary process
   <math display="inline">
    (X_t)
   </math>
   satisfies a mixing condition and
   <math display="inline">
    D'
   </math>
   , then
  </p>
  <p block-type="Equation">
   <math display="block">
    P(a_n^{-1}M_n \le x) \to G(x) \tag{41}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Returning to the GARCH setting, we assume that the conditions of Theorem 1 are satisfied. Then we know that
   <math display="inline">
    P(|X| &gt; x) \sim c_1 x^{-\alpha}
   </math>
   for some
   <math display="inline">
    \alpha, c_1 &gt; 0
   </math>
   , and we can even specify the value of
   <math display="inline">
    \alpha
   </math>
   in the
   <math display="inline">
    GARCH(1, 1)
   </math>
   case by solving equation (27). Now choosing
   <math display="inline">
    a_n = n^{1/\alpha} c_1^{1/\alpha}
   </math>
   , we have
   <math display="inline">
    nP(|X_1| &gt; a_n) \rightarrow
   </math>
   1 and defining
   <math display="inline">
    M_n = \max\{|X_1|, \ldots, |X_n|\}
   </math>
   , we obtain
  </p>
  <p block-type="Equation">
   <math display="block">
    P(a_n^{-1}M_n \le x) \to \exp\{-\theta_1 x^{-\alpha}\}\tag{42}
   </math>
  </p>
  <p block-type="Text">
   where the extremal index
   <math display="inline">
    \theta_1
   </math>
   is strictly less than 1. Explicit formulae for the extremal index of a general GARCH process are hard to come by. In some special cases, such as the ARCH
   <math display="inline">
    (1)
   </math>
   and the GARCH
   <math display="inline">
    (1,1)
   </math>
   , there are more explicit expressions. For example, in the GARCH(1,1) case, the extremal index
   <math display="inline">
    \theta_1
   </math>
   for the maxima of the absolute values of the GARCH process is given by Mikosch and Stărică [34]
  </p>
  <p block-type="Equation">
   <math display="block">
    \theta_{1} = \frac{\lim_{k \to \infty} E\left( |Z_{1}|^{\alpha} - \max_{j=2,\dots,k+1} \left| Z_{j}^{2} \prod_{i=2}^{j} A_{i} \right|^{\alpha/2} \right)_{+}}{E|Z_{1}|^{\alpha}}\n
   </math>
   (43)
  </p>
  <p block-type="Text">
   The above expression can be evaluated by Monte-Carlo simulation, see, for example, [25] for the ARCH(1) case with standard normal noise
   <math display="inline">
    Z_t
   </math>
   ; see [18], Section 8.1, where one can also find some advice as to how the extremal index of a stationary sequence can be estimated from data.
  </p>
  <p block-type="Text">
   The situation is markedly different for SV processes. For the SV process with either light- or heavy-tailed noise, one can show that
   <math display="inline">
    D'
   </math>
   is satisfied and hence the extremal index is always 1 (see
   <math display="inline">
    [3]
   </math>
   for the light-tailed case and [10] for the heavy-tailed case). Hence, although both GARCH and SV models exhibit stochastic clustering, only the GARCH process displays extremal clustering.
  </p>
  <h3>
   Convergence of Point Processes
  </h3>
  <p block-type="Text" class="has-continuation">
   The theory of point processes plays a central role in extreme value theory and in combination with regular variation can be a powerful tool for establishing limiting behavior of other statistics beyond extreme order statistics. As in the previous section, suppose that
   <math display="inline">
    (\hat{X}_t)
   </math>
   is an i.i.d. sequence of nonnegative random variables with common distribution
   <math display="inline">
    F
   </math>
   that has
  </p>
  <p block-type="TextInlineMath">
   regularly varying tails with index
   <math display="inline">
    -\alpha
   </math>
   . Choosing the sequence
   <math display="inline">
    a_n
   </math>
   satisfying
   <math display="inline">
    n(1 - F(a_n)) \rightarrow 1
   </math>
   , we have
  </p>
  <p block-type="Equation">
   <math display="block">
    nP(\hat{X}_1 &gt; a_n x) \to x^{-\alpha} \tag{44}
   </math>
  </p>
  <p block-type="TextInlineMath">
   as
   <math display="inline">
    n \to \infty
   </math>
   . Now equation (44) can be strengthened to the statement
  </p>
  <p block-type="Equation">
   <math display="block">
    n P(a_n^{-1}\hat{X}_1 \in B) \to \nu(B) \tag{45}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for all suitably chosen Borel sets
   <math display="inline">
    B
   </math>
   , where the measure
   <math display="inline">
    \nu
   </math>
   is defined by its value on intervals of the form
   <math display="inline">
    (a, b]
   </math>
   with
   <math display="inline">
    a &gt; 0
   </math>
   as
  </p>
  <p block-type="Equation">
   <math display="block">
    \nu(a,b] = a^{-\alpha} - b^{-\alpha} \tag{46}
   </math>
  </p>
  <p block-type="TextInlineMath">
   The convergence in equation
   <math display="inline">
    (46)
   </math>
   can be connected with the convergence in the distribution of a sequence of point processes. For a bounded Borel set B in
   <math display="inline">
    E =
   </math>
   <math display="inline">
    [0,\infty] \setminus \{0\}
   </math>
   , define the sequence of point processes
   <math display="inline">
    (\hat{N}_n)
   </math>
   by
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{N}_n(B) = \#\left\{ a_n^{-1} \hat{X}_j \in B \ , \ j = 1, 2, \dots, n \right\} \tag{47}
   </math>
  </p>
  <p block-type="TextInlineMath">
   If B is the interval
   <math display="inline">
    (a, b]
   </math>
   with
   <math display="inline">
    0 &lt; a &lt; b \leq \infty
   </math>
   , then since the
   <math display="inline">
    \hat{X}_i
   </math>
   are i.i.d.,
   <math display="inline">
    \hat{N}_n(B)
   </math>
   has a binomial distribution with number of trials
   <math display="inline">
    n
   </math>
   and probability of success
  </p>
  <p block-type="Equation">
   <math display="block">
    p_n = P(a_n^{-1}\hat{X}_1 \in (a, b]) \tag{48}
   </math>
  </p>
  <p block-type="TextInlineMath">
   It then follows from equation (46) that
   <math display="inline">
    \hat{N}_n(B)
   </math>
   converges in distribution to a Poisson random variable
   <math display="inline">
    N(B)
   </math>
   with mean
   <math display="inline">
    \nu(B)
   </math>
   . In fact, we have the stronger point process convergence:
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{N}_n \stackrel{d}{\to} N \tag{49}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    N
   </math>
   is a Poisson process on
   <math display="inline">
    E
   </math>
   with mean measure
   <math display="inline">
    v(dx)
   </math>
   and
   <math display="inline">
    \stackrel{d}{\rightarrow}
   </math>
   denotes convergence in distribution of point processes. For our purposes,
   <math display="inline">
    \stackrel{d}{\rightarrow}
   </math>
   for point processes means that for any collection of
   <i>
    bounded
   </i>
   &lt;sup&gt;b&lt;/sup&gt; Borel sets
   <math display="inline">
    B_1, \ldots, B_k
   </math>
   for which
   <math display="inline">
    P(N(\partial B_i) &gt; 0) =
   </math>
   <math display="inline">
    0, j = 1, ..., k
   </math>
   , we have
  </p>
  <p block-type="Equation">
   <math display="block">
    (\hat{N}_n(B_1), \dots, \hat{N}_n(B_k)) \xrightarrow{d} (N(B_1), \dots, N(B_k))
   </math>
   (50)
  </p>
  <p block-type="TextInlineMath">
   on
   <math display="inline">
    \mathbb{R}^k
   </math>
   [18, 29, 36].
  </p>
  <p block-type="TextInlineMath">
   As an application of equation (49), define
   <math display="inline">
    \hat{M}_{n,k}
   </math>
   to be the
   <i>
    k
   </i>
   th largest among
   <math display="inline">
    \hat{X}_1, \ldots, \hat{X}_n
   </math>
   . For
   <math display="inline">
    y \leq x
   </math>
   , the event
   <math display="inline">
    \{a_n^{-1}\hat{M}_n \le x, a_n^{-1}\hat{M}_{n,k} \le y\} = \{\hat{N}_n(x,\infty) = 0,
   </math>
   <math display="inline">
    \hat{N}_n(y, x] &lt; k-1
   </math>
   and hence
  </p>
  <p block-type="Equation">
   <math display="block">
    P(a_n^{-1}\hat{M}_n \le x, a_n^{-1}\hat{M}_{n,k} \le y)
   </math>
   <br/>
   =
   <math>
    P(\hat{N}_n(x,\infty) = 0, \hat{N}_n(y,x] \le k-1)
   </math>
   <br/>
   <math>
    \rightarrow P(N(x,\infty) = 0, N(y,x] \le k-1)
   </math>
   <br/>
   =
   <math>
    e^{-x^{-\alpha}} \sum_{j=0}^{k-1} (y^{-\alpha} - x^{-\alpha})^j / j!
   </math>
   (51)
  </p>
  <p block-type="TextInlineMath">
   As a second application of the limiting Poisson convergence in equation (49), the limiting Poisson process
   <math display="inline">
    \hat{N}
   </math>
   has points located at
   <math display="inline">
    \Gamma_k^{-1/\alpha}
   </math>
   , where
   <math display="inline">
    \Gamma_k =
   </math>
   <math display="inline">
    E_1 + \cdots + E_k
   </math>
   is the sum of k i.i.d. unit exponentially distributed random variables. Then if
   <math display="inline">
    \alpha &lt; 1
   </math>
   , the result is more complicated; if
   <math display="inline">
    \alpha &gt; 1
   </math>
   , we obtain the convergence of partial sums:
  </p>
  <p block-type="Equation">
   <math display="block">
    a_n^{-1} \sum_{t=1}^n \hat{X}_t \stackrel{d}{\to} \sum_{j=0}^\infty \Gamma_j^{-1/\alpha} \tag{52}
   </math>
  </p>
  <p block-type="Text">
   In other words, the sum of the points of the point process
   <math display="inline">
    N_n
   </math>
   converges in distribution to the sum of points in the limiting Poisson process.
  </p>
  <p block-type="TextInlineMath">
   For a stationary time series
   <math display="inline">
    (X_t)
   </math>
   with heavy tails that satisfy a suitable mixing condition, such as strong mixing, and the anticlustering condition
   <math display="inline">
    D'
   </math>
   , then the convergence in equation (49) remains valid, as well as the limit in equation (52), at least for positive random variables. For example, this is the case for SV processes. If the condition
   <math display="inline">
    D'
   </math>
   is replaced by the assumption that all finite-dimensional random variables are regularly varying, then there is a point convergence result for
   <math display="inline">
    N_n
   </math>
   corresponding to
   <math display="inline">
    (X_t)
   </math>
   . However, the limit point process in this case is more difficult to describe. Essentially, the point process has anchors located at the Poisson points
   <math display="inline">
    \Gamma_i^{-1/\alpha}
   </math>
   . At each of these anchor locations, there is an independent cluster of points that can be described by the distribution of the angular measures in the regular variation condition [8, 9]. These conditions can then be applied to functions of the data, such as lagged products, to establish the convergence in distribution of the sample autocovariance function. This is the subject of the following section.
  </p>
  <h1>
   The Behavior of the Sample Autocovariance and Autocorrelation Functions
  </h1>
  <p block-type="TextInlineMath">
   The ACF is one of the principal tools used in classical time series modeling. For a stationary Gaussian process, the dependence structure of the process is completely determined by the ACF. The ACF also conveys important dependence information for linear process. To some extent, the dependence governed by a linear filter can be fully recovered from the ACF. For the time series consisting of financial returns, the data are uncorrelated, so the value of the ACF is substantially diminished. Nevertheless, the ACF of other functions of the process such as the squares and absolute values can still convey useful information about the nature of the nonlinearity in the time series. For example, slow decay of the ACF of the squares is consistent with the volatility clustering present in the data. For a stationary time series
   <math display="inline">
    (X_t)
   </math>
   , the ACVF and ACF are defined as
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_X(h) = \text{cov}(X_0, X_h) \quad \text{and}
   </math>
   <math display="block">
    \rho_X(h) = \text{corr}(X_0, X_h) = \frac{\gamma_X(h)}{\gamma_X(0)}, \quad h \ge 0 \tag{53}
   </math>
  </p>
  <p block-type="TextInlineMath">
   respectively. Now for observations
   <math display="inline">
    X_1, \ldots, X_n
   </math>
   from the stationary time series, the ACVF and ACF are estimated by their sample counterparts, namely, by
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\gamma}_X(h) = \frac{1}{n} \sum_{t=1}^{n-h} (X_t - \overline{X}_n) \left( X_{t+h} - \overline{X}_n \right) \quad (54)
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\rho}_X(h) = \frac{\hat{\gamma}_X(h)}{\hat{\gamma}_X(0)} = \frac{\sum_{t=1}^{n-h} (X_t - \overline{X}_n)(X_{t+h} - \overline{X}_n)}{\sum_{t=1}^{n} (X_t - \overline{X}_n)^2} \tag{55}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \overline{X}_n = n^{-1} \sum_{t=1}^n X_t
   </math>
   is the sample mean.
  </p>
  <p block-type="Text">
   Even though the sample ACVF is an average of random variables, its asymptotic behavior is determined by the extremes values, at least in the case of heavy-tailed data. Regular variation and point process theory are the two ingredients that play a key role in deriving limit theory for the sample ACVF and ACF. In particular, one applies the point process techniques alluded to in the previous section to the
  </p>
  <p block-type="Text">
   stationary process consisting of products
   <math display="inline">
    (X_t X_{t+h})
   </math>
   . The first such results were established by Davis and Resnick
   <math display="inline">
    [14-16]
   </math>
   in a linear process setting. Extensions by Davis and Hsing [8] and Davis and Mikosch [9] allowed one to consider more general time series models beyond those linear. The main idea is to consider a point process
   <math display="inline">
    N_n
   </math>
   based on products of the form
   <math display="inline">
    X_t X_{t+h}/a_n^2
   </math>
   . After establishing convergence of this point process, in many cases one can apply the continuous mapping theorem to show that the sum of the points that comprise
   <math display="inline">
    N_n
   </math>
   converges in distribution to the sum of the points that make up the limiting point process. Although the basic idea for establishing these results is rather straightforward, the details are slightly complex. These ideas have been applied to the case of GARCH processes in [1] and to SV processes in [10], which are summarized below.
  </p>
  <h3>
   The GARCH Case
  </h3>
  <p block-type="Text">
   The scaling in the limiting distribution for the sample ACF depends on the index of regular variation
   <math display="inline">
    \alpha
   </math>
   specified in Theorem 1. We summarize the results for the various cases of
   <math display="inline">
    \alpha
   </math>
   .
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     1. If
     <math display="inline">
      \alpha \in (0, 2)
     </math>
     , then
     <math display="inline">
      \hat{\rho}_X(h)
     </math>
     and
     <math display="inline">
      \hat{\rho}_{|X|}(h)
     </math>
     have nondegenerate limit distributions. The same statement holds for
     <math display="inline">
      \hat{\rho}_{X^2}(h)
     </math>
     when
     <math display="inline">
      \alpha \in (0, 4)
     </math>
     .
    </li>
    <li block-type="ListItem">
     2. If
     <math display="inline">
      \alpha \in (2, 4)
     </math>
     , then both
     <math display="inline">
      \hat{\rho}_X(h)
     </math>
     ,
     <math display="inline">
      \hat{\rho}_{|X|}(h)
     </math>
     converge in probability to their deterministic counterparts
     <math display="inline">
      \rho_X(h)
     </math>
     ,
     <math display="inline">
      \rho_{|X|}(h)
     </math>
     , respectively, at the rate
     <math display="inline">
      n^{1-2/\alpha}
     </math>
     and the limit distribution is a complex function of non-Gaussian stable random variables.
    </li>
    <li block-type="ListItem">
     3. If
     <math display="inline">
      \alpha \in (4, 8)
     </math>
     , then
    </li>
   </ul>
  </p>
  <p block-type="Equation">
   <math display="block">
    n^{1-4/(2\alpha)}(\hat{\rho}_{X^2}(h) - \rho_{X^2}(h)) \xrightarrow{d} S_{\alpha/2}(h) \quad (56)
   </math>
  </p>
  <p block-type="Text">
   where the random variable
   <math display="inline">
    S_{\alpha/2}(h)
   </math>
   is a function of infinite variance stable random variables.
  </p>
  <p block-type="Text">
   4. If
   <math display="inline">
    \alpha &gt; 4
   </math>
   , then the one can apply standard central limit theorems for stationary mixing sequences to establish a limiting normal distribution [17, 26]. In particular,
   <math display="inline">
    (\hat{\rho}_X(h))
   </math>
   and
   <math display="inline">
    (\hat{\rho}_{|X|}(h))
   </math>
   have Gaussian limits at
   <math display="inline">
    \sqrt{n}
   </math>
   -rates. The corresponding result holds for
   <math display="inline">
    (X_t^2)
   </math>
   when
   <math display="inline">
    \alpha &gt; 8
   </math>
   .
  </p>
  <p block-type="Text">
   These results show that the limit theory for the sample ACF of a GARCH process is rather complicated when the tails are heavy. In fact, there is considerable empirical evidence based on extreme
  </p>
  <p block-type="Text">
   value statistics as described in the second section, indicating that log-return series might not have a finite fourth or fifth moment&lt;sup&gt;c&lt;/sup&gt; and then the limit results above would show that the usual confidence bands for the sample ACF based on the central limit theorem and the corresponding
   <math display="inline">
    \sqrt{n}
   </math>
   -rates are far too optimistic in this case.
  </p>
  <h2>
   <b>
    The Stochastic Volatility Case
   </b>
  </h2>
  <p block-type="Text">
   For a more direct comparison with the GARCH process, we choose a distribution for the noise process that matches the power law tail of the GARCH with index
   <math display="inline">
    \alpha
   </math>
   . Then
  </p>
  <p block-type="Equation">
   <math display="block">
    \left(\frac{n}{\ln n}\right)^{1/\alpha} \hat{\rho}_X(h)
   </math>
   and
   <math>
    \left(\frac{n}{\ln n}\right)^{1/(2\alpha)} \hat{\rho}_{X^2}(h)
   </math>
   (57)
  </p>
  <p block-type="TextInlineMath">
   converge in distribution for
   <math display="inline">
    \alpha \in (0, 2)
   </math>
   and
   <math display="inline">
    \alpha \in (0, 4)
   </math>
   , respectively. This illustrates the excellent large sample behavior of the sample ACF for SV models even if
   <math display="inline">
    \rho_X
   </math>
   and
   <math display="inline">
    \rho_{X^2}
   </math>
   are not defined [11, 13]. Thus, even if
   <math display="inline">
    \text{var}(Z_t) = \infty
   </math>
   or
   <math display="inline">
    EZ_t^4 = \infty
   </math>
   , the estimates
   <math display="inline">
    \hat{\rho}_X(h)
   </math>
   and
   <math display="inline">
    \hat{\rho}_{X^2}(h)
   </math>
   , respectively, converge to zero at a rapid rate. This is in marked contrast with the situation for GARCH processes, where under similar conditions on the marginal distribution, the respective sample ACFs converge in distribution to random variables without any scaling.
  </p>
  <h2>
   <b>
    End Notes
   </b>
  </h2>
  <p block-type="Text">
   &lt;sup&gt;a.&lt;/sup&gt;Basrak
   <i>
    et al.
   </i>
   [1] proved this result under the condition that
   <math display="inline">
    \alpha/2
   </math>
   is not an even integer. Boman and Lindskog [5] removed this condition.
  </p>
  <p block-type="Text">
   &lt;sup&gt;b.&lt;/sup&gt;Here bounded means bounded away from zero.
  </p>
  <p block-type="Text">
   &lt;sup&gt;c.&lt;/sup&gt;See, for example, [18], Chapter 6, and [33].
  </p>
  <h2>
   References
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Basrak, B., Davis, R.A. &amp; Mikosch, T. (2002). Regular variation of GARCH processes. Stochastic Processes and Their Applications 99, 95–116.
    </li>
    <li block-type="ListItem">
     Basrak, B., Davis, R.A. &amp; Mikosch, T. (2002). A [2] characterization of multivariate regular variation, The Annals of Applied Probability 12, 908-920.
    </li>
    <li block-type="ListItem">
     [3] Breidt, F.J. &amp; Davis, R.A. (1998). Extremes of stochastic volatility models, The Annals of Applied Probability 8, 664-675.
    </li>
    <li block-type="ListItem">
     [4] Bougerol, P. &amp; Picard, N. (1992). Stationarity of GARCH processes and of some nonnegative time series, Journal of Econometrics 52, 115-127.
    </li>
   </ul>
  </p>
 </body>
</html>
