<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Autoregressive Moving
   </b>
   Average (ARMA) Processes
  </h1>
  <p block-type="TextInlineMath">
   Autoregressive moving average (ARMA) processes
   <math display="inline">
    \{Y_t, t = 0, \pm 1, \ldots\}
   </math>
   have played a key role as stationary models for time series observed at regularly spaced times. They constitute a very convenient parametric family, capable of exhibiting a broad range of dependence structures and marginal distributions. Well-established prediction, model-selection, and estimation techniques are also available and the asymptotic properties of the customary estimators are well understood. Continuous-time ARMA (denoted CARMA) processes play an analogous role in continuous time. Although continuous-time data is very rarely available, it is often the case that observations of a time series at discrete times can be fruitfully regarded as sampled values of an underlying continuous-time process. The continuous-time framework is very convenient for the modeling of highfrequency data and for dealing with irregularly spaced discrete-time data. Many of the problems of mathematical finance are formulated most conveniently in a continuous time setting, and in constructing more complex continuous-time models for financial data, CARMA processes also play a role. Although ARMA and CARMA processes with infinite second moments can be defined (see
   <math display="inline">
    [3]
   </math>
   and
   <math display="inline">
    [7]
   </math>
   ) we restrict our attention here to second-order processes, for which these moments are finite.
  </p>
  <h4>
   <b>
    ARMA Processes
   </b>
  </h4>
  <p block-type="TextInlineMath">
   The process
   <math display="inline">
    \{Y_n, n = 0, \pm 1, \ldots\}
   </math>
   is said to be an
   <math display="inline">
    ARMA(p, q)
   </math>
   process with (real-valued) parameters
   <math display="inline">
    \{\phi_1,\ldots,\phi_p;\theta_1,\ldots,\theta_q;\sigma\}
   </math>
   if it is a stationary solution of the equations
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n - \phi_1 Y_{n-1} - \dots - \phi_p Y_{n-p}
   </math>
   <br/>
   =
   <math>
    \sigma (Z_n + \theta_1 Z_{n-1} + \dots + \theta_q Z_{n-q})
   </math>
   (1)
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \sigma &gt; 0
   </math>
   ,
   <math display="inline">
    \phi_p \neq 0
   </math>
   ,
   <math display="inline">
    \theta_q \neq 0
   </math>
   ,
   <math display="inline">
    \{Z_n\}
   </math>
   is standard white noise, i.e., a sequence of uncorrelated random variables with mean 0 and variance 1 (written
   <math display="inline">
    \{Z_n\}
   </math>
   ~
   <math display="inline">
    \text{WN}(0, 1)
   </math>
   ) and the polynomials
   <math display="inline">
    \phi(z) := (1 - \phi_1 z -
   </math>
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    \cdots - \phi_{p}z^{p}
   </math>
   and
   <math display="inline">
    \theta(z) := (1 + \theta_{1}z + \cdots + \theta_{q}z^{q})
   </math>
   have no common zeros. Stationarity here means
   <math display="inline">
    EY_t
   </math>
   is independent of t and
   <math display="inline">
    E(Y_{t+h}Y_t)
   </math>
   is independent of t for all
   <math display="inline">
    h
   </math>
   . It can be shown (see [5]) that there is a unique stationary solution of equation (1) for
   <math display="inline">
    \{Y_n\}
   </math>
   in terms of the white noise sequence
   <math display="inline">
    \{Z_n\}
   </math>
   if and only if
   <math display="inline">
    \phi(z)
   </math>
   is nonzero for all complex z such that
   <math display="inline">
    |z| = 1
   </math>
   . The solution is
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n = \sum_{j = -\infty}^{\infty} \sigma \psi_j Z_{n-j} \tag{2}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \psi(z) := \sum_{j=-\infty}^{\infty} \psi_j z^j
   </math>
   is the Laurent expansion of
   <math display="inline">
    \theta(z)/\phi(z)
   </math>
   , valid in an annulus of the form
   <math display="inline">
    |z-1| &lt; \delta
   </math>
   . If, in addition, the reciprocals
   <math display="inline">
    \xi_1, \ldots, \xi_n
   </math>
   of the zeros of the polynomial
   <math display="inline">
    \phi(z)
   </math>
   satisfy
  </p>
  <p block-type="Equation">
   <math display="block">
    |\xi_r| &lt; 1, \quad r = 1, \dots, p
   </math>
   (3)
  </p>
  <p block-type="TextInlineMath">
   then
   <math display="inline">
    \psi_j = 0
   </math>
   for
   <math display="inline">
    j &lt; 0
   </math>
   and
   <math display="inline">
    \{Y_n\}
   </math>
   is said to be a
   <i>
    causal function of
   </i>
   <math display="inline">
    \{Z_n\}
   </math>
   . Since, from a second-order point of view, i.e., taking into account the first- and second-order moments of
   <math display="inline">
    \{Y_n\}
   </math>
   only, every noncausal
   <math display="inline">
    ARMA(p, q)
   </math>
   process can be written as a causal function of a different standard white noise process, attention is usually restricted to processes defined by equation
   <math display="inline">
    (1)
   </math>
   with condition
   <math display="inline">
    (3)
   </math>
   satisfied. We can then write
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n = \sum_{j = -\infty}^n g_{n-j} Z_j \tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    \psi(z) := \sum_{j=0}^{\infty} \psi_j z^j = \theta(z) / \phi(z), \quad |z| \le 1,
   </math>
   where and
  </p>
  <p block-type="Equation">
   <math display="block">
    g_j = \sigma \psi_j, \ \ j = 0, 1, 2, \dots
   </math>
   (5)
  </p>
  <p block-type="TextInlineMath">
   The sequence
   <math display="inline">
    \{g_i\}
   </math>
   is called the
   <i>
    kernel
   </i>
   or
   <i>
    absolutely
   </i>
   summable linear filter relating the ARMA process
   <math display="inline">
    \{Y_n\}
   </math>
   to
   <math display="inline">
    \{Z_n\}
   </math>
   . In all that follows, we shall assume causality so that the representation (4) holds with
   <math display="inline">
    g_i
   </math>
   given by equation
   <math display="inline">
    (5)
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   If we suppose, in addition, that the sequence
   <math display="inline">
    \{Z_t\}
   </math>
   is an independent and identically distributed (iid) sequence, then
   <math display="inline">
    \{Y_n\}
   </math>
   is said to be a
   <i>
    strict
   </i>
   ARMA process. In this case,
   <math display="inline">
    \{Y_n\}
   </math>
   is a strictly stationary process, i.e., for each positive integer
   <math display="inline">
    n
   </math>
   and for each
   <math display="inline">
    (t_1, \ldots, t_n)
   </math>
   , the joint distribution of
   <math display="inline">
    (Y_{t_1+h}, \ldots, Y_{t_n+h})
   </math>
   is independent of
   <math display="inline">
    h
   </math>
   .
  </p>
  <p block-type="Text">
   The ARMA process defined by equation
   <math display="inline">
    (1)
   </math>
   has an equivalent and illuminating state-space represen
   <i>
    tation
   </i>
   , specified by the equations,
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n = \sigma \boldsymbol{\theta}' \mathbf{X}_n \tag{6}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{X}_{n+1} - \Phi \mathbf{X}_n = \mathbf{e} Z_{n+1} \tag{7}
   </math>
  </p>
  <p block-type="Text">
   where
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi = \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\ \phi_{r} &amp; \phi_{r-1} &amp; \phi_{r-2} &amp; \cdots &amp; \phi_{1} \end{bmatrix},\n
   </math>
   <math display="block">
    \n\mathbf{e} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{bmatrix}, \quad \boldsymbol{\theta} = \begin{bmatrix} \theta_{r-1} \\ \theta_{r-2} \\ \vdots \\ \theta_{1} \\ \theta_{0} \end{bmatrix}
   </math>
  </p>
  <p block-type="TextInlineMath">
   <math display="inline">
    r := \max(p, q + 1), \ \theta_0 := 1, \ \theta_j := 0 \ \text{for} \ j &gt; q \ \text{and}
   </math>
   <math display="inline">
    \phi_j := 0
   </math>
   for
   <math display="inline">
    j &gt; p
   </math>
   . The causality condition (3) implies that the eigenvalues of the matrix
   <math display="inline">
    \Phi
   </math>
   are all less than 1 in absolute value so that the state-vector
   <math display="inline">
    \mathbf{X}_t
   </math>
   has the representation,
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{X}_n = \sum_{j = -\infty}^n \Phi^{n-j} \mathbf{e} \ Z_j \tag{8}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Equations
   <math display="inline">
    (6)
   </math>
   and
   <math display="inline">
    (8)
   </math>
   show that the kernel sequence
   <math display="inline">
    \{g_i\}
   </math>
   in the representation (4) can also be written as
  </p>
  <p block-type="Equation">
   <math display="block">
    g_j = \sigma \boldsymbol{\theta}' \Phi^j \ \mathbf{e}, \ \ j = 0, 1, 2, \dots \tag{9}
   </math>
  </p>
  <p block-type="TextInlineMath">
   From equations (4) and (9) we easily find that
   <math display="inline">
    EY_n =
   </math>
   <math display="inline">
    0
   </math>
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_Y(h) := \text{Cov}(Y_{t+h}, Y_t) = \sigma^2 \boldsymbol{\theta}' \ \Phi^{|h|} \Xi \ \boldsymbol{\theta} \qquad (10)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \Xi = E[\mathbf{X}_t \mathbf{X}_t'] = \sum_{j=0}^{\infty} \Phi^j \mathbf{e} \mathbf{e}' \Phi^{j}
   </math>
   . The ARMA process, as we have defined it, has mean 0. We can, of course, easily extend the definition by saying that
   <math display="inline">
    \{U_n\}
   </math>
   is an ARMA process with mean
   <math display="inline">
    \mu
   </math>
   if
   <math display="inline">
    \{U_n - \mu\}
   </math>
   is a zero-mean ARMA process.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Remark 1
   </b>
   In the case when
   <math display="inline">
    p &gt; q
   </math>
   and the reciprocals,
   <math display="inline">
    \xi_1, \ldots, \xi_p
   </math>
   , of the zeros of the polynomial
   <math display="inline">
    \phi(z)
   </math>
   are distinct, it follows from equations (4) and (9) and the spectral representation of the matrix
   <math display="inline">
    \Phi
   </math>
   , that
   <math display="inline">
    \{Y_n\}
   </math>
   has a
   <i>
    canonical representation
   </i>
   as a linear combination of (possibly complex-valued)
   <math display="inline">
    ARMA(1,0)
   </math>
   processes,
   <math display="inline">
    \{Y_{r,n}\}\.
   </math>
   Thus
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n = \sum_{r=1}^p \beta_r Y_{r,n} \tag{11}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    Y_{r,n} = \sum_{j=-\infty}^{n} \xi^{n-j} Z_j
   </math>
   and
   <math display="inline">
    \beta_r = -\sigma \xi_r \theta(\xi_r^{-1})/
   </math>
   <math display="inline">
    \phi'(\xi_r^{-1})
   </math>
   . Notice that the driving white noise sequence is the same for each of the component processes
   <math display="inline">
    \{Y_{r,n}\}\
   </math>
   , so they are not independent. Corresponding to the canonical decomposition
   <math display="inline">
    (11)
   </math>
   , there is an analogous representation of the autocovariance function, namely,
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_Y(h) = \sum_{j=1}^p \gamma_j \xi_j^{|h|},\tag{12}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \gamma_j = -\sigma^2 \xi_j \theta(\xi_j) \theta(\xi_j^{-1})/[\phi(\xi_j)\phi'(\xi_j^{-1})]
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   Remark 2 Parallel to the time domain representation (4) of
   <math display="inline">
    \{Y_n\}
   </math>
   , there is a
   <i>
    spectral
   </i>
   or
   <i>
    frequency domain
   </i>
   representation,
  </p>
  <p block-type="Equation">
   <math display="block">
    Y_n = \int_{-\pi}^{\pi} \sigma \frac{\theta(e^{-i\omega})}{\phi(e^{-i\omega})} e^{i\omega n} \, \mathrm{d}Z(\omega) \tag{13}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \{Z(\omega), -\pi \leq \omega \leq \pi\}
   </math>
   is an orthogonal increment process with mean 0 and
   <math display="inline">
    E|dZ(\omega)|^2 = d\omega/
   </math>
   <math display="inline">
    (2\pi)
   </math>
   , and the autocovariance function has the corresponding spectral representation
   <math display="inline">
    \gamma_Y(h) = \int_{-\pi}^{\pi} f(\omega)
   </math>
   <math display="inline">
    e^{ih\omega} d\omega
   </math>
   , where the spectral density function f is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    f(\omega) = \frac{\sigma^2}{2\pi} \left| \frac{\theta(\mathbf{e}^{-i\omega})}{\phi(\mathbf{e}^{-i\omega})} \right|^2, \ \omega \in [-\pi, \pi] \tag{14}
   </math>
  </p>
  <p block-type="TextInlineMath">
   <b>
    Remark 3
   </b>
   So far we have considered only secondorder properties of
   <math display="inline">
    \{Y_n\}
   </math>
   . If we make the stronger assumption that the driving white noise sequence is iid with log characteristic function, log
   <math display="inline">
    E \exp(i\omega Z_n)
   </math>
   <math display="inline">
    =\xi(\omega)
   </math>
   , and if
   <math display="inline">
    t_1,\ldots,t_n
   </math>
   are integers such that
   <math display="inline">
    t_1 &lt;
   </math>
   <math display="inline">
    t_2 &lt; \cdots &lt; t_n
   </math>
   and
   <math display="inline">
    \omega_1, \ldots, \omega_n \in \mathbb{R}
   </math>
   , then we can use the representation
   <math display="inline">
    (4)
   </math>
   to derive the log of the joint characteristic function,
  </p>
  <p block-type="Equation">
   <math display="block">
    \log E \exp(i \sum_{r=1}^{n} \omega_r Y_{t_r})
   </math>
   <br/>
   =
   <math display="block">
    \sum_{k=0}^{n-1} \sum_{j=t_{n-k-1}+1}^{t_{n-k}} \xi \left( \sum_{r=0}^{k} \omega_{n-r} g_{t_{n-r}-j} \right) \quad (15)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    t_0 := -\infty
   </math>
   .
  </p>
  <p block-type="Text">
   2
  </p>
  <h4>
   <b>
    CARMA Processes
   </b>
  </h4>
  <p block-type="Text">
   A natural analog, in continuous time, of the stochastic difference equation (1) is the stochastic differential equation,
  </p>
  <p block-type="Equation">
   <math display="block">
    a(D)Y(t) = \sigma b(D)DL(t) \tag{16}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \sigma
   </math>
   is a strictly positive scale parameter, D denotes differentiation with respect to t,
   <math display="inline">
    a(z) := z^p +
   </math>
   <math display="inline">
    a_1z^{p-1} + \ldots + a_p, \ b(z) := b_0 + b_1z + \ldots + b_{p-1}
   </math>
   <math display="inline">
    z^{p-1}
   </math>
   , and the coefficients
   <math display="inline">
    b_j
   </math>
   satisfy
   <math display="inline">
    b_q = 1
   </math>
   and
   <math display="inline">
    b_j =
   </math>
   0 for
   <math display="inline">
    q &lt; j &lt; p
   </math>
   . To avoid trivial and easily eliminated complications, we shall assume that
   <math display="inline">
    a(z)
   </math>
   and
   <math display="inline">
    b(z)
   </math>
   have no common factors.
  </p>
  <p block-type="TextInlineMath">
   The continuous-time analogs of the driving noise terms
   <math display="inline">
    Z_n
   </math>
   in equation (1) are the increments of the process
   <math display="inline">
    L
   </math>
   . We shall assume that
   <math display="inline">
    L
   </math>
   is a Lévy process on
   <math display="inline">
    (-\infty, \infty)
   </math>
   , i.e., a process with homogeneous independent increments, continuous in probability, with cadlag sample-paths and
   <math display="inline">
    L(0) = 0
   </math>
   . We shall also restrict attention to second-order Lévy processes, i.e., those satisfying the condition
   <math display="inline">
    E(L(1)^2) &lt; \infty
   </math>
   , and suppose, without further loss of generality, that
   <math display="inline">
    \text{Var}L(1) = 1
   </math>
   and
   <math display="inline">
    EL(t) = \mu t
   </math>
   for some
   <math display="inline">
    \mu \in \mathbb{R}
   </math>
   . The increments of
   <math display="inline">
    L
   </math>
   on disjoint intervals of equal length are then iid random variables with finite variance and some infinitely divisible distribution which could, for example, be Gaussian, gamma, compound Poisson, inverse Gaussian, or one of many other possibilities.
  </p>
  <p block-type="Text">
   Since the derivatives on the right of equation
   <math display="inline">
    (16)
   </math>
   do not exist in the usual sense, we write the equation in state-space form (cf. equations
   <math display="inline">
    (6)
   </math>
   and
   <math display="inline">
    (7)
   </math>
   ),
  </p>
  <p block-type="Equation">
   <math display="block">
    Y(t) = \sigma \mathbf{b}' \mathbf{X}(t) \tag{17}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    d\mathbf{X}(t) - \mathbf{A}\mathbf{X}(t) dt = \mathbf{e} dL(t) \tag{18}
   </math>
  </p>
  <p block-type="Text">
   where
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{A} = \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\ -a_p &amp; -a_{p-1} &amp; -a_{p-2} &amp; \cdots &amp; -a_1 \end{bmatrix},
   </math>
   <math display="block">
    \mathbf{e} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} b_0 \\ b_1 \\ \vdots \\ b_{p-2} \\ b_{p-1} \end{bmatrix}
   </math>
  </p>
  <p block-type="Text">
   with solution satisfying
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{X}(t) = \mathbf{e}^{A(t-s)} \mathbf{X}(s)
   </math>
   <math display="block">
    + \int_{s}^{t} \mathbf{e}^{A(t-u)} \mathbf{e} \ dL(u), \text{ for all } t &gt; s \quad (19)
   </math>
  </p>
  <p block-type="TextInlineMath">
   In the Gaussian case
   <math display="inline">
    L
   </math>
   is Brownian motion, equation
   <math display="inline">
    (18)
   </math>
   is interpreted as an Ito equation and the integral in equation
   <math display="inline">
    (19)
   </math>
   is an Ito integral. In the general case, the integral in equation
   <math display="inline">
    (19)
   </math>
   is defined as in [12].
  </p>
  <p block-type="TextInlineMath">
   If we restrict attention to causal solutions, i.e., if we make the assumption that
   <math display="inline">
    \mathbf{X}(s)
   </math>
   is independent of
   <math display="inline">
    \{L(t) - L(s), t &gt; s\}
   </math>
   for every s, then necessary and sufficient conditions for the existence of a strictly stationary process
   <math display="inline">
    \mathbf{X}
   </math>
   satisfying equation (19) (see [6] are
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathcal{R}e(\lambda_r) &lt; 0, \quad r = 1, \dots, p \tag{20}
   </math>
  </p>
  <p block-type="Text">
   and, under these conditions, the stationary solution must satisfy
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{X}(t) \text{ is distributed as } \int_0^\infty e^{Au} \mathbf{e} \, dL(u) \text{ for all } t \tag{21}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \lambda_1, \ldots, \lambda_p
   </math>
   are the eigenvalues of A (which are also the zeros of the autoregressive polynomial
   <math display="inline">
    a(z)
   </math>
   ). Condition (21) specifies the stationary marginal distribution of
   <math display="inline">
    \mathbf{X}(t)
   </math>
   and condition (20) is the continuous-time analog of the causality condition (3).
  </p>
  <p block-type="Text">
   If we assume that the conditions
   <math display="inline">
    (20)
   </math>
   and
   <math display="inline">
    (21)
   </math>
   hold, and let
   <math display="inline">
    s \to -\infty
   </math>
   in equation (19) we find that
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{X}(t) = \int_{-\infty}^{t} \mathbf{e}^{A(t-u)} \mathbf{e} \, \mathrm{d}L(u) \tag{22}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Conversely, if
   <math display="inline">
    \mathbf{X}(t)
   </math>
   is defined by equation (22) then
   <math display="inline">
    \mathbf{X}
   </math>
   is a strictly stationary causal process satisfying equation (19). Consequently,
   <math display="inline">
    \{X(t)\}\
   </math>
   as defined in equation
   <math display="inline">
    (22)
   </math>
   is the unique, strictly stationary causal function of
   <math display="inline">
    L
   </math>
   satisfying equation (19).
  </p>
  <p block-type="TextInlineMath">
   We now define the strictly stationary causal CARMA process by equation (17) with
   <math display="inline">
    X
   </math>
   given by equation
   <math display="inline">
    (22)
   </math>
   . Thus,
  </p>
  <p block-type="Equation">
   <math display="block">
    Y(t) = \int_{-\infty}^{t} \sigma \mathbf{b}' e^{A(t-u)} \mathbf{e} \, \mathrm{d}L(u) \tag{23}
   </math>
  </p>
  <p block-type="TextInlineMath">
   This is the continuous-time analog of equation
   <math display="inline">
    (4)
   </math>
   with the discrete-time kernel,
   <math display="inline">
    \{g_j = \sigma \boldsymbol{\theta}' \Phi^j \mathbf{e}, j =
   </math>
   <math display="inline">
    0, 1, 2, \ldots
   </math>
   , replaced by the continuous-time kernel,
  </p>
  <p block-type="Equation">
   <math display="block">
    g(t) = \sigma \mathbf{b}' e^{At} \mathbf{e}, \ t &gt; 0 \tag{24}
   </math>
  </p>
  <p block-type="TextInlineMath">
   From equation (23), we find that
   <math display="inline">
    EY(t) = \sigma \mu b_0/a_p
   </math>
   (where
   <math display="inline">
    \mu = EL(1)
   </math>
   ) and
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma_Y(h) := \text{Cov}[Y(t+h), Y(t)] = \sigma^2 \mathbf{b}' e^{A|h|} \Sigma \mathbf{b} \quad (25)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \Sigma = E[\mathbf{X}(t)\mathbf{X}(t)'] = \int_0^\infty e^{Ay} \mathbf{e} \mathbf{e}' e^{A'y} \, \mathrm{d}y.
   </math>
  </p>
  <p block-type="TextInlineMath">
   <b>
    Remark 4
   </b>
   If the zeros,
   <math display="inline">
    \lambda_1, \ldots, \lambda_p
   </math>
   , of the polynomial
   <math display="inline">
    a(z)
   </math>
   are distinct, it follows from equation (23) and the spectral representation of the matrix
   <math display="inline">
    A
   </math>
   that
   <math display="inline">
    \{Y_n\}
   </math>
   has a
   <i>
    canonical representation
   </i>
   as a linear combination of (possibly complex-valued)
   <math display="inline">
    \text{CARMA}(1,0)
   </math>
   processes,
   <math display="inline">
    \{Y_r(t)\}\
   </math>
   . Thus,
  </p>
  <p block-type="Equation">
   <math display="block">
    Y(t) = \sum_{r=1}^{p} \beta_r Y_r(t) \tag{26}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    Y_r(t) = \int_{-\infty}^t e^{\lambda_r(t-u)} dL(u)
   </math>
   and
   <math display="inline">
    \beta_r = \sigma b(\lambda_r) /
   </math>
   <math display="inline">
    a'(\lambda_r)
   </math>
   . Notice that the driving process L is the same for each of the component processes
   <math display="inline">
    \{Y_r(t)\}\
   </math>
   , so they are not independent. Corresponding to the canonical decomposition
   <math display="inline">
    (26)
   </math>
   , there is an analogous representation of the autocovariance function, namely,
  </p>
  <p block-type="Equation">
   <math display="block">
    \gamma(h) = \sum_{r=1}^{p} \gamma_r e^{\lambda_r |h|} \tag{27}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \gamma_r = \sigma^2 b(\lambda_r) b(-\lambda_r) / [a(-\lambda_r) a'(\lambda_r)].
   </math>
  </p>
  <p block-type="TextInlineMath">
   <b>
    Remark 5
   </b>
   The mean-corrected process
   <math display="inline">
    \{Y^*(t) =
   </math>
   <math display="inline">
    Y(t) - \sigma \mu b_0/a_n
   </math>
   has a spectral representation
  </p>
  <p block-type="Equation">
   <math display="block">
    Y^*(t) = \int_{-\infty}^{\infty} \sigma \frac{b(i\omega)}{a(i\omega)} e^{i\omega t} \, \mathrm{d}Z(\omega) \tag{28}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \{Z(\omega), -\infty &lt; \omega &lt; \infty\}
   </math>
   is an orthogonal increment process with mean 0 and
   <math display="inline">
    E|dZ(\omega)|^2 = d\omega/
   </math>
   <math display="inline">
    (2\pi)
   </math>
   , and the autocovariance function of Y (and of
   <math display="inline">
    Y^*
   </math>
   ) has the corresponding spectral representation
   <math display="inline">
    \gamma_Y(h) = \int_{-\infty}^{\infty} f(\omega) e^{ih\omega} d\omega
   </math>
   , where the spectral density function
   <math display="inline">
    f
   </math>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    f(\omega) = \frac{\sigma^2}{2\pi} \left| \frac{b(i\omega)}{a(i\omega)} \right|^2, \ \omega \in (-\infty, \infty) \tag{29}
   </math>
  </p>
  <p block-type="Text">
   Gaussian processes with such rational spectral densities were discussed extensively in [9].
  </p>
  <p block-type="TextInlineMath">
   <b>
    Remark 6
   </b>
   If
   <math display="inline">
    \log E \exp(i\omega L(1)) = \xi(\omega)
   </math>
   , and if
   <math display="inline">
    t_1, \ldots, t_n
   </math>
   satisfy
   <math display="inline">
    t_1 &lt; t_2 &lt; \cdots &lt; t_n
   </math>
   and
   <math display="inline">
    \omega_1, \ldots, \omega_n \in
   </math>
   <math display="inline">
    \mathbf{R}
   </math>
   , then from the representation (23),
  </p>
  <p block-type="Equation">
   <math display="block">
    \log E \exp(i \sum_{r=1}^{n} \omega_r Y(t_r))
   </math>
   <br/>
   =
   <math display="block">
    \sum_{k=0}^{n-1} \int_{t_{n-k-1}}^{t_{n-k}} \xi \left( \sum_{r=0}^{k} \omega_{n-r} g(t_{n-r} - u) \right) du \ (30)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    t_0 := -\infty
   </math>
   .
  </p>
  <h4>
   <b>
    Prediction and Inference
   </b>
  </h4>
  <p block-type="TextInlineMath">
   From observations at discrete times,
   <math display="inline">
    t_1 &lt; t_2 &lt; \cdots &lt;
   </math>
   <math display="inline">
    t_{k-1}
   </math>
   , of either an ARMA or CARMA process
   <math display="inline">
    \{Y(t)\}
   </math>
   with specified parameter values, various techniques are available for computing the minimum meansquared error linear predictor,
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{Y}(t_k) = \alpha_{k,0} + \alpha_{k,1}Y(t_{k-1}) + \cdots + \alpha_{k,k-1}Y(t_1)
   </math>
   (31)
  </p>
  <p block-type="TextInlineMath">
   of
   <math display="inline">
    Y(t_k)
   </math>
   ,
   <math display="inline">
    t_k &gt; t_{k-1}
   </math>
   , and its mean-squared error,
   <math display="inline">
    v_k =
   </math>
   <math display="inline">
    E(Y(t_k) - \hat{Y}(t_k))^2
   </math>
   . For details see, for example [5] and [11]. If we assume that the series is Gaussian, we can write the joint density of
   <math display="inline">
    (Y(t_1), \ldots, Y(t_n))
   </math>
   at
   <math display="inline">
    (y(t_1),..., y(t_n))
   </math>
   as
  </p>
  <p block-type="Equation">
   <math display="block">
    g(y((t_1),...,y(t_n))) = \prod_{k=1}^n \frac{1}{\sqrt{v_k}} f\left(\frac{y(t_k) - m(t_k)}{\sqrt{v_k}}\right)
   </math>
   (32)
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   where
   <math display="inline">
    f
   </math>
   is the standard normal density function,
   <math display="inline">
    m(t_k) = \alpha_{k,0} + \alpha_{k,1}y(t_{k-1}) + \cdots + \alpha_{k,k-1}y(t_1)
   </math>
   <math display="inline">
    k &gt; 1
   </math>
   ,
   <math display="inline">
    m_1 = EY(t)
   </math>
   and
   <math display="inline">
    v_1 = \text{Var}(Y(t))
   </math>
   . For given values of
   <math display="inline">
    p
   </math>
   and
   <math display="inline">
    q
   </math>
   , maximum Gaussian likelihood (MGL) estimators of the parameters are obtained by maximizing
   <math display="inline">
    g
   </math>
   with respect to the process parameters. This is a nonlinear maximization problem, and requires the use of efficient numerical techniques. Many other estimation procedures are available, with varying asymptotic efficiencies, but it is known that under rather mild conditions and even when the process
   <math display="inline">
    \{Y(t)\}\
   </math>
   is not in fact Gaussian, MGL estimators
  </p>
  <p block-type="Text">
   have good asymptotic properties. Model selection, i.e., choice of
   <i>
    p
   </i>
   and
   <i>
    q
   </i>
   , is usually made by minimization of an information criterion, well-known examples being the AIC, AICC, and BIC. See [5] and [10] for details.
  </p>
  <h2>
   <b>
    Some Extensions and Applications
   </b>
  </h2>
  <p block-type="Text">
   One of the features frequently observed in financial and geophysical time series is the very slow rate of decay with increasing lag of the sample autocorrelation function. In order to model this phenomenon,
   <i>
    fractionally integrated
   </i>
   ARMA and CARMA processes were introduced. These have the property that the autocorrelation function decreases asymptotically at a hyperbolic rate, rather than at the exponential rate of decay for both ARMA and CARMA processes. For information on fractionally integrated processes see [2, 6, 8].
  </p>
  <p block-type="Text">
   Nonlinear generalizations of ARMA and CARMA processes, in particular threshold models (see [14]), have also found a wide variety of applications.
  </p>
  <p block-type="Text">
   In mathematical finance, the CARMA(1,0) (or stationary Ornstein–Uhlenbeck) process, driven by a nondecreasing Levy process, has been used to rep- ´ resent stochastic volatility in the stochastic volatility model of Barndorff-Nielsen and Shephard [1]. Higher order CARMA models for stochastic volatility have also been used to allow for more complex autocorrelation functions (see [4] and [13]). In the COGARCH models of (see Further Reading), the volatility process is a nonlinear self-exciting CARMA process and for COGARCH models in which the volatility process has finite variance, the volatility has the autocorrelation function of a CARMA process, a property analogous to the corresponding result for the volatility of a GARCH process, which, if the volatility has finite variance, has the autocorrelation function of an ARMA process.
  </p>
  <h2>
   <b>
    Acknowledgments
   </b>
  </h2>
  <p block-type="Text">
   The author gratefully acknowledges support of this work by the National Science Foundation under Grant, DMS-0744058.
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Barndorff-Nielsen, O.E. &amp; Shephard, N. (2001). Non-Gaussian ornstein—Uhlenbeck based models and some of their uses in financial economics (with discussion),
     <i>
      Journal of the Royal Statistical Society, Series B
     </i>
     <b>
      63
     </b>
     , 167–241.
    </li>
    <li block-type="ListItem">
     [2] Beran, J. (1994).
     <i>
      Statistics for Long-Memory Processes
     </i>
     , Chapman &amp; Hall, New York.
    </li>
    <li block-type="ListItem">
     [3] Brockwell, P.J. (2001). Levy-driven CARMA processes, ´
     <i>
      Annals of the Institute of Statistical Mathematics
     </i>
     <b>
      53
     </b>
     , 113–124.
    </li>
    <li block-type="ListItem">
     [4] Brockwell, P.J. (2004). Representations of continuoustime ARMA processes,
     <i>
      Journal of Applied Probability
     </i>
     <b>
      41A
     </b>
     , 375–382.
    </li>
    <li block-type="ListItem">
     [5] Brockwell, P.J. &amp; Davis, R.A. (1991).
     <i>
      Time Series: Theory and Methods
     </i>
     , 2nd Edition, Springer, New York.
    </li>
    <li block-type="ListItem">
     [6] Brockwell, P.J. &amp; Marquardt, T. (2005). Fractionally integrated continuous-time ARMA processes,
     <i>
      Statistica Sinica
     </i>
     <b>
      15
     </b>
     , 477–494.
    </li>
    <li block-type="ListItem">
     [7] Cline, D.B.H. &amp; Brockwell, P.J. (1985). Linear prediction of ARMA processes with infinite variance,
     <i>
      Stochastic Processes and their Applications
     </i>
     <b>
      19
     </b>
     , 281–296.
    </li>
    <li block-type="ListItem">
     [8] Comte, F. &amp; Renault, E. (1996). Long memory continuous time models,
     <i>
      Journal of Econometrics
     </i>
     <b>
      73
     </b>
     , 101–149.
    </li>
    <li block-type="ListItem">
     [9] Doob, J.L. (1944). The elementary Gaussian processes,
     <i>
      Annals of the Mathematics and Statistics
     </i>
     <b>
      25
     </b>
     , 229–282.
    </li>
    <li block-type="ListItem">
     [10] Hannan, E.J. (1973). The asymptotic theory of linear time series models,
     <i>
      Journal of Applied Probability
     </i>
     <b>
      10
     </b>
     , 130–145.
    </li>
    <li block-type="ListItem">
     [11] Jones, R.H. (1985). Time series analysis with unequally spaced data, in
     <i>
      Time Series in the Time Domain
     </i>
     ,
     <i>
      Handbook of Statistics
     </i>
     , E.J. Hannan, P.R. Krishnaiah &amp; M.M. Rao, eds, North Holland, Amsterdam, Vol. 5, pp. 157–178.
    </li>
    <li block-type="ListItem">
     [12] Protter, P.E. (2004).
     <i>
      Stochastic Integration and Differential Equations
     </i>
     , 2nd Edition, Springer, New York.
    </li>
    <li block-type="ListItem">
     [13] Todorov, V. &amp; Tauchen, G. (2006). Simulation methods for Levy-driven CARMA sto ´ chastic volatility models,
     <i>
      Journal of Business and Economic Statistics
     </i>
     <b>
      24
     </b>
     , 455–469.
    </li>
    <li block-type="ListItem">
     [14] Tong, H. (1990).
     <i>
      Non-linear Time Series : A Dynamical System Approach
     </i>
     , Clarendon Press, Oxford.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    Further Reading
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     Brockwell, P.J., Chadraa, E. &amp; Lindner, A. (2006). Continuoustime GARCH processes,
     <i>
      Annals of Applied Probability
     </i>
     <b>
      16
     </b>
     , 790–826.
    </li>
    <li block-type="ListItem">
     Kluppelberg, C., Lindner, A. &amp; Maller, R. (2004). A continuous ¨ time GARCH process driven by a Levy process: stationarity ´ and second order behaviour,
     <i>
      Journal of Applied Probability
     </i>
     <b>
      41
     </b>
     , 601–622.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   PETER J. BROCKWELL
  </p>
 </body>
</html>
