# **Extreme Value Theory**

Univariate extreme value theory is primarily concerned with the limiting behavior of large (respectively, *small*) values of real-valued random observations  $X_1, \ldots, X_n$  as the *sample size* n increases indefinitely. We denote this by  $n \to \infty$ , bearing in mind that, in applications,  $n$  may be as small as 10 or 20. In some models,  $n$  may also be random, which brings some added technicalities (see, e.g.,  $\S$  6.2 in [25]). For example, the number *n* of insurance claims  $X_1, \ldots, X_n$  observed in a time period is often assumed to follow a Poisson distri*bution* independent of the claim sequence [3]. In the following, we let  $n \ge 1$  be nonrandom, and denote by

$$X_{1,n} \le \dots \le X_{n,n} \tag{1}$$

the *order statistics* of  $X_1, \ldots, X_n$ , obtained by sorting the observations in increasing order. Of interest is the study of the *extreme values*  $M_n := X_{n,n} =$  $\max\{X_1,\ldots,X_n\}$  and  $m_n := X_{1,n} = \min\{X_1,\ldots,X_n\}$ as  $n \to \infty$ . Multivariate extreme value theory concerns observations  $\mathbf{X}_1,\ldots,\mathbf{X}_n$  in  $\mathbb{R}^d$ , with *extremal* vectors obtained by taking maxima (or minima) coordinate-wise. We start with the univariate case  $d = 1$ , before briefly considering extremes in higher dimensions, in the section Multivariate Extremes. We note that the multivariate theory of extreme values relies heavily on the notion of *copula*, which has given rise to intensive research in the last decades  $[12,$ 45]. An example of its application to *option pricing* is provided in  $[34]$ .

The study of limit laws for  $M_n$  and  $m_n$  is motivated by the quest of *realistic probabilistic models* for extreme values of large collections of random variables (rv's). This is inspired by the common practice of statistical science, which often uses asymptotic formulae to approximate finite sample distributions. In the setup of estimation based on *sample means*, rates of convergence to limiting Gaussian laws are of order  $O(n^{-1/2})$ , which is accurate enough for a number of applications, even when  $n = 50$  or less. In extreme value theory, one may obtain convergence rates as slow as  $O((\log n)^{-1/2})$ , in which case, the limiting theory becomes irrelevant unless the sample size  $n$ is huge (see, e.g., [29] and § 2.10 in [25]). Unfortunately, other than the *standard limit laws* discussed in the section Extremes of Independent and Identically

Distributed (IID) Sequences, there are not many alternative models adapted to this situation, especially when not much is known about the data structure. One should therefore always remain extremely cautious in the conclusions that may be drawn from a blunt application of extreme value models. This drawback (the *curse of small sample sizes*) turns out here to be one of the major practical difficulties to cope with.

There are two great families of statistical problems dealing with extremes, depending on whether the *sample components*  $X_1, \ldots, X_n$  are observed or not. In the first case, one infers from  $X_1, \ldots, X_n$ information concerning the law of  $M_n$  (or  $m_n$ ), and, possibly, on extremes of future observations from analog sequences. This may often be reduced to tail estimation, where one evaluates the tail behavior of the component distributions out of a selected number of extreme order statistics (see the section Tail Estimation). In the second case,  $X_1, \ldots, X_n$  are generally unknown, and only extremes generated by these hidden quantities are at hand. Extreme value theory is then used for *model building*, and statistical applications rely on observations of random samples of extreme values. In this case, there are many competing methods of *parametric estimation* for the standard extreme-value distributions (refer to [8, 39], Chapter  $19-21$  in [35] and Chapter 22 in [36]); however, this problem is not discussed here. We mention that these estimation techniques are always difficult to use in practice, especially for small sample sizes. A major application can be found in *reliability theory* and *life*time analysis, where the basic historical references are the books of Barlow and Proschan [1], Crowder et al. [10], and Kalbfleisch and Prentice [37].

Univariate extreme values has received considerable attention since the pioneering work of Fréchet [24], Fisher and Tippett [22] and Gnedenko [28], from 1925 to 1945. Somewhat later, between 1935 and 1965, Gumbel [30] illustrated the statistical interest of these methods on a number of natural phenomenon (in particular, *floods*). The probabilistic aspects of the theory made serious progress after 1960. In particular, the contributions of de Haan [13] in the years  $1970-1985$ , through a use of the theory of slow and regular variation, allowed to fully characterize the domains of attraction of extremal limit laws for independent and identically distributed (i.i.d.) sequences [5, 51]. Other remarkable developments include the point process theory related to records and extremal processes. These are not considered here, and we refer to [46] and [49] for details. Multivariate extreme value theory was mostly initiated by Geffroy [27], Sibuya [52], and Tiago de Oliveira [54] during 1958–1960. We refer to [7, 15, 16, 47, 53], and the references therein for more recent contributions. A series of books on the subject are available. We may cite, among others, the monographs, surveys and proceedings volumes [4, 8, 9,  $13, 14, 19-21, 23, 25, 26, 39, 40, 48, 55$ , and [57]. We have not attempted to cite more than a tiny fraction of the thousands of papers of interest, and limit ourselves to a few selected references.

#### **Upper and Lower Extremes**

Given an integer  $k \ge 1$ , the *k* upper (respectively, *lower*) sample *extremes* of  $X_1, \ldots, X_n$ , are denoted by

$$M_n^{(1)} := X_{n,n} \ge \dots \ge M_n^{(k)} := X_{n-k+1,n}$$
  
(respectively  $m_n^{(1)} := X_{1,n} \le \dots \le m_n^{(k)} := X_{k,n}$ )  
(2)

for each  $n > k$ . The (sample) *maximum* and *minimum*, are, respectively given by

$$M_n := M_n^{(1)} = X_{n,n} = \max\{X_1, \dots, X_n\} \quad \text{and}$$
  
$$m_n := m_n^{(1)} = X_{1,n} = \min\{X_1, \dots, X_n\} \tag{3}$$

These quantities are of major interest in various fields, especially in economics and finance [3, 14, 19, 21]. For example, when  $X_1, \ldots, X_n$  denote the successive claims registered in an insurance port*folio*, the upper extremes  $M_n^{(1)} \ge M_n^{(2)} \ge \ldots$ , must be closely monitored to evaluate reinsurance premi*ums* [3]. Extreme values are also critical in a number of natural phenomenon, especially when  $X_1, \ldots, X_n$ are measures of *wind speeds*, *sea levels*, *temperature*, or other environmental indicators of interest [6, 8]. Lower extreme values are essential to life studies and *reliability* [1]. In the latter application,  $X_1, \ldots, X_n$ denote the *life spans* of the  $n$  critical components of an equipment. Failure occurs (under the weakest link principle, see, e.g., p. 25 in [8]) at time  $m_n^{(1)} = \min\{X_1, \ldots, X_n\}$ . All kind of variants of this basic model are at hand (such as the  $k$ -out-of- $n$  rule, where the equipment fails when  $k$  of its components have stopped functioning (see, e.g., p. 25 in  $[8]$ ).

In the sequel, we concentrate on *upper extremes*. The description of lower extremes is identical after a scale reversal. Namely, the change of  $X_1, \ldots, X_n$ into  $-X_1, \ldots, -X_n$  replaces the upper (respectively, lower) extremes of the first sequence, into  $(-1)$  times the lower (respectively, upper) extremes of the second sequence.

#### **Extremes of Independent and Identically Distributed (IID) Sequences**

In the standard (or i.i.d.) model of extreme value theory,  $X = X_1, X_2, \ldots$  are *i.i.d.* rv's with distribution function (df)  $F(x) = \mathbb{P}(X \leq x)$ . Here,  $\mathbb{P}(A)$  denotes the probability of the event  $A$ . Even though this model may be far from reality (because of departures from the i.i.d. assumption caused by *trends*, dependence between observations or nonidentically distributed components), it is useful as a starting point in a number of real-life applications [8, 39]. The fundamental results of this model are captured into the following facts.

We first seek conditions for the maximum  $M_n :=$  $M_n^{(1)}$  to have a limiting *nondegenerate* distribution (meaning the law of a nonconstant rv)  $G(\cdot)$ . This is equivalent to the existence of constants  $a_n > 0$  and  $b_n, n = 1, 2, \ldots$ , such that, as  $n \to \infty$ 

$$\mathbb{P}\left(\frac{M_n - b_n}{a_n} \le x\right) = F^n(a_n x + b_n) \longrightarrow G(x) \tag{4}$$

for almost all x in  $\mathbb{R} := (-\infty, \infty)$ . The choice of  $a_n$ ,  $b_n$ , and  $G(\cdot)$  in equation (4) being not unique, the limiting  $G(\cdot)$ , when it exists, is only defined through its type (two df's  $G_1(\cdot)$  and  $G_2(\cdot)$  are of the same type iff there are constants  $A > 0$  and B fulfilling  $G_1(Ax + B) = G_2(x)$  for almost all  $x \in \mathbb{R}$ ).

**Fact 1** The limit law (4) holds for some  $F(\cdot)$ , and appropriate *norming sequences*  $a_n > 0$  and  $b_n$  iff, up to a type equivalence (through the replacement of  $G(x)$  by  $G((x - \mu)/\sigma)$  for some  $\sigma > 0$  and  $\mu$ ),  $G(\cdot)$ belongs to the class of extreme value distributions, collecting the following df's.

The *Gumbel* df:

$$G(x) = \Lambda(x) := \exp\left(-e^{-x}\right) \text{ for } x \in \mathbb{R} \quad (5)$$

• The *Fr´echet* (class of) distribution function(s), defined, for *α >* 0 by

$$G(x) = \Phi_{\alpha}(x) := \begin{cases} \exp(-x^{-\alpha}) & \text{for } x > 0\\ 0 & \text{for } x \le 0 \end{cases}$$
(6)

• The (reverse) *Weibull* (class of) distribution function(s), defined, for *α >* 0, by

$$G(x) = \Psi_{\alpha}(x) := \begin{cases} 1 & \text{for } x \ge 0 \\ \exp(-(-x)^{\alpha}) & \text{for } x < 0 \end{cases}$$
(7)

**Remark 1** Whenever equation *(*4*)* holds, we say that *F (*·*)* belongs to the *domain of attraction* of *G(*·*)* and write *F* ∈ D*(G)*. An extreme value df, given by *(*5*)*, *(*6*)*, or *(*7*)*, is always of the form *G(x)* = exp*(*−*H (x))*, where *H (x)* is either exponential (in the Gumbel case), or a power function (in the Frechet and Weibull cases). In the section, Beyond the ´ Independent and Identically Distributed (IID) Case, this property is extended to nonidentically distributed sequences. In the present setup, *(*4*)* holds iff, for all *x* ∈ , as *n* → ∞,

$$n\left(1 - F(a_n x + b_n)\right) \longrightarrow H(x) \tag{8}$$

**Remark 2** In extreme value theory, Fact 1 plays a role of the same importance as the *central limit theorem* (CLT) in "classical" statistics. The CLT leads experimenters to favor the use of *Gaussian* distributions (within the class of *stable* laws) to describe statistics based on *sums* (or *means*) of large numbers of i.i.d. components. In the same spirit, Fact 1, hints that extreme value laws should provide *reasonable candidates* to model the distributions of extremes of large numbers of rv's. This point is discussed further in the section, Beyond the Independent and Identically Distributed (IID) Case.

**Fact 2** The existence of a limiting df *G(*·*)*, such that *F* ∈ D*(G))*, is, unfortunately, limited to a *relatively small class of smooth distributions F (*·*)*, and applied scientists should not always take it for granted that this property is fulfilled by the data they consider. The characterizations of D*(α)* and D*(α)* are simple in terms of the *survivor function* 1 − *F (x)* = *-(X > x)* (see, for example, *(.*1–2*)* and *(.*1–2*)* below). On the other hand, the characterization of D*(-)* is a difficult mathematical question, which was only solved, after decades of research, by De Haan [13], Mejzler [43] and Marcus and Pinsky [41], among others, in terms of the *upper quantile function U (t)* = *F*inv*(*1 − *t)* = inf{*x* : *F (x)* ≥ 1 − *t*} for 0 *<t<* 1. These results (see equations (11)– (13) below) rely on the notion of *slow variation*. A positive function *L(t)* defined for *t* ≥ *t*<sup>0</sup> is *slowly varying* (in the neighborhood of infinity) iff, independently of *λ >* 0,

$$\lim_{t \to \infty} \frac{L(\lambda t)}{L(t)} = 1\tag{9}$$

For example, for *C >* 0 and *r* ∈ , *L(t)* = *C(*log *t)r* is slowly varying. Such functions have been used in various branches of mathematics, such as *Tauberian theory* (see, e.g., Chapter V in [56]). Their structure is governed by *Karamata's theorem* (see [38], and [51]), showing that *L(*·*)* fulfills equation *(*9*)* iff there exist functions *c(t)* → *c* ∈ *(*0*,*∞*)* and *(t)* → 0 as *t* → ∞, such that, for all *t* ≥ *t*0,

$$L(t) = c(t) \exp\left(\int_{t_0}^t \frac{\epsilon(u)}{u} \, \mathrm{d}u\right) \tag{10}$$

In view of equation *(*10*)*, we may characterize the domains of attraction of the extreme value laws as follows:

- 1. *F* ∈ D*(α)* iff *F* has an infinite upper endpoint *ω* = sup{*x* : *F (x) <* 1}=∞, and fulfills one of the following equivalent conditions.
  - *(.*1*)* There exists a slowly varying function *L*1*(*·*)* such that 1 − *F (x)* = *x*<sup>−</sup>*αL*1*(x)*.
  - *(.*2*)* There exists a slowly varying function <sup>1</sup>*(*·*)* such that *U (t)* = *t*<sup>−</sup>1*/α* 1*(*1*/t)* (0 *< t <* 1).

The df's *F (*·*)* fulfilling *(.*1*)* compose the class of *Pareto-type* laws, which is of major importance in applications of extreme value theory to distributions with infinite upper endpoints.

- 2. *F* ∈ D*(α)*, iff *F* has a finite upper endpoint *ω* = sup{*x* : *F (x) <* 1} *<* ∞, and fulfills one of the following equivalent conditions.
  - *(.*1*)* There exists a slowly varying function *L*2*(*·*)* such that 1−*F (ω* −*x)*=*xαL*2*(*1*/x)* (*x >* 0).

- There exists a slowly varying function  $(\Psi.2)$  $\ell_2(\cdot)$  such that  $U(t) = \omega - t^{1/\alpha} \ell_2(1/t)$  $(0 < t < 1)$ .
- 3.  $F \in \mathcal{D}(\Lambda)$  for distributions with arbitrary upper endpoint, iff, the following condition holds.
  - $(\Lambda)$ There exists a slowly varying function  $\ell_3(\cdot)$  such that, for each  $r > 0$ , as  $t \downarrow 0$ ,

$$U(rt) - U(t) = -(1 + o(1))(\log r)\ell_3(1/t)$$
(11)

The conditions  $(\Phi.1)-(\Phi.2)$ ,  $(\Psi.1)-(\Psi.2)$  and  $(\Lambda)$ , may be combined as follows [13]. The existence of  $G \in \{\Phi_{\alpha}, \Psi_{\alpha}, \Lambda\}$  such that  $F \in \mathcal{D}(G)$  is equivalent to the existence (in  $\mathbb{R}$ ) of the limit, for all  $r > 0$ and  $s > 0$  with  $s \neq 1$ ,

$$L(r,s) = \lim_{t \downarrow 0} \frac{U(rt) - U(t)}{U(st) - U(t)}$$
(12)

Whenever it exists, the limit in equation  $(12)$  can only be of the form  $L_{\nu}(r, s)$ , for some  $\gamma \in \mathbb{R}$ , where

$$L(r,s) = L_{\gamma}(r,s) := \frac{r^{-\gamma} - 1}{s^{-\gamma} - 1} \quad \text{when} \quad \gamma \neq 0$$
(13)

or

$$L(r,s) = L_0(r,s) := \frac{\log r}{\log s} \tag{14}$$

The constant  $\gamma \in \mathbb{R}$  is usually called the *extremal index* pertaining to  $F(\cdot)$  (or  $U(\cdot)$ ). We have the following equivalent statements:

$$\gamma > 0 \Longleftrightarrow F \in \mathcal{D}(\Phi_{\alpha}) \quad \text{with} \quad \alpha = 1/\gamma \quad (15)$$

 $\n\gamma < 0 \Longleftrightarrow F \in \mathcal{D}(\Psi_{\alpha}) \quad \text{with} \quad \alpha = -1/\gamma\n$  $(16)$ 

$$\gamma = 0 \Longleftrightarrow F \in \mathcal{D}(\Lambda) \tag{17}$$

Remark 3 As follows from Fact 2, a serious *amount of smoothness* is needed for a df  $F(\cdot)$  to belong to  $\mathcal{D}(G)$  for some  $G(\cdot)$ . This is especially the case for  $\mathcal{D}(\Lambda)$  and somewhat less for  $\mathcal{D}(\Phi_{\alpha})$  and  $\mathcal{D}(\Psi_{\alpha})$ . A number of *standard distributions* of the literature fulfill the appropriate conditions, but there are some notable exceptions (in particular for the discrete df's, as mentioned below). Also, one should observe that the *standard* continuous distributions of

statistics [35, 36] are always extremely smooth, and this property may not be shared by *real-life* distributions originating from experimental data. Therefore, one should always keep in mind that *the basic* assumptions needed for extreme value models are not likely to be fulfilled by real-life observations. For example, distributions  $F(\cdot)$  with  $U(1/t)$  slowly varying as  $t \to \infty$  do not necessarily fulfill ( $\Lambda$ ), and may very well not belong to  $\mathcal{D}(\Lambda)$ . This illustrates the fact that the use of Gumbel laws to model extremal data is often problematic. For standard distributions, the situation is more clear cut, as follows  $[35, 36]$ :

- All standard continuous df's  $F(\cdot)$  of interest do belong to  $\mathcal{D}(G)$  for some  $G \in \{\Lambda, \Phi_{\alpha}, \Psi_{\alpha}\}$ . In particular, upper extremes of smooth *exponentialtailed* distributions, such as the exponential, gamma, normal, lognormal, and Weibull laws, belong to the domain of attraction  $\mathcal{D}(\Lambda)$  of the Gumbel law. The Fréchet  $\mathcal{D}(\Phi_{\alpha})$  domain holds for distributions with polynomial-type tails, such as exact Pareto laws of the form  $1 - F(x) =$  $C(x-a)^{-\alpha}$  for  $x \ge x_0 > a$ , which are special cases of  $(\Phi.1)$ . Another important example of distributions in the Fréchet domain  $\mathcal{D}(\Phi_{\alpha})$  is given by the non-Gaussian stable laws. The case of the (reverse) Weibull law  $\Psi_{\alpha}$  is somewhat similar to  $\Phi_{\alpha}$ , and is not discussed here.
- Almost all standard *discrete* df's  $F(\cdot)$  of interest do not belong to  $\mathcal{D}(G)$  for any possible  $G \in$  $\{\Lambda, \Phi_{\alpha}, \Psi_{\alpha}\}\.$  For example, the upper extremes of Poisson, geometric, or negative binomial distributions are not in the domain of attraction of an extreme value law. This disappointing fact brings some obvious practical difficulties, since all possible data sets are given on discrete scales.
- Whenever the upper endpoint  $\omega := \sup\{x :$  $F(x) < 1$  of the distribution  $F \in \mathcal{D}(G)$  is finite, the only possible choice for  $G(\cdot)$  is either Weibull  $\Psi_{\alpha}$  or Gumbel  $\Lambda$ .
- Whenever the upper endpoint  $\omega := \sup\{x :$  $F(x) < 1$  of the distribution  $F \in \mathcal{D}(G)$  is infi*nite*, the only possible choice for  $G(\cdot)$  is either Fréchet  $\n\Phi_{\alpha}\n$  or Gumbel Λ.

**Remark 4** Because of Fact 2, the study of upper extremes for *unbounded* distributions may be limited to df's  $F(\cdot)$  belonging to the Fréchet and Gumbel domains of attraction, with a nonnegative extremal index  $\gamma > 0$ . In the sequel, we concentrate on this case, which is the most relevant for financial models. Our point of view would be different if we had been concerned with *reliability theory*, for which the underlying distributions are bounded below by 0 (since the lifetime of a component cannot be negative). For this reason, the (direct or usual) Weibull distributions (including the exponential law) have become reference models for lifetime distributions. These laws correspond to  $\gamma \leq 0$ . We note that the standard form of a Weibull survival time  $T$  is obtained by changing signs in equation (7), so that (after some proper changes of scale and origin)  $\mathbb{P}(T > t) = \exp(-t^{\alpha})$  for  $t > 0$  and some  $\alpha > 0$ . The special case  $\alpha = 1$  yields the *exponential law*, which appears as the foremost example of extreme value law for minima.

**Fact 3** The constants  $a_n$  and  $b_n$  in equation (4) can be chosen as follows. Recall the definition  $U(t) = F^{\text{inv}}(1 - t) := \inf\{x : F(x) \ge 1 - t\} \text{ for } 0 <$  $t < 1$  of the *upper quantile function* of F.

When  $F \in \mathcal{D}(\Phi_{\alpha})$ , we may set  $a_n = U(1/n)$  and  $b_n = 0$  in equation (4), so that, as  $n \to \infty$ ,

$$\mathbb{P}\left(\frac{M_n}{a_n} \le x\right) \longrightarrow \Phi_{\alpha}(x) = \exp(-x^{-\alpha})$$
 for  $x > 0$  (18)

When  $F \in \mathcal{D}(\Lambda)$ , we may set  $a_n = U(1/(ne))$  –  $U(1/n)$  and  $b_n = U(1/n)$  in equation (4).

**Remark 5** As a consequence of the results above, when  $F \in \mathcal{D}(\Phi_{\alpha})$ , we have, for some slowly varying function  $L_0(\cdot)$ , the convergence in distribution, as  $n \to \infty$ 

$$\frac{M_n}{n^{1/\alpha}L_0(n)} \stackrel{d}{\to} \Phi_\alpha \tag{19}$$

This implies a very fast rate of increase for  $M_n$  as  $n \to \infty$ , especially when  $0 < \alpha < 1$ , in which case, it may be shown (see, e.g.,  $§4.5$  in [25]) that the sample sum  $X_1 + \ldots + X_n$  and maximum  $\max\{X_1, \ldots, X_n\}$ have the same order of magnitude. On the other hand, when  $F \in \mathcal{D}(\Lambda)$ , we have, for any choice of  $\alpha > 0$ and  $L_0(\cdot)$ , the convergence in probability, as  $n \to \infty$ 

$$\frac{M_n}{n^{1/\alpha}L_0(n)} \stackrel{\mathbb{P}}{\to} 0 \tag{20}$$

This has important practical consequences. In many real-life examples, the data sets are mixtures of rv's with different distributions. For example, in an insurance portfolio, it is often the case that the central part of the claim distribution is well fitted with a *lognormal distribution*. The exceptions, when they occur, are mostly in the upper tails, where one often detects a small proportion of *outliers* [2]. which cannot fit into the lognormal model. When such a phenomenon occurs, it should be interpreted as the presence in the data set of a *Pareto-type* component (fulfilling  $(\Phi.1)$ ), by definition, in the domain of attraction of a Fréchet law [50]. What one should do then is to perform some tail-estimation procedure on the outlying data. This, however, is always problematic, especially if the number of outliers (or identified as such) is relatively small. On the other hand, the presence of Pareto-type claims in a portfolio may generate huge losses if they are not properly taken into account.

Fact 4 In the standard model, the limiting behavior of the maximum  $M_n = M_n^{(1)}$  governs the joint limiting behavior of any fixed number  $k \ge 1$  of upper extremes  $M_n^{(1)}, \ldots, M_n^{(k)}$ . More specifically, for each fixed  $k \ge 1$ , the existence of a nondegenerate limiting distribution  $G(\cdot)$  for the maximum  $M_n^{(1)}$  fulfilling equation  $(4)$  is equivalent to the existence of a limiting joint distribution for any (nonempty) subset of the  $k \ge 1$  upper extremes  $M_n^{(1)}, \ldots, M_n^{(k)}$ . In particular, equation (4) holds if and only if, for each  $x_1,\ldots,x_k\in\mathbb{R}$ , as  $n\to\infty$ ,

$$\mathbb{P}\left(\frac{M_n^{(1)} - b_n}{a_n} \le x_1, \dots, \frac{M_n^{(k)} - b_n}{a_n} \le x_k\right) \longrightarrow G_k(x_1, \dots, x_k) \tag{21}$$

where  $G_k$  is a nondegenerate multivariate distribution depending only upon G and  $k \ge 1$ . Its structure is of major interest, and is detailed below. First, we consider the  $k$ th maximum, for an arbitrary (but fixed)  $k \ge 1$ . Let  $G(x) = \exp(-H(x))$  in equation (4). This statement is equivalent to having, for each specified  $k \ge 1$  and all  $x \in \mathbb{R}$ , as  $n \to \infty$ ,

$$\mathbb{P}\left(\frac{M_n^{(k)} - b_n}{a_n} \le x\right) \longrightarrow \Big\{ \sum_{j=0}^{k-1} \frac{H(x)^j}{j!} \Big\} \times \exp(-H(x)) \quad (22)$$

It is convenient to define the inverse function of  $H(x) = -\log G(x)$  by  $H^{\text{inv}}(t) = \inf\{x : H(x) \ge t\}$ 

<table>

 **Table 1** Scaling functions for extreme value laws

| G(x)               | H(x)                            | $H^{\text{inv}}(t)$         |
|--------------------|---------------------------------|-----------------------------|
| $\Lambda(x)$       | $e^{-x}$ for $x \in \mathbb{R}$ | $-\log t$ for $t > 0$       |
| $\Phi_{\alpha}(x)$ | $x^{-\alpha}$ for $x > 0$       | $t^{-1/\alpha}$ for $t > 0$ |
| $\Psi_{\alpha}(x)$ | $(-x)^{-\alpha}$ for $x < 0$    | $-t^{-1/\alpha}$ for $t>0$  |
|                    |                                 |                             |

for  $t > 0$ . We obtain the particular cases given in Table 1.

In view of Table 1, we may give an explicit form for the limit law in equation (21). Denote by  $\omega_1, \omega_2, \ldots$  i.i.d. exponential rv's with mean 1. In other words,  $\mathbb{P}(\omega_i > y) = e^{-y}$  for  $y \ge 0$ . Then, for each specified  $k > 1$ , we have the convergence in distribution, as  $n \to \infty$ .

$$\left\{\frac{M_n^{(1)} - b_n}{a_n}, \dots, \frac{M_n^{(k)} - b_n}{a_n}\right\}\n$$

$$\n\stackrel{d}{\rightarrow} \left\{H^{\text{inv}}(\omega_1), \dots, H^{\text{inv}}(\omega_1 + \dots + \omega_k)\right\}\n$$
(23)

Remark 6 This result has the important consequence that (in the standard model, subject to  $F \in$  $\mathcal{D}(G)$ , for each fixed  $k > 1$ , the limiting structure of the k upper extremes depends only upon  $G(x)$  =  $\exp(-H(x))$  through the nonlinear change of scale  $t \to H^{\text{inv}}(t)$ .

Remark 7 The fact that the Gumbel class is a boundary case between the Fréchet and Weibull laws becomes straightforward when  $\Lambda$ ,  $\Phi_{\alpha}$ ,  $\Psi_{\alpha}$  are united into the class of generalized extreme value (GEV) laws. The latter, in standard form, depend only upon the *extreme* value index  $\gamma$  (or shape parameter) through

$$G_{\gamma}(x) = \begin{cases} \exp(-(1+\gamma x)^{1/\gamma})\\ \text{for } 1+\gamma x > 0 \text{ and } \gamma \neq 0\\ \exp(-\mathrm{e}^{-x})\\ \text{for } x \in \mathbb{R} \text{ and } \xi = 0 \end{cases}$$
(24)

In the GEV setup,  $G_{\gamma}$  is of Weibull  $\Psi_{-1/\gamma}$  type when  $\gamma < 0$ , of Fréchet  $\Phi_{1/\gamma}$  type when  $\gamma > 0$ , and Gumbel  $\Lambda$  type when  $\gamma = 0$ . The representation of extreme value laws through  $\Lambda$ ,  $\Phi_{\alpha}$ , and  $\Psi_{\alpha}$ with  $\alpha > 0$ , or through  $G_{\gamma}$  with  $\gamma \in \mathbb{R}$  is a matter of pure convenience, and corresponds to identical

models. In each case, an extreme value distribution is characterized by the extremal index (also called the *shape parameter*)  $\gamma$ , and by centering and scale factors  $\mu$  and  $\sigma > 0$ . The general form of  $G(\cdot)$  in equation  $(4)$  is therefore given by

$$G(x) = G_{\gamma} \left(\frac{x-\mu}{\sigma}\right) \tag{25}$$

where  $G_{\gamma}$  is as in equation (24). The GEV model has the advantage of characterizing an extreme value distribution through the triplet of parameters  $\gamma \in \mathbb{R}$ ,  $\mu \in \mathbb{R}$ , and  $\sigma > 0$ .

# **Beyond the Independent and Identically** Distributed (IID) Case

The simplest variant of the standard model of the section, Extremes of Independent and Identically Distributed (IID) Sequences, is when  $X_1, X_2, \ldots$ are independent but nonidentically distributed. Setting  $F_i(x) = \mathbb{P}(X_i < x)$  for  $i = 1, 2, \ldots$ , the corresponding version of equation  $(4)$  is given by

$$\mathbb{P}\left(\frac{M_n - b_n}{a_n} \le x\right) = \prod_{j=1}^n F_j(a_n x + b_n) \longrightarrow G(x)$$
(26)

If no further assumption is made, then the asymptotic theory based upon equation (26) is rather disappointing, since any given df  $G(\cdot)$  may be the limit law of  $(M_n - b_n)/a_n$  for some suitable  $b_n$  and  $a_n > 0$ , subject to the proper choice of  $F_1, F_2, \ldots$  [33]. It is natural to impose some additional *uniformity assumptions* (see, e.g.,  $\S$  3.10 in [25]) implying, in particular, that the limiting law in equation  $(26)$  is unaffected by deleting one component of the sequence. If so, setting  $G(x) = \exp(-H(x))$ , in view of equation (8), we see that equation  $(26)$  (subject to the uniformity assumptions) is equivalent to

$$\sum_{j=1}^{n} (1 - F_j(a_n x + b_n)) \longrightarrow H(x) \tag{27}$$

In this case (see, e.g., § 3.9 in [25]), the class of  $H(\cdot)$  on the right-hand side of equation (27) is limited to the functions fulfilling one among the following conditions.

- 1.  $H(\cdot) = -\log G(\cdot)$  is convex.
- The upper endpoint  $\omega_G := \sup\{x : G(x) < \infty\}$ 2. of  $G(\cdot)$  is finite and  $H(\omega_G - e^{-x})$  is convex.
- The lower endpoint  $a_G := \inf\{x : G(x) > 0\}$  of 3.  $G(\cdot)$  is finite and  $H(a_G + e^x)$  is convex.

It follows that (1) holds when  $G(x) = \Lambda(x)$ , since  $H(x) = e^{-x}$  is convex. When  $G(x) = \Phi_{\alpha}(x)$ , we have  $a_G = 0$  and  $H(x) = x^{\alpha}$ . In this case, (3) holds, since  $H(e^x) = e^{-\alpha x}$  is convex. A similar result holds for  $G(x) = \Psi_{\alpha}(x)$ , which is covered by (2).

At this point, we reach the unpleasant conclusion that some slight variations in the assumption that the sample components are identically distributed may result in extremes with distributions far away from the classical triplet  $\Phi_{\alpha}$ ,  $\Psi_{\alpha}$ , and  $\Lambda$  of the section Extremes of Independent and Identically Distributed (IID) Sequences. Recalling Remark (2), we may compare this with the erroneous belief (which is quite common among nonexpert statisticians) that most distributions generated by sums of random components should be close to Gaussian laws. Any serious applied scientist knows quite well that this is very often untrue. Moreover, there is no such thing as a *perfect Gaussian sample*, even when it comes from simulated data. In the same spirit, there is no such thing as a *perfect extreme-value sample*, in the sense that, on real-life data sets,  $\Phi_{\alpha}$ ,  $\Psi_{\alpha}$ , and  $\Lambda$  provide, at best, some rough approximations of some more complex unknown distributions. There is no general recipe available in the scientific literature to go beyond extreme value laws, and one must, therefore, proceed with care in the interpretation of data by extreme value laws, in particular, by using nonparametric methods when the sample size allows their application.

There is a very important literature on extremes generated by *dependent sequences*. We refer to [25] for details on exchangeable and related models, and to [40] for a general treatment of extremes generated by various types of stochastic processes, such as stationary sequences. As far as financial time series are concerned, the recent review  $[44]$  is most useful. It turns out that, under rather weak *tail-mixing conditions*, measuring the asymptotic independence of random components, subject to the fact that they

are simultaneously large and with indices far form each other, most of the results of the standard case still apply. This is even true for *tail estimators* [32]. Extremes generated by *continuous processes* tend to be much more complex than those that we have been considering up to now for integer-indexed sequences. For this reason, this question is not discussed here. The case of *Gaussian processes* has been extensively explored, and we refer to [40] for an advanced initiation to this problem.

#### **Tail Estimation**

We limit ourselves to the case where the upper endpoint  $\omega = \sup\{x : F(x) < 1\}$  of the df F is infinite  $(\omega = \infty)$ , and where F is in the domain of attraction of an (upper) extreme value distribution. As discussed in the section Upper and Lower Extremes we have, either  $F \in \mathcal{D}(\Lambda)$ , or  $F \in \mathcal{D}(\Phi_{\alpha})$  for some  $\alpha > 0$ . Since the Gumbel law  $\Lambda$  is the limit of  $\Phi_{\alpha}$ when  $\alpha \to \infty$ , we set, for convenience,  $\Lambda = \Phi_{\infty}$ , and assume that  $F \in \mathcal{D}(\Phi_{\alpha})$  for some  $\alpha \in (0, \infty]$ . The problem is then to estimate the regular varia*tion index*  $\alpha > 0$ , or equivalently, the *tail index*  $\gamma =$  $1/\alpha \in [0,\infty)$ , out of  $X_1,\ldots,X_n$ . In the following, we restrict our study to  $\gamma \in (0, \infty)$ , for Pareto-type laws, fulfilling, as  $x \to \infty$ ,

$$1 - F(x) = x^{-1/\gamma} L_1(x) = x^{-1/\gamma} L_1(x) \tag{28}$$

where  $L_0(\cdot)$  is a slowly varying function. The modern approach to this problem was initiated in 1975 by Hill [31], who introduced the estimator

$$\widehat{\gamma}_n = \frac{1}{k} \sum_{j=1}^k \left\{ \log X_{n-j+1,n} - \log X_{n-k,n} \right\}$$
$$= \sum_{j=1}^k \frac{j}{k} \left\{ \log X_{n-j+1,n} - \log X_{n-j,n} \right\} \tag{29}$$

where  $1 \leq k = k_n < n$  is an integer sequence varying with n. The use of  $\widehat{\gamma}_n$  is motivated by its *optimal character* as an estimator of  $\gamma$  based upon the k upper extremes  $\{X_{n-j+1,n}: 1 \le j \le k+1\},\$ in the particular case where  $F(x) = 1 - x^{-\alpha}$  for  $x \ge 1$ , follows an *exact* Pareto law. Moreover, the *Hill estimator* remains consistent under equation (28) subject to minimal conditions imposed upon  $k_n$ . This was proved in 1982 by Mason [42] (see also Deheuvels *et al.*  $[17]$ ), who showed that the conditions  $k = k_n \to \infty$  and  $k_n/n \to 0$  are *necessary* and sufficient for the convergence (in probability) of  $\gamma_n$  to  $\gamma$  as  $n \to \infty$  for all  $F \in \mathcal{D}(\Phi_{1/\gamma})$ . For an *exact* Pareto law, these conditions also imply the asymptotic normality of  $\widehat{\gamma}_n$ , which fulfills the convergence in distribution (with  $N(0, 1)$  denoting the standard normal law), as  $n \to \infty$ ,

$$\sqrt{k_n} \left\{ \frac{\widehat{\gamma}_n}{\gamma} - 1 \right\} \stackrel{d}{\to} N(0, 1) \tag{30}$$

Unfortunately, with the exception of the special case of exact Pareto laws, the Hill estimator is always *biased*, and problematic to use. Besides the choice of the origin (to allow the definition of the logarithms in equation 29), the selection of  $k = k_n$  is quite delicate. This was illustrated in a number of papers where the performance of the Hill estimator was investigated on simulated and real data sets [3, 50], with different choices of  $k$ . At times, the results are so poor that these graphs have been called Hill horror plots. Hill's estimator as well as a number of alternative estimators of  $\gamma$  fall into the general class of *Kernel* estimators introduced in 1985 by Csörgő et al. [11]. These require a nonincreasing kernel  $K(t) > 0$  on  $[0, 1]$  and are defined by

$$\widetilde{\gamma}_{n} = \left(\sum_{j=1}^{k} \frac{j}{k} K\left(\frac{j}{k}\right) \left\{\log X_{n-j+1,n} - \log X_{n-j,n}\right\}\right)$$
$$\times \left(\sum_{j=1}^{k} \frac{1}{k} K\left(\frac{j}{k}\right)\right)^{-1} \tag{31}$$

Hill's estimator is the special case of equation (31) obtained by choosing  $K(t) = 1$  for  $0 < t < 1$ . As seen from the results of [11], the optimality of Hill's estimator is restricted to exact Pareto laws. An interesting application is given when  $\widetilde{\gamma}_n$  is applied to non-Gaussian stable laws. It turns out that in this case, the optimal choice of  $K(\cdot)$  is given by  $K(t) = 1 - t$ for  $0 \le t \le 1$ , and the optimal choice of  $k_n$  is of the form  $Rn^{1/3}$  for some appropriate constant R. This illustrates again the curse of small sample sizes. For example, when  $n = 1000$ , which is a rather large sample, we get  $n^{1/3} = 10$ . It is then rather difficult to admit that such a small value should fall within the range of  $k_n \to \infty$ .

In applied science, tail estimation is a mix between know-how and advanced mathematics. The latter provides nice limit laws, with assumptions which are, more or less, never really fulfilled by the data of interest. At this point, an expert opinion becomes closer to a fancy guess than a seriously established scientific fact. We omit a detailed discussion of the various methods in use [3, 8, 14, 18, 19, 21, 39].

#### Multivariate Extremes

For convenience, we limit our exposition to an i.i.d. sequence  $(X_1, Y_1), \ldots, (X_n, Y_n)$  of random vectors in  $\mathbb{R}^2$ . We are concerned with the limiting behavior of  $(U_n, V_n) := (\max\{X_1, \ldots, X_n\}, \max\{Y_1, \ldots, Y_n\})$ as  $n \to \infty$ , assuming the existence of sequences of constants  $a'_n > 0$ ,  $a''_n > 0$ ,  $b'_n$  and  $b''_n$  such that, as  $n \to \infty$ ,

$$\mathbb{P}\Big(\frac{U_n - b'_n}{a'_n} \le x, \frac{V_n - b''_n}{a''_n} \le y\Big) \longrightarrow G(x, y) \tag{32}$$

Here,  $G(\cdot, \cdot)$  denotes a bivariate df with nondegenerate marginal df's  $K_1(x) = G(x, \infty)$  and  $K_2(y) =$  $G(\infty, y)$ . This implies that  $K_1$  and  $K_2$  belong to the class  $\{\Phi_{\alpha}, \Psi_{\alpha}, \Lambda : \alpha > 0\}$  of extreme value laws. This, however, is not enough. The copula function  $C(\cdot,\cdot)$  of  $G(\cdot,\cdot)$  defined through the identity

$$G(x, y) = C(K_1(x), K_2(y))$$
 (33)

must belong to the class of extreme value copulas. As shown by Pickands  $[47]$ , the characterization of a bivariate extreme value copula reduces to a convex function  $\{D(x) : x \in [0, 1]\}$  fulfilling the inequalities max $\{x, 1 - x\} \le D(x) \le 1$ . Several nonparametric estimation procedures have been proposed ( $[7, 16,$ 53, 57], and, for example, Chapter 3 [39]) to estimate  $D(\cdot)$ , which is often called the *dependence function* (even though this denomination is ambiguous, having been used with other meanings) of  $G(\cdot, \cdot)$ . This is a very active research field at the present time. The situation in higher dimensions ( $d \ge 3$ ) is even more complex, and has not yet led to very practical solutions.

### **Conclusion**

Extreme value theory is a fascinating mathematical domain, which is often rather difficult to apply to real-life experiments, mostly because it relies on *tail properties* of the underlying distributions, for which very few observations are available. One should therefore always keep in mind, in this domain, the general principle that *statistical conclusions are always limited by the information carried by the data itself*. When this information is sparse or not available, scientists should remain careful about its interpretation.

### **References**

- [1] Barlow, R.E. & Proschan, F. (1975). *Statistical Theory of Reliability and Life Testing: Probability Models*, Holt, Rinehart and Winston, New York.
- [2] Barnett, V. & Lewis, T. (1995). *Outliers in Statistical Data*, 3rd Edition, Wiley, New York.
- [3] Beirlant, J., Teugels, J.L. & Vynckier, P. (1994). Extremes in non-life insurance, in *Extreme Value Theory and Applications*, J. Galambos, J. Lechner, E. Simiu & N. Macri, eds, Kluwer, Dordrecht, pp. 489–510.
- [4] Beirlant, J., Teugels, J.L. & Vynckier, P. (1996). *Practical Analysis of Extreme Values*, Leuven University Press, Leuven.
- [5] Bingham, N.H., Goldie, C. & Teugels, J. (1987). *Regular Variation*, Encyclopedia of Mathematics and its Applications, Cambridge University Press, Cambridge, Vol. 27.
- [6] Buishand, T.A. (1989). Statistics of extremes in climatology, *Statistica Neerlandica* **43**, 1–30.
- [7] Capera ´ a, P. & Foug ` eres, A.-L. (2000). Estimation of ` a bivariate extreme value distribution, *Extremes* **3**, 311–329.
- [8] Castillo, E. (1988). *Extreme Value Theory in Engineering*, Academic Press, New York.
- [9] Coles, S.G. (2001). *An Introduction to Statistical Modeling of Extreme Values*, Springer, New York.
- [10] Crowder, M.J., Kimber, A.C., Smith, R.L. & Sweeting, T.J. (1991). *Statistical Analysis of Reliability Data*, Chapman & Hall, London.
- [11] Csorg ¨ o, S., Deheuvels, P. & Mason, D. (1985). Kernel ˝ estimates of the tail index of a distribution, *Annals of Statistics* **13**, 1050–1077.
- [12] Dall'Aglio, G. (1991). *Advances in Probability Distributions with given Marginals*, Kluwer, Dordrecht.
- [13] De Haan, L. (1970). *On Regular Variation and Its Application to the Weak Convergence of Sample Extremes*, Mathematical Centre Tracts, Mathematisch Centrum, Amsterdam, Vol. 32.
- [14] De Haan, L., De Vries, C.G., Husler, J. & Smith, W.B. ¨ (1996). Multivariate extreme value estimation with

applications to economics and finance, *Communications in Statistics – Theory and Methods* **25**(4), 685–908.

- [15] Deheuvels, P. (1978). Caracterisation compl ´ ete des lois ` extremes multivari ˆ ees et de la convergence des types ´ extremes. ˆ *Publications de l'Institut de Statistique de l'Universit´e de Paris* **23**, 1–36.
- [16] Deheuvels, P. (1991). On the limiting behavior of the Pickands estimator for bivariate extreme value distributions, *Statistics and Probability Letters* **12**, 429–439.
- [17] Deheuvels, P., Haeusler, E. & Mason, D.M. (1988). Almost sure convergence of the Hill estimator, *Mathematical Proceedings of the Cambridge Philosophical Society* **104**, 371–381.
- [18] Dekkers, A.L.M., Einmahl, J.H.J. & De Haan, L. (1989). A moment estimator for the index of an extreme value distribution, *Annals of Statistics* **17**, 1833–1855.
- [19] Embrechts, P., Kluppelberg, C. & Mikosch, T. (1997). ¨ *Modelling Extremal Events for Insurance and Finance*, Springer, Berlin.
- [20] Epstein, N. (1960). Elements of the theory of extreme values, *Technometrics* **2**, 27–41.
- [21] Finkenstadt, B. & Rootz ¨ en, H. (2004). ´ *Extreme Values in Finance, Telecommunications, and the Environment*, Chapman & Hall/CRC, Boca Raton.
- [22] Fisher, R.A. & Tippett, L.H.C. (1928). Limiting forms of the frequency distributions of largest or smallest member of a sample, *Mathematical Proceedings of the Cambridge Philosophical Society* **24**, 180–190.
- [23] Fougeres, A.-L. (2004). Multivariate extremes, in ` *Extreme Values in Finance, Telecommunications, and the Environment.*, B. Finkenstadt & H. Rootzen, eds, Chapman & Hall/CRC, Boca Raton, pp. 373–388.
- [24] Frechet, M. (1927). Sur la loi de probabilit ´ e de l' ´ ecart ´ maximum. *Annales de la Soci´et´e Math´ematique Polonaise de Cracovie (Krak´ow),* **6**, 93–116.
- [25] Galambos, J. (1987). *The Asymptotic Theory of Extreme Order Statistics*, 2nd Edition, Krieger, Dordrecht.
- [26] Galambos, J., Lechner, J. & Simiu, E. (1994). *Extreme Value Theory and Applications*, Kluwer, Dordrecht.
- [27] Geffroy, J. (1958–59). Contributions a la th ` eorie ´ des valeurs extremes. I,II, ˆ *Publications de l'Institut de. Statistique de l'Universit´e de Paris* **7,8**, 37–121,124 –184.
- [28] Gnedenko, B. (1943). Sur la distribution limite du terme maximum d'une serie al ´ eatoire, ´ *Annals of Mathematics* **44**, 423–453.
- [29] Gomes, M.I. (1984). Penultimate limiting forms in extreme value theory, *Annals of the Institute of Statistical Mathematics* **36**, 71–85.
- [30] Gumbel, E.J. (1958). *Statistics of Extremes*, Columbia University Press, New York.
- [31] Hill, B.M. (1975). A simple general approach to inference about the tail of a distribution, *Annals of Statistics* **3**, 1163–1174.
- [32] Hsing, T. (1991). On tail index estimation using dependent data, *Annals of Statistics* **19**, 1547–1569.
- [33] Husler, J. (1994). Extremes: limit results for univari- ¨ ate and multivariate nonstationary sequences, in *Extreme*

*Value Theory and Applications*, J. Galambos, J. Lechner, E. Simiu & N. Macri, eds, Kluwer, Dordrecht, pp. 283–304.

- [34] Husler, J. (1996). Multivariate option pricing and ¨ extremes, in *Multivariate Extreme Value Estimation with Applications to Economics and Finance*, W.B. Smith, J. Husler, L. DeHaan & C.G. DeVries, eds, Communications in Statistics, Special Issue, pp. 853–870, Vol. 25.
- [35] Johnson, N.L., Kotz, S. & Balakrishnan, N. (1994). *Continuous Univariate Distributions*, 2nd Edition, Wiley, New York, Vol. 1.
- [36] Johnson, N.L., Kotz, S. & Balakrishnan, N. (1995). *Continuous Univariate Distributions*, 2nd Edition, Wiley, New York, Vol. 2.
- [37] Kalbfleisch, J.D. & Prentice, R.L. (1980). *The Statistical Analysis of Failure Time Data*, Wiley, New York.
- [38] Karamata, J. (1933). Sur un mode de croissance reguli ´ ere. Th ` eor ´ emes fondamentaux, ` *Bulletin de la Soci´et´e Math´ematique de France* **61**, 55–62.
- [39] Kotz, S. & Nadarajah, S. (2000). *Extreme Value Distributions – Theory and Applications*, Imperial College Press, London.
- [40] Leadbetter, M.R., Lindgren, G. & Rootzen, H. (1983). ´ *Extremes and Related Properties of Random Sequences and Processes*, Springer, New York.
- [41] Marcus, M.B. & Pinsky, M.A. (1969). On the domain of attraction of exp*(*−e<sup>−</sup>*<sup>x</sup> )*, *Journal of Mathematical Analysis and Applications* **28**, 440–449.
- [42] Mason, D.M. (1982). Laws of large numbers for sums of extreme values, *Annals of Probability* **10**, 754–764.
- [43] Mejzler, D.G. (1949). On a theorem of B.V. Gnedenko (Russian). *Sbornik Nauchnik Trudov Institut Akademiya Nauk Ukrainskoyi SSR,* **12**, 31–35.
- [44] Mikosch, T. (2004). Modeling dependence and tails of financial time series, in *Extreme Values in Finance, Telecommunications, and the Environment.*, B. Finkenstadt & H. Rootzen, eds, Chapman & Hall/CRC, Boca Raton, pp. 187–278.

- [45] Nelsen, R.B. (1998). *An Introduction to Copulas*, Springer, New York.
- [46] Nevzorov, V.B (2001). *Records: Mathematical Theory*. Translation of Mathematical Monographs, American Mathematical Society, Providence. Vol. 194.
- [47] Pickands, J. (1981). Multivariate extreme value distributions. *Proceedings of the 43d ISI Session, Buenos Aires,* 859–878.
- [48] Reiss, R.D. & Thomas, M. (1997). *Statistical Analysis of Extreme Values*, Birkhauser, Basel. ¨
- [49] Resnick, S. (1987). *Extreme Values, Regular Variation and Point Processes*, Springer, New York.
- [50] Resnick, S. (2004). Modeling data networks, in *Extreme Values in Finance, Telecommunications, and the Environment*, B. Finkenstadt & H. Rootzen, eds, Chapman & Hall/CRC, Boca Raton, pp. 289–361.
- [51] Seneta, E. (1976). *Regularly Varying Functions*, Lecture Notes in Mathematics, Springer, Berlin, Vol. 508.
- [52] Sibuya, M. (1960). Bivariate extremal statistics, *Ann. Inst. Statist. Math.* **11**, 195–210.
- [53] Tawn, J.A. (1988). Bivariate extreme value theory: models and estimation, *Biometrika* **75**, 397–415.
- [54] Tiago de Oliveira, J. (1958). Extremal distributions, *Revista da Faculdade Ciˆencias. Lisboa* **2**, Ser. A Math., **7**, 215–227.
- [55] Tiago de Oliveira, J. (1984). *Statistical Extremes and Applications*, Reidel, Dordrecht.
- [56] Widder, D.V. (1972). *The Laplace Transform*, Princeton University Press, Princeton.
- [57] Xin, H. (1992). *Statistics of Bivariate Extreme Values*. Tinbergen Institute Research Series, Tinbergen.

# **Related Articles**

**Copulas in Econometrics**; **Heavy Tails**; **Moment Explosions**; **Risk Measures: Statistical Estimation**; **Stylized Properties of Asset Returns**.

PAUL DEHEUVELS