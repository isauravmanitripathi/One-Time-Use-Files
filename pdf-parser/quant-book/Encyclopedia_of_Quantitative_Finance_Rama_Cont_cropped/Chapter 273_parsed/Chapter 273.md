# **Rare-event Simulation**

In finance, the need for evaluation of the probability  $\mathbb{P}(A)$  that an event is rare (in the sense that  $\mathbb{P}(A)$  is small) arises, for example, in pricing out-ofthe-money options and Value-at-Risk (VaR) calculations in credit risk and operational risk. However, similar problems arise in a large variety of application areas: insurance risk (ruin probabilities and probabilities of large accumulated claims); communications engineering (probabilities of blocking, bit errors, packet loss, etc.); reliability (probabilities of unavailability in steady state or in a given time interval): and so on.

In all of these examples, explicit evaluation of  $\mathbb{P}(A)$  is seldom possible unless unrealistically simple model assumptions have been made. Monte Carlo simulation is therefore one of the main tools in this area. However, the fact that  $\mathbb{P}(A)$  is small raises some specific problems.

By an estimator Z for  $z = \mathbb{P}(A)$ , we refer to a random variable  $(rv)$  Z that can be generated by simulation and satisfies  $\mathbb{E}Z = z$ . In practice, one simulates  $R$  (say) independent copies of  $Z$  and gives the estimate of z as the average  $Z_R$ , supplemented with a confidence interval, say, an equitailed 95% confidence interval  $\widehat{Z}_R \pm w_R$  where  $s^2$  is the empirical variance of the  $R$  simulated values of  $Z$  (the natural estimator of  $\sigma_Z^2 = \mathbb{V}arZ$ ) and  $w_R = 1.96s/R^{1/2}$ is the half-width. In most other Monte Carlo contexts, a small  $w_R$  is the relevant criterion for an estimator to be efficient. However, in a rare-event setting, one is rather interested in the relative halfwidth  $w_R/\mathbb{P}(A)$ . For example, the confidence interval  $10^{-5} \pm 10^{-4}$  may look narrow, but it does not help to tell whether z is of the magnitude  $10^{-4}$ ,  $10^{-5}$ , or even much smaller. Another way to illustrate the problem is in terms of the sample size  $R$  needed to acquire a given relative precision, say 10%, in terms of the half-width of the 95% confidence interval. This leads to the equation  $1.96 \sigma_Z/(z\sqrt{R}) = 0.1$ , that is,

$$R = \frac{100 \cdot 1.96^2 z (1-z)}{z^2} \sim \frac{100 \cdot 1.96^2}{z} \quad (1)$$

which increases like  $z^{-1}$  as  $z \downarrow 0$ . Thus, if z is small, large sample sizes are required.

The standard formal setup for discussing such efficiency concepts is to consider a family  $\{A(x)\}\$ 

where  $x \in (0, \infty)$  or  $x \in \mathbb{N}$ , assume that  $z(x) \stackrel{\text{def}}{=}$  $\mathbb{P}(A(x)) \to 0$  as  $x \to \infty$ , and for each x let  $Z(x)$  be an unbiased estimator for  $z(x)$ , that is,  $\mathbb{E}Z(x) = z(x)$ . An algorithm is defined as a family  $\{Z(x)\}\$  of such rv's.

The best performance that has been observed in realistic rare-event settings is *bounded relative error* as  $x \to \infty$ , meaning

$$\limsup_{x \to \infty} \frac{\mathbb{V}ar\ Z(x)}{z(x)^2} \ < \ \infty \tag{2}$$

In particular, such an algorithm will have the feature that  $R$  as computed in equation (1), with  $z(1-z)$  replaced by  $\mathbb{V}ar Z(x)$ , remains bounded as  $x \to \infty$ .

An efficiency concept slightly weaker than equation (2) is *logarithmic efficiency*:  $\forall ar(Z(x)) \rightarrow 0$  so quickly that

$$\limsup_{x \to \infty} \frac{\sqrt[3]{ar \ Z(x)}}{z(x)^{2-\varepsilon}} = 0 \tag{3}$$

for all  $\varepsilon > 0$ , or, equivalently, that

$$\liminf_{x \to \infty} \frac{|\log \mathbb{V} ar \ Z(x)|}{|\log z(x)^2|} \ \geq \ 1 \tag{4}$$

The most established method for producing estimators with such efficiency properties is *importance* sampling, where one simulates not from the physical measure  $\mathbb{P}$  but another probability measure  $\mathbb{P}$ with changed parameters. Then  $Z = I(A)W$  where  $W = d\mathbb{P}/d\mathbb{P}$  is the likelihood ratio. Here are two standard examples:

**Example 1**  $A = I(S_n > nm)$  where  $S_n = X_1 +$  $\cdots + \bar{X}_n$ <sup>a</sup> with  $X_1, X_2, \ldots$  independent and identically distributed (i.i.d.) with density  $f$  and  $m >$  $\mathbb{E}X$ . Here one defines  $\widetilde{f}(y) = e^{\theta y} f(y) / \mathbb{E} e^{\theta X}$  with  $\theta$  chosen such that  $\int y \widetilde{f}(y) dy = m$ . The estimator becomes

$$Z(n) = I(S_n > nm) \prod_{k=1}^{n} \frac{f(X_k)}{\widetilde{f}(X_k)}$$
(5)

and is logarithmically efficient as  $n \to \infty$ .

**Example 2**  $A = I(\max S_n > x)$  with  $X_1, X_2, \ldots$ as in (1) and  $\mathbb{E}X < 0$ . Here one defines  $\widetilde{f}(y) =$  $e^{\theta y} f(y)$  with  $\theta > 0$  chosen such that  $\int \widetilde{f}(y) dy = 1$ 

(this  $\theta$  is always unique and will exist for most standard light-tailed distributions). The estimator becomes  $Z(x) = \exp\{-S_{\tau(x)}\}$  where  $\tau(x) = \inf\{n :$  $S_n > x$  and has bounded relative error as  $x \to \infty$ .

The definition of bounded relative error implies that in Example 2, the variance, compared to crude Monte Carlo, is reduced by a factor of order  $z(x)$  =  $\mathbb{P}(A(x))$ . Similar remarks apply to Example 1, though the variance reduction may be slightly smaller since there is only logarithmical efficiency there.

The principle behind the choice of  $\mathbb{P}$  in these two examples (exponential change of measure) is to take  $\mathbb{P}$  as an asymptotic conditional distribution given the rare event  $A$ . This is motivated from the exact conditional distribution giving zero variance. Often, the asymptotics of the conditional distribution is found by using the theory of large deviations, as illustrated by the following example:

**Example 3** For a digital **barrier option**, the problem that arises (after some rewriting) is of estimating

$$z = \mathbb{P}(\underline{W}(T) \le -a, W(T) \ge b),$$
  
where  $\underline{W}(T) \stackrel{\text{def}}{=} \inf_{t \le T} W(t)$  (6)

and W is a Brownian motion with drift  $\mu$  and volatility  $\sigma$ , denoted as BM( $\mu, \sigma^2$ ). If a, b are not too close to 0, this is a rare-event problem: if  $\mu < 0$ , W is unlikely to go from  $-a$  to b, and if  $\mu > 0$ , W is unlikely to ever hit  $-a$ . It seems reasonable that on a fluid scale, the most likely path  $\varphi^*$  should be piecewise linear as in Figure 1.

The theory of large deviations suggests that finding  $\varphi^*$  is equivalent to a one-dimensional minimization problem, where one looks for the infimum over  $t$  of

![](_page_1_Figure_8.jpeg)

Figure 1 The most likely path for the barrier option

$$+ \int_{t}^{T} (c/(T-t) - \mu)^{2} \, \mathrm{d}s$$
$$= t(a/t + \mu)^{2} + (T-t)$$
$$\times (c/(T-t) - \mu)^{2} \tag{7}$$

where  $c = b + a$ , we have taken  $\sigma^2 = 1$ , and  $I(y) = (y - \mu)^2/2$  is the so-called rate function for BM( $\mu$ , 1) (see Large Deviations). Elementary calculus shows that the minimum is attained at  $t =$  $aT/(a+c)$ . This means that the most likely path is linear with slope  $-\mu^*$  on [0, t] and slope  $\mu^*$  on  $(t, T]$ where  $\mu^* = (a + 2b)/T$ , so that the above principle of approximating the conditional distribution given the rare event suggests that importance sampling may be done with the Brownian drift changed as stated.

This suggests simulating with drift  $-\mu^*$  until the time  $\tau$  where  $-a$  is hit and then changing the drift to  $\mu^*$ . Since the likelihood ratio for changing the drift from  $\mu$  to  $\widetilde{\mu}$  in an interval [0, t] is

$$Q = Q(t, \widetilde{\mu}) = \exp\{-(\widetilde{\mu} - \mu)W(t) + (\widetilde{\mu} - \mu)^2\}$$
(8)

and this formula extends to stopping times, it follows that the importance sampling estimator is

$$Q(\tau, -\mu^*)Q((T-\tau), \mu^*)$$
$$\times I(\underline{W}(T) \le -a, W(T) \ge b) \tag{9}$$

(modified to 0 if  $\tau > T$ ). For a further discussion, see [4, pp. 264 ff].

**Example 4** Glasserman *et al.* [5] considered a portfolio exposed to  $d$  (dependent) risk factors  $X_1, \ldots, X_d$  in a certain time horizon h, say, 1 day or 2 weeks. The initial value of the portfolio is denoted by  $v$  and the (discounted) terminal value by  $V = V(h, X_1, \dots, X_d)$ . The loss is  $L = v - V$ , and the VaR is a quantile of  $L$  (say the 99% one or the 99.97% one). It is assumed that  $X \sim \mathcal{N}(0, \Sigma)$ .

The basis of the algorithm is to invoke the *delta-gamma approximation*, which is based on the Taylor expansion

$$L \approx -\frac{\partial V}{\partial h}h - \sum_{i=1}^{d} \delta_i X_i - \frac{1}{2} \sum_{i,j=1}^{d} \gamma_{ij} X_i X_j \qquad (10)$$

where  $\delta_i \stackrel{\text{def}}{=} \partial V/\partial x_i$ ,  $\gamma_{ij} \stackrel{\text{def}}{=} \partial^2 V/\partial x_i \partial x_j$ . For brevity, we rewrite the right-hand side of equation  $(10)$ as  $a_0 + Q$ , where  $Q \stackrel{\text{def}}{=} -\boldsymbol{\delta} X - X^{\mathsf{T}} \boldsymbol{\Gamma} X/2$ ,  $a_0 \stackrel{\text{def}}{=}$  $-h \partial V/\partial h$ , and the proposal of Glasserman *et al.* [5] is to use the same exponential change of measure for the  $X_i$  as one would if the conditional distribution of Q, not L, was the target. Writing  $X = CY$  where the components of  $Y = (Y_1 \ldots Y^d)$  are i.i.d. standard normal, one can choose  $C$  to satisfy

$$Q = \sum_{i=1}^{d} (b_i Y_i + \lambda_i Y_i^2)$$
 (11)

and then it can be seen (e.g., [1] pp. 432-434) that under the conditional probability thus described,  $Y_1,\ldots,Y^d$  are still independent and Gaussian but have mean and variance parameters

$$\mu_i = \frac{\theta b_i}{1 - \theta \lambda_i}, \quad \sigma_i^2 = \frac{1}{1 - \theta \lambda_i} \n$$
(12)

where  $\theta$  is determined by

$$x - a_0 = \widetilde{\mathbb{E}}Q = \sum_{i=1}^d [b_i \mu_i + \lambda_i (\mu_i^2 + \sigma_i^2)] \quad (13)$$

Thus, the importance sampling estimator for estimating  $\mathbb{P}(L > \ell)$  is

$$I(L > \ell) \prod_{i=1}^{2} \frac{e^{-Y_i^2/2}/\sqrt{2\pi}}{e^{-(Y_i - \mu_i)^2/2\sigma_i^2}/\sqrt{2\pi\sigma_i^2}} \qquad (14)$$

The empirical finding of Glasserman et al. [5] is that this procedure typically (i.e., in the case of a number of selected test portfolios) reduces the variance by a factor of  $20-50$ .

The examples we have mentioned so far involve light tails. However, heavy tails are relevant particularly in areas such as credit risk and operational risk. The algorithms with heavy tails typically look completely different from those with light tails since exponential moments do not exist and hence exponential change of measure is impossible. We consider only the most important case of a tail  $\overline{F}(x)$  that is regularly varying, that is, of the form  $\overline{F}(x) = L(x)/x^{\alpha+1}$  with  $L(\cdot)$  slowly varying (e.g., a Pareto tail). Statistical tests for distinguishing between light and heavy tails based on i.i.d. observations  $X_1, \ldots, X_n$  from  $F$  are discussed

in [6] (see also [1] VI.4). A popular tool is the mean excess plot, where the mean of the observations exceeding  $x$  is plotted as function of  $x$ . For a heavy-tailed distribution, and not for a light-tailed one, one expects to see a function going to infinity. The standard tool for estimating  $\alpha$  in the regularly varying case is the so-called Hill estimator; see [1, 6].

**Example 5** Let  $X_1, \ldots, X_n$  be i.i.d. with regularly varying distribution F and  $S_N = X_1 + \cdots + X_N$ where  $N$  is fixed or an independent rv. The problem of estimating  $z(x) = \mathbb{P}(S_N > x)$  arises in a number of areas, for example, insurance risk, credit risk, and operational risk. The first efficient algorithms for this problem are remarkable in that they use conditional Monte Carlo, and not importance sampling. Currently, the most efficient of such algorithms uses the identity  $z(x) = n \mathbb{P}(S_n > x, M_n = X_n)$  (keeping  $N = n$  fixed and writing  $M_k = \max(X_1, \ldots, X_k)$ ; the intuition behind involving  $M_n$  is the fact that basically one  $X_i$  is large (and then equal to  $M_n$ ) when  $S_N > x$ . One then simulates  $X_1, \ldots, X_{n-1}$  and returns the conditional expectation

$$Z(x) = n\mathbb{P}(S > x, M_n = X_n | X_1, \dots, X_{n-1})$$
  
=  $n\overline{F}(\max(x - S_{n-1}, M_{n-1}))$  (15)

**Example 6** A very active area is dynamic importance sampling, where the importance distribution varies with time and current state (e.g., in the barrier option example one would let the Brownian drift depend on both time t and  $W(t)$ ). A principle for doing this is based upon Doob's  $h$  transform and requires that approximations for  $z(x)$  are available; see [1] VI.7. Some of the important recent examples  $\mathbf{v}$ in the heavy-tailed area are found in [2] and [3], and the approach seems to carry greater hope for generality than conditional Monte Carlo.

We conclude by mentioning some ideas beyond standard importance sampling (as exemplified above) that are relevant in the general area of rare-event simulation. An interesting recent development is the cross-entropy method [7], which performs an automatic search for a good importance distribution within a given parametric class. Another development is that of splitting the methods (cf [1] V.6, VI.9), where the rare event is decomposed as the intersection of events, each of which is nonrare.

## **End Notes**

a*.* Note that the parameter indexing the rare event is discrete in this example and hence denoted as *n* rather than *x*.

## **References**

- [1] Asmussen, S. & Glynn, P.W. (2007). *Stochastic Simulation. Algorithms and Analysis*, Springer-Verlag.
- [2] Blanchet, J. & Glynn, P.W. (2008/09). Efficient rareevent simulation for the maximum of heavy-tailed random walks, *Annals of Applied Probability* **18**, 1351–1378.
- [3] Dupuis, P., Leder, K. & Wang, H. (2007). Importance sampling for sums of random variables with heavy tails, *ACM TOMACS* **17**, 1–21.
- [4] Glasserman, P. (2004). *Monte Carlo Methods in Financial Engineering*, Springer-Verlag.
- [5] Glasserman, P., Heidelberger, P. & Shahabuddin, P. (2000). Variance reduction techniques for estimating value-at-risk, *Management Science* **46**, 1349–1364.
- [6] Resnick, S. (2007). *Heavy-Tailed Phenomena. Probabilistic and Statistical Modelling*, Springer-Verlag.
- [7] Rubinstein, R.Y. & Kroese, D.P. (2005). *The Cross-Entropy Method. A Unified Approach to Combinatorial Optimization, Simulation and Machine Learning*, Springer-Verlag.

## **Further Reading**

- Asmussen, S. & Rubinstein, R.Y. (1995). Steady–state rare events simulation in queueing models and its complexity properties, in *Advances in Queueing: Models, Methods and Problems*, J. Dshalalow, ed., CRC Press, pp. 429–466.
- Heidelberger, P. (1995). Fast simulation of rare events in queueing and reliability models, *ACM TOMACS* **6**, 43–85.
- Juneja, S. & Shahabuddin, P. (2006). Rare event simulation techniques, in *Simulation*, S.G. Henderson & B.L. Nelson, eds, Handbooks in Operations Research and Management Science, Elsevier, pp. 291–350.

## **Related Articles**

**Barrier Options**; **Heavy Tails**; **Large Deviations**; **Monte Carlo Simulation**; **Operational Risk**; **Saddlepoint Approximation**; **Value-at-Risk**; **Variance Reduction**.

SØREN ASMUSSEN