<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Rare-event Simulation
   </b>
  </h1>
  <p block-type="Text">
   In finance, the need for evaluation of the probability
   <math display="inline">
    \mathbb{P}(A)
   </math>
   that an event is rare (in the sense that
   <math display="inline">
    \mathbb{P}(A)
   </math>
   is small) arises, for example, in pricing out-ofthe-money options and Value-at-Risk (VaR) calculations in credit risk and operational risk. However, similar problems arise in a large variety of application areas: insurance risk (ruin probabilities and probabilities of large accumulated claims); communications engineering (probabilities of blocking, bit errors, packet loss, etc.); reliability (probabilities of unavailability in steady state or in a given time interval): and so on.
  </p>
  <p block-type="Text">
   In all of these examples, explicit evaluation of
   <math display="inline">
    \mathbb{P}(A)
   </math>
   is seldom possible unless unrealistically simple model assumptions have been made. Monte Carlo simulation is therefore one of the main tools in this area. However, the fact that
   <math display="inline">
    \mathbb{P}(A)
   </math>
   is small raises some specific problems.
  </p>
  <p block-type="TextInlineMath">
   By an estimator Z for
   <math display="inline">
    z = \mathbb{P}(A)
   </math>
   , we refer to a random variable
   <math display="inline">
    (rv)
   </math>
   Z that can be generated by simulation and satisfies
   <math display="inline">
    \mathbb{E}Z = z
   </math>
   . In practice, one simulates
   <math display="inline">
    R
   </math>
   (say) independent copies of
   <math display="inline">
    Z
   </math>
   and gives the estimate of z as the average
   <math display="inline">
    Z_R
   </math>
   , supplemented with a confidence interval, say, an equitailed 95% confidence interval
   <math display="inline">
    \widehat{Z}_R \pm w_R
   </math>
   where
   <math display="inline">
    s^2
   </math>
   is the empirical variance of the
   <math display="inline">
    R
   </math>
   simulated values of
   <math display="inline">
    Z
   </math>
   (the natural estimator of
   <math display="inline">
    \sigma_Z^2 = \mathbb{V}arZ
   </math>
   ) and
   <math display="inline">
    w_R = 1.96s/R^{1/2}
   </math>
   is the half-width. In most other Monte Carlo contexts, a small
   <math display="inline">
    w_R
   </math>
   is the relevant criterion for an estimator to be efficient. However, in a rare-event setting, one is rather interested in the relative halfwidth
   <math display="inline">
    w_R/\mathbb{P}(A)
   </math>
   . For example, the confidence interval
   <math display="inline">
    10^{-5} \pm 10^{-4}
   </math>
   may look narrow, but it does not help to tell whether z is of the magnitude
   <math display="inline">
    10^{-4}
   </math>
   ,
   <math display="inline">
    10^{-5}
   </math>
   , or even much smaller. Another way to illustrate the problem is in terms of the sample size
   <math display="inline">
    R
   </math>
   needed to acquire a given relative precision, say 10%, in terms of the half-width of the 95% confidence interval. This leads to the equation
   <math display="inline">
    1.96 \sigma_Z/(z\sqrt{R}) = 0.1
   </math>
   , that is,
  </p>
  <p block-type="Equation">
   <math display="block">
    R = \frac{100 \cdot 1.96^2 z (1-z)}{z^2} \sim \frac{100 \cdot 1.96^2}{z} \quad (1)
   </math>
  </p>
  <p block-type="TextInlineMath">
   which increases like
   <math display="inline">
    z^{-1}
   </math>
   as
   <math display="inline">
    z \downarrow 0
   </math>
   . Thus, if z is small, large sample sizes are required.
  </p>
  <p block-type="Text">
   The standard formal setup for discussing such efficiency concepts is to consider a family
   <math display="inline">
    \{A(x)\}\
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    x \in (0, \infty)
   </math>
   or
   <math display="inline">
    x \in \mathbb{N}
   </math>
   , assume that
   <math display="inline">
    z(x) \stackrel{\text{def}}{=}
   </math>
   <math display="inline">
    \mathbb{P}(A(x)) \to 0
   </math>
   as
   <math display="inline">
    x \to \infty
   </math>
   , and for each x let
   <math display="inline">
    Z(x)
   </math>
   be an unbiased estimator for
   <math display="inline">
    z(x)
   </math>
   , that is,
   <math display="inline">
    \mathbb{E}Z(x) = z(x)
   </math>
   . An algorithm is defined as a family
   <math display="inline">
    \{Z(x)\}\
   </math>
   of such rv's.
  </p>
  <p block-type="TextInlineMath">
   The best performance that has been observed in realistic rare-event settings is
   <i>
    bounded relative error
   </i>
   as
   <math display="inline">
    x \to \infty
   </math>
   , meaning
  </p>
  <p block-type="Equation">
   <math display="block">
    \limsup_{x \to \infty} \frac{\mathbb{V}ar\ Z(x)}{z(x)^2} \ &lt; \ \infty \tag{2}
   </math>
  </p>
  <p block-type="TextInlineMath">
   In particular, such an algorithm will have the feature that
   <math display="inline">
    R
   </math>
   as computed in equation (1), with
   <math display="inline">
    z(1-z)
   </math>
   replaced by
   <math display="inline">
    \mathbb{V}ar Z(x)
   </math>
   , remains bounded as
   <math display="inline">
    x \to \infty
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   An efficiency concept slightly weaker than equation (2) is
   <i>
    logarithmic efficiency
   </i>
   :
   <math display="inline">
    \forall ar(Z(x)) \rightarrow 0
   </math>
   so quickly that
  </p>
  <p block-type="Equation">
   <math display="block">
    \limsup_{x \to \infty} \frac{\sqrt[3]{ar \ Z(x)}}{z(x)^{2-\varepsilon}} = 0 \tag{3}
   </math>
  </p>
  <p block-type="TextInlineMath">
   for all
   <math display="inline">
    \varepsilon &gt; 0
   </math>
   , or, equivalently, that
  </p>
  <p block-type="Equation">
   <math display="block">
    \liminf_{x \to \infty} \frac{|\log \mathbb{V} ar \ Z(x)|}{|\log z(x)^2|} \ \geq \ 1 \tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   The most established method for producing estimators with such efficiency properties is
   <i>
    importance
   </i>
   sampling, where one simulates not from the physical measure
   <math display="inline">
    \mathbb{P}
   </math>
   but another probability measure
   <math display="inline">
    \mathbb{P}
   </math>
   with changed parameters. Then
   <math display="inline">
    Z = I(A)W
   </math>
   where
   <math display="inline">
    W = d\mathbb{P}/d\mathbb{P}
   </math>
   is the likelihood ratio. Here are two standard examples:
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 1
   </b>
   <math display="inline">
    A = I(S_n &gt; nm)
   </math>
   where
   <math display="inline">
    S_n = X_1 +
   </math>
   <math display="inline">
    \cdots + \bar{X}_n
   </math>
   &lt;sup&gt;a&lt;/sup&gt; with
   <math display="inline">
    X_1, X_2, \ldots
   </math>
   independent and identically distributed (i.i.d.) with density
   <math display="inline">
    f
   </math>
   and
   <math display="inline">
    m &gt;
   </math>
   <math display="inline">
    \mathbb{E}X
   </math>
   . Here one defines
   <math display="inline">
    \widetilde{f}(y) = e^{\theta y} f(y) / \mathbb{E} e^{\theta X}
   </math>
   with
   <math display="inline">
    \theta
   </math>
   chosen such that
   <math display="inline">
    \int y \widetilde{f}(y) dy = m
   </math>
   . The estimator becomes
  </p>
  <p block-type="Equation">
   <math display="block">
    Z(n) = I(S_n &gt; nm) \prod_{k=1}^{n} \frac{f(X_k)}{\widetilde{f}(X_k)}
   </math>
   (5)
  </p>
  <p block-type="Text">
   and is logarithmically efficient as
   <math display="inline">
    n \to \infty
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 2
   </b>
   <math display="inline">
    A = I(\max S_n &gt; x)
   </math>
   with
   <math display="inline">
    X_1, X_2, \ldots
   </math>
   as in (1) and
   <math display="inline">
    \mathbb{E}X &lt; 0
   </math>
   . Here one defines
   <math display="inline">
    \widetilde{f}(y) =
   </math>
   <math display="inline">
    e^{\theta y} f(y)
   </math>
   with
   <math display="inline">
    \theta &gt; 0
   </math>
   chosen such that
   <math display="inline">
    \int \widetilde{f}(y) dy = 1
   </math>
  </p>
  <p block-type="TextInlineMath">
   (this
   <math display="inline">
    \theta
   </math>
   is always unique and will exist for most standard light-tailed distributions). The estimator becomes
   <math display="inline">
    Z(x) = \exp\{-S_{\tau(x)}\}
   </math>
   where
   <math display="inline">
    \tau(x) = \inf\{n :
   </math>
   <math display="inline">
    S_n &gt; x
   </math>
   and has bounded relative error as
   <math display="inline">
    x \to \infty
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The definition of bounded relative error implies that in Example 2, the variance, compared to crude Monte Carlo, is reduced by a factor of order
   <math display="inline">
    z(x)
   </math>
   =
   <math display="inline">
    \mathbb{P}(A(x))
   </math>
   . Similar remarks apply to Example 1, though the variance reduction may be slightly smaller since there is only logarithmical efficiency there.
  </p>
  <p block-type="TextInlineMath">
   The principle behind the choice of
   <math display="inline">
    \mathbb{P}
   </math>
   in these two examples (exponential change of measure) is to take
   <math display="inline">
    \mathbb{P}
   </math>
   as an asymptotic conditional distribution given the rare event
   <math display="inline">
    A
   </math>
   . This is motivated from the exact conditional distribution giving zero variance. Often, the asymptotics of the conditional distribution is found by using the theory of large deviations, as illustrated by the following example:
  </p>
  <p block-type="Text">
   <b>
    Example 3
   </b>
   For a digital
   <b>
    barrier option
   </b>
   , the problem that arises (after some rewriting) is of estimating
  </p>
  <p block-type="Equation">
   <math display="block">
    z = \mathbb{P}(\underline{W}(T) \le -a, W(T) \ge b),
   </math>
   <br/>
   where
   <math>
    \underline{W}(T) \stackrel{\text{def}}{=} \inf_{t \le T} W(t)
   </math>
   (6)
  </p>
  <p block-type="TextInlineMath">
   and W is a Brownian motion with drift
   <math display="inline">
    \mu
   </math>
   and volatility
   <math display="inline">
    \sigma
   </math>
   , denoted as BM(
   <math display="inline">
    \mu, \sigma^2
   </math>
   ). If a, b are not too close to 0, this is a rare-event problem: if
   <math display="inline">
    \mu &lt; 0
   </math>
   , W is unlikely to go from
   <math display="inline">
    -a
   </math>
   to b, and if
   <math display="inline">
    \mu &gt; 0
   </math>
   , W is unlikely to ever hit
   <math display="inline">
    -a
   </math>
   . It seems reasonable that on a fluid scale, the most likely path
   <math display="inline">
    \varphi^*
   </math>
   should be piecewise linear as in Figure 1.
  </p>
  <p block-type="TextInlineMath">
   The theory of large deviations suggests that finding
   <math display="inline">
    \varphi^*
   </math>
   is equivalent to a one-dimensional minimization problem, where one looks for the infimum over
   <math display="inline">
    t
   </math>
   of
  </p>
  <p>
   <img src="_page_1_Figure_8.jpeg"/>
  </p>
  <p>
   Figure 1 The most likely path for the barrier option
  </p>
  <p block-type="Equation">
   <math display="block">
    + \int_{t}^{T} (c/(T-t) - \mu)^{2} \, \mathrm{d}s
   </math>
   <math display="block">
    = t(a/t + \mu)^{2} + (T-t)
   </math>
   <math display="block">
    \times (c/(T-t) - \mu)^{2} \tag{7}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    c = b + a
   </math>
   , we have taken
   <math display="inline">
    \sigma^2 = 1
   </math>
   , and
   <math display="inline">
    I(y) = (y - \mu)^2/2
   </math>
   is the so-called rate function for BM(
   <math display="inline">
    \mu
   </math>
   , 1) (see Large Deviations). Elementary calculus shows that the minimum is attained at
   <math display="inline">
    t =
   </math>
   <math display="inline">
    aT/(a+c)
   </math>
   . This means that the most likely path is linear with slope
   <math display="inline">
    -\mu^*
   </math>
   on [0, t] and slope
   <math display="inline">
    \mu^*
   </math>
   on
   <math display="inline">
    (t, T]
   </math>
   where
   <math display="inline">
    \mu^* = (a + 2b)/T
   </math>
   , so that the above principle of approximating the conditional distribution given the rare event suggests that importance sampling may be done with the Brownian drift changed as stated.
  </p>
  <p block-type="TextInlineMath">
   This suggests simulating with drift
   <math display="inline">
    -\mu^*
   </math>
   until the time
   <math display="inline">
    \tau
   </math>
   where
   <math display="inline">
    -a
   </math>
   is hit and then changing the drift to
   <math display="inline">
    \mu^*
   </math>
   . Since the likelihood ratio for changing the drift from
   <math display="inline">
    \mu
   </math>
   to
   <math display="inline">
    \widetilde{\mu}
   </math>
   in an interval [0, t] is
  </p>
  <p block-type="Equation">
   <math display="block">
    Q = Q(t, \widetilde{\mu}) = \exp\{-(\widetilde{\mu} - \mu)W(t) + (\widetilde{\mu} - \mu)^2\}
   </math>
   (8)
  </p>
  <p block-type="Text">
   and this formula extends to stopping times, it follows that the importance sampling estimator is
  </p>
  <p block-type="Equation">
   <math display="block">
    Q(\tau, -\mu^*)Q((T-\tau), \mu^*)
   </math>
   <math display="block">
    \times I(\underline{W}(T) \le -a, W(T) \ge b) \tag{9}
   </math>
  </p>
  <p block-type="TextInlineMath">
   (modified to 0 if
   <math display="inline">
    \tau &gt; T
   </math>
   ). For a further discussion, see [4, pp. 264 ff].
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 4
   </b>
   Glasserman
   <i>
    et al.
   </i>
   [5] considered a portfolio exposed to
   <math display="inline">
    d
   </math>
   (dependent) risk factors
   <math display="inline">
    X_1, \ldots, X_d
   </math>
   in a certain time horizon h, say, 1 day or 2 weeks. The initial value of the portfolio is denoted by
   <math display="inline">
    v
   </math>
   and the (discounted) terminal value by
   <math display="inline">
    V = V(h, X_1, \dots, X_d)
   </math>
   . The loss is
   <math display="inline">
    L = v - V
   </math>
   , and the VaR is a quantile of
   <math display="inline">
    L
   </math>
   (say the 99% one or the 99.97% one). It is assumed that
   <math display="inline">
    X \sim \mathcal{N}(0, \Sigma)
   </math>
   .
  </p>
  <p block-type="Text">
   The basis of the algorithm is to invoke the
   <i>
    delta-gamma approximation
   </i>
   , which is based on the Taylor expansion
  </p>
  <p block-type="Equation">
   <math display="block">
    L \approx -\frac{\partial V}{\partial h}h - \sum_{i=1}^{d} \delta_i X_i - \frac{1}{2} \sum_{i,j=1}^{d} \gamma_{ij} X_i X_j \qquad (10)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \delta_i \stackrel{\text{def}}{=} \partial V/\partial x_i
   </math>
   ,
   <math display="inline">
    \gamma_{ij} \stackrel{\text{def}}{=} \partial^2 V/\partial x_i \partial x_j
   </math>
   . For brevity, we rewrite the right-hand side of equation
   <math display="inline">
    (10)
   </math>
   as
   <math display="inline">
    a_0 + Q
   </math>
   , where
   <math display="inline">
    Q \stackrel{\text{def}}{=} -\boldsymbol{\delta} X - X^{\mathsf{T}} \boldsymbol{\Gamma} X/2
   </math>
   ,
   <math display="inline">
    a_0 \stackrel{\text{def}}{=}
   </math>
   <math display="inline">
    -h \partial V/\partial h
   </math>
   , and the proposal of Glasserman
   <i>
    et al.
   </i>
   [5] is to use the same exponential change of measure for the
   <math display="inline">
    X_i
   </math>
   as one would if the conditional distribution of Q, not L, was the target. Writing
   <math display="inline">
    X = CY
   </math>
   where the components of
   <math display="inline">
    Y = (Y_1 \ldots Y^d)
   </math>
   are i.i.d. standard normal, one can choose
   <math display="inline">
    C
   </math>
   to satisfy
  </p>
  <p block-type="Equation">
   <math display="block">
    Q = \sum_{i=1}^{d} (b_i Y_i + \lambda_i Y_i^2)
   </math>
   (11)
  </p>
  <p block-type="TextInlineMath">
   and then it can be seen (e.g., [1] pp. 432-434) that under the conditional probability thus described,
   <math display="inline">
    Y_1,\ldots,Y^d
   </math>
   are still independent and Gaussian but have mean and variance parameters
  </p>
  <p block-type="Equation">
   <math display="block">
    \mu_i = \frac{\theta b_i}{1 - \theta \lambda_i}, \quad \sigma_i^2 = \frac{1}{1 - \theta \lambda_i} \n
   </math>
   (12)
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    \theta
   </math>
   is determined by
  </p>
  <p block-type="Equation">
   <math display="block">
    x - a_0 = \widetilde{\mathbb{E}}Q = \sum_{i=1}^d [b_i \mu_i + \lambda_i (\mu_i^2 + \sigma_i^2)] \quad (13)
   </math>
  </p>
  <p block-type="Text">
   Thus, the importance sampling estimator for estimating
   <math display="inline">
    \mathbb{P}(L &gt; \ell)
   </math>
   is
  </p>
  <p block-type="Equation">
   <math display="block">
    I(L &gt; \ell) \prod_{i=1}^{2} \frac{e^{-Y_i^2/2}/\sqrt{2\pi}}{e^{-(Y_i - \mu_i)^2/2\sigma_i^2}/\sqrt{2\pi\sigma_i^2}} \qquad (14)
   </math>
  </p>
  <p block-type="Text">
   The empirical finding of Glasserman et al. [5] is that this procedure typically (i.e., in the case of a number of selected test portfolios) reduces the variance by a factor of
   <math display="inline">
    20-50
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The examples we have mentioned so far involve light tails. However, heavy tails are relevant particularly in areas such as credit risk and operational risk. The algorithms with heavy tails typically look completely different from those with light tails since exponential moments do not exist and hence exponential change of measure is impossible. We consider only the most important case of a tail
   <math display="inline">
    \overline{F}(x)
   </math>
   that is regularly varying, that is, of the form
   <math display="inline">
    \overline{F}(x) = L(x)/x^{\alpha+1}
   </math>
   with
   <math display="inline">
    L(\cdot)
   </math>
   slowly varying (e.g., a Pareto tail). Statistical tests for distinguishing between light and heavy tails based on i.i.d. observations
   <math display="inline">
    X_1, \ldots, X_n
   </math>
   from
   <math display="inline">
    F
   </math>
   are discussed
  </p>
  <p block-type="TextInlineMath">
   in [6] (see also [1] VI.4). A popular tool is the mean excess plot, where the mean of the observations exceeding
   <math display="inline">
    x
   </math>
   is plotted as function of
   <math display="inline">
    x
   </math>
   . For a heavy-tailed distribution, and not for a light-tailed one, one expects to see a function going to infinity. The standard tool for estimating
   <math display="inline">
    \alpha
   </math>
   in the regularly varying case is the so-called Hill estimator; see [1, 6].
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 5
   </b>
   Let
   <math display="inline">
    X_1, \ldots, X_n
   </math>
   be i.i.d. with regularly varying distribution F and
   <math display="inline">
    S_N = X_1 + \cdots + X_N
   </math>
   where
   <math display="inline">
    N
   </math>
   is fixed or an independent rv. The problem of estimating
   <math display="inline">
    z(x) = \mathbb{P}(S_N &gt; x)
   </math>
   arises in a number of areas, for example, insurance risk, credit risk, and operational risk. The first efficient algorithms for this problem are remarkable in that they use conditional Monte Carlo, and not importance sampling. Currently, the most efficient of such algorithms uses the identity
   <math display="inline">
    z(x) = n \mathbb{P}(S_n &gt; x, M_n = X_n)
   </math>
   (keeping
   <math display="inline">
    N = n
   </math>
   fixed and writing
   <math display="inline">
    M_k = \max(X_1, \ldots, X_k)
   </math>
   ; the intuition behind involving
   <math display="inline">
    M_n
   </math>
   is the fact that basically one
   <math display="inline">
    X_i
   </math>
   is large (and then equal to
   <math display="inline">
    M_n
   </math>
   ) when
   <math display="inline">
    S_N &gt; x
   </math>
   . One then simulates
   <math display="inline">
    X_1, \ldots, X_{n-1}
   </math>
   and returns the conditional expectation
  </p>
  <p block-type="Equation">
   <math display="block">
    Z(x) = n\mathbb{P}(S &gt; x, M_n = X_n | X_1, \dots, X_{n-1})
   </math>
   <br/>
   =
   <math>
    n\overline{F}(\max(x - S_{n-1}, M_{n-1}))
   </math>
   (15)
  </p>
  <p block-type="TextInlineMath">
   <b>
    Example 6
   </b>
   A very active area is dynamic importance sampling, where the importance distribution varies with time and current state (e.g., in the barrier option example one would let the Brownian drift depend on both time t and
   <math display="inline">
    W(t)
   </math>
   ). A principle for doing this is based upon Doob's
   <math display="inline">
    h
   </math>
   transform and requires that approximations for
   <math display="inline">
    z(x)
   </math>
   are available; see [1] VI.7. Some of the important recent examples
   <math display="inline">
    \mathbf{v}
   </math>
   in the heavy-tailed area are found in [2] and [3], and the approach seems to carry greater hope for generality than conditional Monte Carlo.
  </p>
  <p block-type="Text">
   We conclude by mentioning some ideas beyond standard importance sampling (as exemplified above) that are relevant in the general area of rare-event simulation. An interesting recent development is the cross-entropy method [7], which performs an automatic search for a good importance distribution within a given parametric class. Another development is that of splitting the methods (cf [1] V.6, VI.9), where the rare event is decomposed as the intersection of events, each of which is nonrare.
  </p>
  <h2>
   <b>
    End Notes
   </b>
  </h2>
  <p block-type="Text">
   a
   <i>
    .
   </i>
   Note that the parameter indexing the rare event is discrete in this example and hence denoted as
   <i>
    n
   </i>
   rather than
   <i>
    x
   </i>
   .
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [1] Asmussen, S. &amp; Glynn, P.W. (2007).
     <i>
      Stochastic Simulation. Algorithms and Analysis
     </i>
     , Springer-Verlag.
    </li>
    <li block-type="ListItem">
     [2] Blanchet, J. &amp; Glynn, P.W. (2008/09). Efficient rareevent simulation for the maximum of heavy-tailed random walks,
     <i>
      Annals of Applied Probability
     </i>
     <b>
      18
     </b>
     , 1351–1378.
    </li>
    <li block-type="ListItem">
     [3] Dupuis, P., Leder, K. &amp; Wang, H. (2007). Importance sampling for sums of random variables with heavy tails,
     <i>
      ACM TOMACS
     </i>
     <b>
      17
     </b>
     , 1–21.
    </li>
    <li block-type="ListItem">
     [4] Glasserman, P. (2004).
     <i>
      Monte Carlo Methods in Financial Engineering
     </i>
     , Springer-Verlag.
    </li>
    <li block-type="ListItem">
     [5] Glasserman, P., Heidelberger, P. &amp; Shahabuddin, P. (2000). Variance reduction techniques for estimating value-at-risk,
     <i>
      Management Science
     </i>
     <b>
      46
     </b>
     , 1349–1364.
    </li>
    <li block-type="ListItem">
     [6] Resnick, S. (2007).
     <i>
      Heavy-Tailed Phenomena. Probabilistic and Statistical Modelling
     </i>
     , Springer-Verlag.
    </li>
    <li block-type="ListItem">
     [7] Rubinstein, R.Y. &amp; Kroese, D.P. (2005).
     <i>
      The Cross-Entropy Method. A Unified Approach to Combinatorial Optimization, Simulation and Machine Learning
     </i>
     , Springer-Verlag.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    Further Reading
   </b>
  </h2>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     Asmussen, S. &amp; Rubinstein, R.Y. (1995). Steady–state rare events simulation in queueing models and its complexity properties, in
     <i>
      Advances in Queueing: Models, Methods and Problems
     </i>
     , J. Dshalalow, ed., CRC Press, pp. 429–466.
    </li>
    <li block-type="ListItem">
     Heidelberger, P. (1995). Fast simulation of rare events in queueing and reliability models,
     <i>
      ACM TOMACS
     </i>
     <b>
      6
     </b>
     , 43–85.
    </li>
    <li block-type="ListItem">
     Juneja, S. &amp; Shahabuddin, P. (2006). Rare event simulation techniques, in
     <i>
      Simulation
     </i>
     , S.G. Henderson &amp; B.L. Nelson, eds, Handbooks in Operations Research and Management Science, Elsevier, pp. 291–350.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    Related Articles
   </b>
  </h2>
  <p block-type="Text">
   <b>
    Barrier Options
   </b>
   ;
   <b>
    Heavy Tails
   </b>
   ;
   <b>
    Large Deviations
   </b>
   ;
   <b>
    Monte Carlo Simulation
   </b>
   ;
   <b>
    Operational Risk
   </b>
   ;
   <b>
    Saddlepoint Approximation
   </b>
   ;
   <b>
    Value-at-Risk
   </b>
   ;
   <b>
    Variance Reduction
   </b>
   .
  </p>
  <p block-type="Text">
   SØREN ASMUSSEN
  </p>
 </body>
</html>
