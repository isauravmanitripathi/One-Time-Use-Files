<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Credit Scoring
   </b>
  </h1>
  <p block-type="Text">
   Credit scoring models play a fundamental role in the risk management practice at most banks. Commercial banks' primary business activity is related to extending credit to borrowers and generating loans and credit assets. A significant component of a bank's risk, therefore, lies in the quality of its assets that needs to be in line with the bank's risk appetite.a To manage risk efficiently, quantifying it with the most appropriate and advanced tools is an extremely important factor in determining the bank's success.
  </p>
  <p block-type="Text">
   Credit risk models are used to quantify credit risk at counterparty or transaction level and they differ significantly by the nature of the counterparty (e.g., corporate, small business, private individual). Rating models have a long-term view (through the cycle) and have been always associated with corporate clients, financial institutions, and public sector (
   <i>
    see
   </i>
   <b>
    Credit Rating
   </b>
   ;
   <b>
    Counterparty Credit Risk
   </b>
   ). Scoring models, instead, focus more on the short term (point in time) and have been mainly applied to private individuals and, more recently, extended to small- and medium-sized enterprises (SMEs).b In this article, we focus on credit scoring models, giving an overview of their assessment, implementation, and usage.
  </p>
  <p block-type="Text">
   Since 1960s, larger organizations have been utilizing credit scoring to quickly and accurately assess the risk level of their prospects, applicants, and existing customers mainly in the consumer-lending business. Increasingly, midsize and smaller organizations are appreciating the benefits of credit scoring as well. The credit score is reflected in a number or letter(s) that summarizes the overall risk utilizing available information on the customer. Credit scoring models predict the probability that an applicant or existing borrower will default or become delinquent over a fixed time horizon.c The credit score empowers users to make quick decisions or even to automate decisions, and this is extremely desirable when banks are dealing with large volumes of clients and relatively small margin of profits at individual transaction level.
  </p>
  <p block-type="Text" class="has-continuation">
   Credit scoring models can be classified into three main categories: application, behavioral, and collection models, depending on the stage of the consumer credit cycle in which they are used. The main difference between them lies in the set of variables that are available to estimate the client's creditworthiness, that is, the earlier the stage in the credit cycle,
  </p>
  <p block-type="Text">
   the lower the number of specific client information available to the bank. This generally means that application models have a lower prediction power than behavioral and collection models.
  </p>
  <p block-type="Text">
   Over the last 50 years, several statistical methodologies have been used to build credit scoring models. The very simplistic univariate analysis applied at the beginning (late 1950s) was replaced as soon as academic research started to focus on credit scoring modeling techniques (late 1960s). The seminal works, in this field, of Beaver [10] and Altman [1] introduced the multivariate discriminant analysis (MDA) that became the most popular statistical methodology used to estimate credit scoring models until Ohlson [26], for the first time, applied the conditional logit model to the default prediction's study. Since Ohlson's research (early 1980s), several other statistical techniques have been utilized to improve the prediction power of credit scoring models (e.g., linear regression, probit analysis, Bayesian methods, neural network, etc.), but the logistic regression still remains the most popular method.
  </p>
  <p block-type="Text">
   Lately, credit scoring has gained new importance with the new Basel Capital Accord. The so-called Basel II replaces the current 1988 capital accord and focuses on techniques that allow banks and supervisors to properly evaluate the various risks that banks face (
   <i>
    see
   </i>
   <b>
    Internal-ratings-based Approach
   </b>
   ;
   <b>
    Regulatory Capital
   </b>
   ). Since credit scoring contributes broadly to the internal risk assessment process of an institution, regulators have enforced more strict rules about model development, implementation, and validation to be followed by banks that wish to use their internal models in order to estimate capital requirements.
  </p>
  <p block-type="Text">
   The remainder of the article is structured as follows. In the second section, we review some of the most relevant research related to credit scoring modeling methodologies. In the third section, following the model lifecycle structure, we analyze the main steps related to the model assessment, implementation, and validation process.
  </p>
  <p block-type="Text" class="has-continuation">
   The statistical techniques used for credit scoring are based on the idea of discrimination between several groups in a data sample. These procedures originated in the 1930s and 1940s of the previous century [18]. At that time, some of the finance houses and mail order firms were having difficulties with their credit management. Decision whether to give loans or send merchandise to the applicants was
  </p>
  <p block-type="Text">
   made judgmentally by credit analysts. The decision procedure was nonuniform, subjective, and opaque; it depended on the rules of each financial house and on the personal and empirical knowledge of each single clerk. With the rising number of people applying for a credit card, it was impossible to rely only on credit analysts; an automated system was necessary. The first consultancy was formed in San Francisco by Bill Fair and Earl Isaac in the late 1950s.
  </p>
  <p block-type="Text">
   After the first empirical solutions, academic interest on the topic rose and, given the lack of consumerlending figures, researchers focused their attention on small business clients. The seminal works in this field were Beaver [10] and Altman [1], who developed univariate and multivariate models, applying an MDA technique to predict business failures using a set of financial ratios.d
  </p>
  <p block-type="Text">
   For many years thereafter, MDA was the prevalent statistical technique applied to the default prediction models and it was used by many authors [2, 3, 13, 15, 16, 24, 29]. However, in most of these studies, authors pointed out that two basic assumptions of MDA are often violated when applied to the default prediction problems.e Moreover, in MDA models, the standardized coefficients cannot be interpreted such as the slopes of a regression equation and, hence, do not indicate the relative importance of the different variables. Considering these MDA's problems, Ohlson [26], for the first time, applied the conditional logit model to the default prediction's study.f The practical benefits of the logit methodology are that it does not require the restrictive assumptions of MDA and allows working with disproportional samples. The performance of his models, in terms of classification accuracy, was lower than the one reported in the previous studies based on MDA, but he pointed out some reasons to prefer the logistic analysis.
  </p>
  <p block-type="Text" class="has-continuation">
   From a statistical point of view, logit regression seems to fit well the characteristics of the default prediction problem, where the dependent variable is binary (default/nondefault) and with the groups being discrete, nonoverlapping, and identifiable. The logit model yields a score between 0 and 1, which conveniently can be transformed in the probability of default (PD) of the client. Lastly, the estimated coefficients can be interpreted separately as the importance or significance of each of the independent variables in the explanation of the estimated PD. After the work of Ohlson [26], most of the academic
  </p>
  <p block-type="Text">
   literature [5, 11, 19, 27, 30] used logit models to predict default.
  </p>
  <p block-type="Text">
   Several other statistical techniques have been tested to improve the prediction accuracy of credit scoring models (e.g., linear regression, probit analysis, Bayesian methods, neural network, etc.), but the empirical results have never shown really significant benefits.
  </p>
  <h2>
   <b>
    Credit Scoring Models Lifecycle
   </b>
  </h2>
  <p block-type="Text">
   As already mentioned, banks that want to implement the most advanced approach to calculate their minimum capital requirements (i.e., advanced internal rating based approach, A-IRB) are subject to more strict and common rules regarding how their internal models should be developed, implemented, and validated.g A standard model lifecycle has been designed to be followed by the financial institutions that will want to implement the A-IRB approach. The lifecycle of every model is divided into several phases (assessment, implementation, validation) and regulators have published specific requirements for each one of them. In this section, we describe the key aspects of each model's lifecycle phase.
  </p>
  <h4>
   <i>
    Model Assessment
   </i>
  </h4>
  <p block-type="Text">
   Credit scoring models are used to risk rank new or existing clients on the basis of the assumption that the future will be similar to the past. If an applicant or an existing client had a certain behavior in the past (e.g., paid back his debt or not), it is likely that a new applicant or client, with similar characteristics, will show the same behavior. As such, to develop a credit scoring model, we need a sample of past applicants or clients' data related to the same product as the one we want to use our scoring model for. If historical data from the bank are available, an empirical model can be developed. When banks do not have data or do not have a sufficient amount of data to develop an empirical model, an expert or a generic model is the most popular solution.h
  </p>
  <p block-type="Text" class="has-continuation">
   When a data sample covering the time horizon necessary for the statistical analysis (usually at least 1 year) is available, the performance of the clients inside the sample can be observed. We define performance as the default or nondefault event associated with each client.i This binary variable is the dependent variable used to run the regression analysis. The
  </p>
  <p block-type="Text">
   characteristics of the client at the beginning of the selected period are the predictors.
  </p>
  <p block-type="Text">
   Following the literature discussed in the second section, a conditional probability model, logit model, is commonly used by most banks to estimate the 1-year score through a range of variables by maximizing the log-likelihood function. This procedure is used to obtain the estimates of the parameters of the following logit model [20, 21]:
  </p>
  <p block-type="Equation">
   <math display="block">
    P_1(Xi) = \frac{1}{[1 + e^{-(B0 + B1Xi1 + B2Xi2 + \dots + BnXin)}]}
   </math>
   <math display="block">
    = \frac{1}{[1 + e^{-(Di)}]}
   </math>
   (1)
  </p>
  <p block-type="TextInlineMath">
   where
   <i>
    P
   </i>
   1
   <i>
    (Xi)
   </i>
   is the score given the vector of attributes
   <i>
    Xi
   </i>
   ;
   <i>
    Bj
   </i>
   is the coefficient of attribute
   <i>
    j
   </i>
   (with
   <i>
    j
   </i>
   = 1
   <i>
    ,...,n
   </i>
   );
   <i>
    B
   </i>
   <sup>
    0
   </sup>
   is the intercept;
   <i>
    Xij
   </i>
   is the value of the attribute
   <i>
    j
   </i>
   (with
   <i>
    j
   </i>
   = 1
   <i>
    ,...,n
   </i>
   ) for customer
   <i>
    I
   </i>
   ; and
   <i>
    Di
   </i>
   is the logit for customer
   <i>
    i
   </i>
   .
  </p>
  <p block-type="TextInlineMath">
   The logistic function implies that the logit score
   <i>
    P
   </i>
   <sup>
    1
   </sup>
   has a value in [0,1] interval and is increasing in
   <i>
    Di
   </i>
   . If
   <i>
    Di
   </i>
   approaches minus infinity,
   <i>
    P
   </i>
   <sup>
    1
   </sup>
   will be zero and if
   <i>
    Di
   </i>
   approaches plus infinity,
   <i>
    P
   </i>
   <sup>
    1
   </sup>
   will be one.
  </p>
  <p block-type="Text">
   The set of attributes that are used in the regression depends on the type of model that is going to be developed. Application models, employed to decide whether to accept or reject an applicant, typically rely only on personal information about the applicant, given the fact that this is usually the only information available to the bank at that stage.j Behavioral and collection models include variables describing the status of the relationship between the client and the bank that may add significant prediction power to the model.k
  </p>
  <p block-type="Text">
   Once the model is developed, it needs to be tested on a test sample to confirm the soundness of its results. When enough data are available, part of the development sample (hold-out sample) is usually kept for the final test of the model. However, an optimal test of the model would require investigating its performance also on an out-of-time and out-ofuniverse sample.
  </p>
  <h4>
   <i>
    Model Implementation
   </i>
  </h4>
  <p block-type="Text" class="has-continuation">
   The main advantage of scoring models is to allow banks to implement automated decision systems to manage their retail clients (private individuals and
  </p>
  <p block-type="Text">
   SMEs). When a large amount of applicants or clients is manually referred to credit analysts to check their information and apply policy rules, most of the benefits associated with the use of scoring models are lost. On the other hand, any scoring model has a "gray" area where it is not able to separate with an acceptable level of confidence between expected "good" clients and expected "bad" ones.l The main challenge for credit risk managers is to define the most appropriate and efficient thresholds (cutoff) for each scoring model.
  </p>
  <p block-type="Text">
   In order to maximize the benefits of a scoring model, the optimal cutoff should be set taking into account the misclassification costs related to the type I and type II error rates as Altman
   <i>
    et al.
   </i>
   [2], Taffler [29], and Koh [23] point out. Moreover, we believe that the optimum cutoff value cannot be found without a careful consideration of each particular bank peculiarities (e.g., tolerance for risk, profit–loss objectives, recovery process costs and efficiency, possible marketing strategies). Today, the most advanced banks set cutoffs using profitability analyses at account level.
  </p>
  <p block-type="Text">
   The availability of sophisticated IT systems has significantly broadened the number of strategies that can be implemented using credit scoring models. The most efficient banks are able to follow the lifecycle of any client, from the application to the end of the relationship, with monthly updated scores calculated by different scorecards related to the phase of the credit cycle where the client is located (e.g., origination, account maintenance, collection, write off). Marketing campaigns (e.g., cross-selling, up-selling), automated limit changes, early collection strategies, and shadow limit management are some of the activities that are fully driven by the output of scoring models in most banks.
  </p>
  <h4>
   <i>
    Model Validation
   </i>
  </h4>
  <p block-type="Text">
   Banks that have adopted or are willing to adopt the Basel II IRB-advanced approach are required to put in place a regular cycle of model validation that should include at least monitoring of the model performance and stability, reviewing of the model relationships, and testing of model outputs against outcomes (i.e., backtesting).m
  </p>
  <p block-type="Text" class="has-continuation">
   Considering the relatively short lifecycle of credit scoring models due to the high volatility of retail markets, their validation has always been completed
  </p>
  <p block-type="Text">
   by banks. Basel II has only given to it a more official shape, prescribing that the validation should be undertaken by a team independent from the one that has developed the models.
  </p>
  <p block-type="Text">
   Stability and performance (i.e., prediction accuracy) are extremely important information about the quality of the scoring models. As such, they should be tracked and analyzed at least monthly by banks, regardless of the validation exercise. As we have discussed above, often scoring models are used to generate a considerable amount of automated decisions that may have a significant impact on the banking business. Even small changes in the population's characteristics can substantially affect the quality of the models, creating undesired selection bias.
  </p>
  <p block-type="Text">
   In the literature, we have found several indexes that have been used to assess the performance of the models. The simple type I and type II error rates that quantify the accuracy of each model in correctly classifying defaulted and nondefaulted observations have been the first measures to be applied to scoring models. More recently, the accuracy ratio (AR) and the Gini index have become the most popular measures (see [17] for further details).
  </p>
  <p block-type="Text">
   Backtesting and benchmarking are an essential part of the scoring models' validation. With the backtesting, we evaluate the calibration and discrimination of a scoring model. Calibration refers to the mapping of a score to a quantitative risk measure (e.g., PD). A scoring model is considered well calibrated if the (
   <i>
    ex ante
   </i>
   ) estimated risk measures (PD) deviate only marginally from what has been observed
   <i>
    ex post
   </i>
   (actual default rate per score band). Discrimination measures how well the scoring model provides an ordinal ranking of the risk profile of the observations in the sample; for example, in the credit risk context, discrimination measures to what extent defaulters were assigned low scores and nondefaulters high scores.
  </p>
  <p block-type="Text">
   Benchmarking is another quantitative validation method that aims at assessing the consistency of the estimated scoring models with those obtained using other estimation techniques and potentially using other data sources. This analysis may be quite difficult to perform for retail portfolios, given the lack of generic benchmarks in the market.
   <sup>
    n
   </sup>
  </p>
  <p block-type="Text" class="has-continuation">
   Lastly, we would like to point out that Basel II specifically requires senior management to be fully involved and aware of the quality and performance
  </p>
  <p block-type="Text">
   of all the scoring models utilized in the daily business (see [9], par. 438, 439, 660, 718 (LXXVI), 728).
  </p>
  <h2>
   <b>
    End Notes
   </b>
  </h2>
  <p block-type="Text">
   a
   <i>
    .
   </i>
   Risk appetite is defined as the maximum risk the bank is willing to accept in executing its chosen business strategy, to protect itself against events that may have an adverse impact on its profitability, the capital base, or share price (
   <i>
    see
   </i>
   <b>
    Economic Capital Allocation
   </b>
   ;
   <b>
    Economic Capital
   </b>
   ).
  </p>
  <p block-type="Text">
   b
   <i>
    .
   </i>
   Recently, several studies [4, 12] have shown the importance for banks of classifying SMEs as retail clients and applying credit scoring models developed specifically for them.
  </p>
  <p block-type="Text">
   c
   <i>
    .
   </i>
   The default definition may be significantly different by bank and type of client. The new Basel Capital Accord [9] (par.452) has given a common definition of default (i.e., 90 days past due over 1-year horizon) that is consistently used by most banks today.
  </p>
  <p block-type="Text">
   d
   <i>
    .
   </i>
   The original
   <i>
    Z
   </i>
   -score model Altman [1] used five ratios: working capital/total assets, retained earnings/total assets, EBIT/total assets, market value equity/BV of total debt, and sales/total assets.
  </p>
  <p block-type="Text">
   e
   <i>
    .
   </i>
   MDA is based on two restrictive assumptions: (i) the independent variables included in the model are multivariate normally distributed and (ii) the group dispersion matrices (or variance–covariance matrices) are equal across the failing and the nonfailing group. See [6, 22, 25] for further discussions about this topic.
  </p>
  <p block-type="Text">
   f
   <i>
    .
   </i>
   Zmijewski [31] was the pioneer in applying probit analysis to predict default, but, until now, logit analysis has given better results in this field.
  </p>
  <p block-type="Text">
   g
   <i>
    .
   </i>
   The new Basel Capital Accord offers financial institutions the possibility to choose between the standardized and the advanced approach to calculate their capital requirements. Only the latter requires banks to use their own internal risk assessment tools to quantify the inputs of the capital requirements formulas (i.e., PD and loss given default).
  </p>
  <p block-type="Text">
   h
   <i>
    .
   </i>
   Expert scorecards are based on subjective weights assigned by an analyst, whereas generic scorecards are developed on pooled data from other banks operating in the same market. For a more detailed analysis of the possible solutions that banks can consider when not enough historical data is available, see [28].
  </p>
  <p block-type="Text">
   i
   <i>
    .
   </i>
   See end note (b).
  </p>
  <p block-type="Text">
   j
   <i>
    .
   </i>
   The most common application variables used are sociodemographic information about the applicants (e.g., marital status, residence type, time at current address, type of work, time at current work, flag phone, number of children, installment on income, etc.). When a credit bureau is available in the market, the information that can be obtained related to the behavior of the applicant with other financial institutions is an extremely powerful variable to be used in application models.
  </p>
  <p block-type="Text" class="has-continuation">
   k
   <i>
    .
   </i>
   Variables used in behavioral and collection scoring models are calculated and updated at least monthly. As such, the
  </p>
  <p block-type="Text">
   correlation between these variables and the default event is significantly high. Examples of behavioral variables are as follows: the number of missed installments (current, max last 3/6/12 months, or ever), number of days in excess (current, max last 3/6/12 months, or ever), outstanding on limit, and so on. Behavioral score can be calculated at facility and customer level (when several facilities are related to the same client).
  </p>
  <p block-type="Text">
   l
   <i>
    .
   </i>
   Depending on the chosen binary-dependent variable, "good" and "bad" will have different meanings. For credit risk models, these terms are usually associated with nondefaulted and defaulted clients, respectively.
  </p>
  <p block-type="TextInlineMath">
   m
   <i>
    .
   </i>
   See par. 417 and 718 (XCix) of the new Basel Capital Accord [7–9] (
   <i>
    see also
   </i>
   <b>
    Model Validation
   </b>
   ;
   <b>
    Backtesting
   </b>
   ). n
   <i>
    .
   </i>
   Recently, rating agencies (e.g., Standard &amp; Poor's and Moody's) and credit bureau providers (e.g., Fair Isaac and Experian) have started to offer services of benchmarking for retail scoring models. For more details about backtesting and benchmarking techniques, see [14].
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [1] Altman, E.I. (1968). Financial ratios, discriminant analysis and the prediction of corporate bankruptcy,
     <i>
      Journal of Finance
     </i>
     <b>
      23
     </b>
     (4), 589–611.
    </li>
    <li block-type="ListItem">
     [2] Altman, E.I., Haldeman, R.G. &amp; Narayanan, P. (1977). Zeta-analysis. A new model to identify bankruptcy risk of corporations,
     <i>
      Journal of Banking and Finance
     </i>
     <b>
      1
     </b>
     , 29–54.
    </li>
    <li block-type="ListItem">
     [3] Altman, E.I., Hartzell, J. &amp; Peck, M. (1995).
     <i>
      A Scoring System for Emerging Market Corporate Debt
     </i>
     . Salomon Brothers Emerging Markets Bond Research, May 15.
    </li>
    <li block-type="ListItem">
     [4] Altman, E.I. &amp; Sabato, G. (2005). Effects of the new Basel capital accord on bank capital requirements for SMEs,
     <i>
      Journal of Financial Services Research
     </i>
     <b>
      28
     </b>
     (1/3), 15–42.
    </li>
    <li block-type="ListItem">
     [5] Aziz, A., Emanuel, D.C. &amp; Lawson, G.H. (1988). Bankruptcy prediction – an investigation of cash flow based models,
     <i>
      Journal of Management Studies
     </i>
     <b>
      25
     </b>
     (5), 419–437.
    </li>
    <li block-type="ListItem">
     [6] Barnes, P. (1982). Methodological implications of nonnormality distributed financial ratios,
     <i>
      Journal of Business Finance and Accounting
     </i>
     <b>
      9
     </b>
     (1), 51–62.
    </li>
    <li block-type="ListItem">
     [7] Basel Committee on Banking Supervision (2005).
     <i>
      Studies on the Validation of Internal Rating Systems
     </i>
     . Working paper 14, www.bis.org.
    </li>
    <li block-type="ListItem">
     [8] Basel Committee on Banking Supervision (2005).
     <i>
      Update on Work of the Accord Implementation Group Related to Validation Under the Basel II Framework
     </i>
     . Newsletter 4, www.bis.org.
    </li>
    <li block-type="ListItem">
     [9] Basel Committee on Banking Supervision (2006).
     <i>
      International Convergence of Capital Measurement and Capital Standards
     </i>
     . www.bis.org.
    </li>
    <li block-type="ListItem">
     [10] Beaver, W. (1967). Financial ratios predictors of failure,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      4
     </b>
     , 71–111.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [11] Becchetti, L. &amp; Sierra, J. (2003). Bankruptcy risk and productive efficiency in manufacturing firms,
     <i>
      Journal of Banking and Finance
     </i>
     <b>
      27
     </b>
     (11), 2099–2120.
    </li>
    <li block-type="ListItem">
     [12] Berger, A.N. &amp; Frame, S.W. (2007). Small business credit scoring and credit availability,
     <i>
      Journal of Small Business Management
     </i>
     <b>
      45
     </b>
     (1), 5–22.
    </li>
    <li block-type="ListItem">
     [13] Blum, M. (1974). Failing company discriminant analysis,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      12
     </b>
     (1), 1–25.
    </li>
    <li block-type="ListItem">
     [14] Castermans, G., Martens, D., Van Gestel, T., Hamers, B. &amp; Baesens, B. (2007). An overview and framework for PD backtesting and benchmarking,
     <i>
      Proceedings of Credit Scoring and Credit Control X
     </i>
     , Edinburgh, Scotland.
    </li>
    <li block-type="ListItem">
     [15] Deakin, E. (1972). A discriminant analysis of predictors of business failure,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      10
     </b>
     (1), 167–179.
    </li>
    <li block-type="ListItem">
     [16] Edmister, R. (1972). An empirical test of financial ratio analysis for small business failure prediction,
     <i>
      Journal of Financial and Quantitative Analysis
     </i>
     <b>
      7
     </b>
     (2), 1477–1493.
    </li>
    <li block-type="ListItem">
     [17] Engelmann, B., Hayden, E. &amp; Tasche, D. (2003). Testing rating accuracy,
     <i>
      Risk
     </i>
     <b>
      16
     </b>
     (1), 82–86.
    </li>
    <li block-type="ListItem">
     [18] Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems,
     <i>
      Annals of Eugenic
     </i>
     <b>
      7
     </b>
     , 179–188.
    </li>
    <li block-type="ListItem">
     [19] Gentry, J.A., Newbold, P. &amp; Whitford, D.T. (1985). Classifying bankrupt firms with funds flow components,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      23
     </b>
     (1), 146–160.
    </li>
    <li block-type="ListItem">
     [20] Gujarati, N.D. (2003).
     <i>
      Basic Econometrics
     </i>
     , 4th Edition, McGraw-Hill, London.
    </li>
    <li block-type="ListItem">
     [21] Hosmer, D.W. &amp; Lemeshow, S. (2000).
     <i>
      Applied Logistic Regression
     </i>
     , 2nd Edition, John Wiley &amp; Sons, New York.
    </li>
    <li block-type="ListItem">
     [22] Karels, G.V. &amp; Prakash, A.J. (1987). Multivariate normality and forecasting of business bankruptcy,
     <i>
      Journal of Business Finance &amp; Accounting
     </i>
     <b>
      14
     </b>
     (4), 573–593.
    </li>
    <li block-type="ListItem">
     [23] Koh, H.C. (1992). The sensitivity of optimal cutoff points to misclassification costs of Type I and Type II errors in the going-concern prediction context,
     <i>
      Journal of Business Finance &amp; Accounting
     </i>
     <b>
      19
     </b>
     (2), 187–197.
    </li>
    <li block-type="ListItem">
     [24] Lussier, R.N. (1995). A non-financial business success versus failure prediction model for young firms,
     <i>
      Journal of Small Business Management
     </i>
     <b>
      33
     </b>
     (1), 8–20.
    </li>
    <li block-type="ListItem">
     [25] Mc Leay, S. &amp; Omar, A. (2000). The sensitivity of prediction models tot the non-normality of bounded an unbounded financial ratios,
     <i>
      British Accounting Review
     </i>
     <b>
      32
     </b>
     , 213–230.
    </li>
    <li block-type="ListItem">
     [26] Ohlson, J. (1980). Financial ratios and the probabilistic prediction of bankruptcy,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      18
     </b>
     (1), 109–131.
    </li>
    <li block-type="ListItem">
     [27] Platt, H.D. &amp; Platt, M.B. (1990). Development of a class of stable predictive variables: the case of bankruptcy prediction,
     <i>
      Journal of Business Finance &amp; Accounting
     </i>
     <b>
      17
     </b>
     (1), 31–51.
    </li>
    <li block-type="ListItem">
     [28] Sabato, G. (2008). Managing credit risk for retail lowdefault portfolios, in
     <i>
      Credit Risk: Models, Derivatives and Management
     </i>
     , N. Wagner, ed., Financial Mathematics Series, Chapman &amp; Hall/CRC.
    </li>
    <li block-type="ListItem">
     [29] Taffler, R.J. &amp; Tisshaw, H. (1977). Going, going, gone – four factors which predict,
     <i>
      Accountancy
     </i>
     <b>
      88
     </b>
     (1083), 50–54.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [30] Zavgren, C. (1983). The prediction of corporate failure: the state of the art,
     <i>
      Journal of Accounting Literature
     </i>
     <b>
      2
     </b>
     , 1–37.
    </li>
    <li block-type="ListItem">
     [31] Zmijewski, M.E. (1984). Methodological issues related to the estimation of financial distress prediction models,
     <i>
      Journal of Accounting Research
     </i>
     <b>
      22
     </b>
     , 59–86.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    Further Reading
   </b>
  </h2>
  <p block-type="Text">
   Taffler, R.J. (1982). Forecasting company failure in the UK using discriminant analysis and financial ratio data,
   <i>
    Journal of the Royal Statistical Society
   </i>
   <b>
    145
   </b>
   (3), 342–358.
  </p>
  <p block-type="Text">
   <b>
    Related Articles
   </b>
  </p>
  <p block-type="Text">
   <b>
    Backtesting
   </b>
   ;
   <b>
    Credit Rating
   </b>
   ;
   <b>
    Credit Risk
   </b>
   ;
   <b>
    Internalratings-based Approach
   </b>
   ;
   <b>
    Model Validation
   </b>
   .
  </p>
  <p block-type="Text">
   GABRIELE SABATO
  </p>
 </body>
</html>
