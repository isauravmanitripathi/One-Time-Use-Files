# **Nested Simulation**

Stochastic default-intensity ("reduced-form") models are widely applied in the pricing of single-name credit instruments such as credit default swap (CDS) and corporate bonds (*see* **Duffie–Singleton Model**; **Intensity-based Credit Risk Models**). Duffie and Garleanu [5] demonstrate how collateralized ˆ debt obligations (CDO) tranches may be priced using a multiname extension of the stochastic intensity framework (*see* **Multiname Reduced Form Models**). This article remains influential as a conceptual benchmark, but practitioners generally find the computational burden of this model prohibitive for real-time trading.a

Risk-management applications introduce additional challenges. Time constraints are less pressing than in trading applications, but the computational task may appear more formidable. When loss is measured on a mark-to-market basis, estimation *via* simulation of VaR and other risk measures calls for a nested procedure: In the outer step, one draws realizations of all default intensities up to the horizon, and in the inner step one uses simulation to reprice each instrument in the portfolio at the horizon conditional on the realized intensities. At first glance, simulation-based pricing algorithms would seem to be impractical in the inner step, because the inner pricing simulation must be executed for each trial in the outer step. This intuition is misleading because a relatively small number of trials in the inner step can suffice, particularly when the portfolio contains a large number of positions.

#### **Model Framework**

For simplicity in exposition, we consider a portfolio of *K* unfunded, synthetic CDO tranches.b The reference names in the CDO collateral pools are drawn from a universe consisting of *m* obligors. Each CDO tranche is defined by a set of variables *(At, Lt, , -, s, T )* where

• *At* is a vector *At* = *(a*1*t,...,amt)* of exposures in the CDO pool to each name in the underlying universe, expressed in currency units. Exposure is zero for names not included in a pool.

- *Lt* is the cumulative default loss since origination as of time *t*.
- and  are the original attachment and detachment points, respectively, for a tranche. These too are expressed in currency units. The residual face value of the tranche at time *t* is *Ft* ≡ max{*-*− *Lt,* 0} − max{ − *Lt,* 0}.
- *s* is the spread on the tranche. We assume that the CDO issuer pays the tranche holder a continuous stochastic premium of *sFt* .
- *T* is the maturity of the CDO.

Let *λjt* denote the stochastic default intensity for obligor *j* . The vector of default intensities is denoted by *t* . In models of multiname derivatives, such as CDOs and basket-default swaps, cross-sectional dependence in *t* is a central concern. For now, we simply assume that the joint process for *t* is specified under risk-neutral and physical measures and defaults are independent conditional on *t* (i.e., we rule out contagion and "frailty" in the sense of what is discussed in [6]). Conditional independence implies that default event risk is diversifiable, and so it should attract no risk premium in a large and efficient market. In this case, the risk-neutral intensity ˜ *<sup>t</sup>* equals the empirical intensity *t* at each moment *t*, even though the two processes evolve into the future under different laws [12].

To keep the focus on credit risk, we assume that risk-free interest rates are constant at *r*. In this case, the price of position *k* at time *t* is a memory-less function of *((t), Ak (t), Lk(t), k, k, sk, Tk* − *t)*.

## **Simulation of Value-at-Risk**

We now develop notation related to the simulation process. The simulation is nested: There is an "outer step" in which we draw histories up to the horizon *H*. For each trial in the outer step, there is an "inner step" simulation needed for repricing at the horizon. Loss is measured on a mark-to-market basis, inclusive of interim cash flows. We normalize the present time to zero and the model horizon is *H*.

Let *M* be the number of trials in the outer step. In each of these trials, we perform the following:

- 1. Draw a path for *(t )* for *t* = *(*0*, H*] under the physical measure.
- 2. Conditional on the {*λj (t)*}, draw default times *τj* up to *H* and sort them in increasing order. For

each default in  $(0, H]$ , the following steps are followed:

- Draw a recovery rate  $R_i$  for defaulted (a) obligor  $i$ .
- For each CDO with exposure to  $j$ , incre- $(b)$ ment cumulative loss  $L_k(t)$  at  $t = \tau_i$  by  $(1 - R_i)a_{k,i}(\tau_i)$  and decrement residual face value  $F_k(\tau_i)$  accordingly. Record the accrued value at  $H$  of the default-leg payment, that is,  $e^{r(H-\tau_j)}dF_k(\tau_j)$ .
- Adjust exposure vector  $A_k(t)$  for termina-(c) tion of exposure to the defaulted obligor.

The information set generated by the outer step trial, denoted by  $\xi$ , consists of  $\{\tau_i, R_i\}$  for obligors defaulting before time H and  $\{\lambda_i(H)\}\$ for the survivors.

- 3. Evaluate the accrued value at  $H$  of the premiumleg cash flows. $^{\rm c}$
- 4. Evaluate the price of each position at  $H$  with  $N$  "inner step" trials for each position. Paths for  $\Lambda(t)$  for  $t = (H, T_k]$  are simulated under the risk-neutral measure.
- 5. Discount prices and cash flows back to time zero and subtract from current prices to get portfolio loss  $Y(\xi)$ .

Observe that the full dependence structure across the portfolio is captured in the period up to the model horizon. Inner step simulations, in contrast, are run independently across positions. This is because the value of position  $k$  at time  $H$  is simply a conditional expectation (given  $\xi_H$  and under the risk-neutral measure) of its own subsequent cash flows, and does not depend on the future cash flows of other positions. Independent repricing implies that pricing errors are independent across the positions, and so they tend to diversify away at the portfolio level. Furthermore, when the positions are priced sequentially, to price tranche  $k$  we need to only draw joint paths for the obligors in the collateral pool for CDO  $k$ . This may greatly reduce the memory footprint of the simulation.

We now consider the problem of efficient estimation of VaR for  $Y$ . For a target insolvency probability  $\alpha$ , VaR is the value  $y_{\alpha}$  given by

$$y_{\alpha} = \text{VaR}_{\alpha}[Y] = \inf\{y : P(Y \le y) \ge 1 - \alpha\} \quad (1)$$

Under mild regularity conditions,  $Y$  is a continuous random variable so that  $P(Y \ge y_\alpha) = \alpha$ .

If we had analytical pricing formulae, then  $Y(\xi)$ would be a nonstochastic function of  $\xi$ , and simulation would involve generating i.i.d. samples  $Y(\xi_1), Y(\xi_2), \ldots, Y(\xi_M)$ . We would sort these draws as  $Y_{[1]} \geq \ldots \geq Y_{[M]}$ , so that  $Y_{\lceil \alpha M \rceil}$  provides an estimate of  $y_{[a]}$ , where  $[a]$  denotes the integer ceiling of the real number  $a$ .

In the absence of an analytical pricing formula,  $Y(\xi)$  is replaced by a noisy estimate  $\tilde{Y}(\xi)$ , which is obtained via the inner step simulations. In place of  $Y_{\lceil \alpha M \rceil}$ , we have the empirical quantile  $\tilde{Y}_{\lceil \alpha M \rceil}$  as our estimate of  $y_{\alpha}$ . Our interest is in characterizing the mean square error (MSE)  $E(Y_{\lceil \alpha M \rceil} - y_{\alpha})^2$ , and then minimizing it. We decompose MSE into variance and squared bias:

$$E[(\tilde{Y}_{\lceil \alpha M \rceil} - y_{\alpha})^2] = V[\tilde{Y}_{\lceil \alpha M \rceil}] + E[\tilde{Y}_{\lceil \alpha M \rceil} - y_{\alpha}]^2 \tag{2}$$

The variance is proportional to  $1/M$ , whereas the bias vanishes with  $1/N$ . It can be shown [11, 16] that

$$E\left[\tilde{Y}_{\lceil \alpha M \rceil}\right] - y_{\alpha} \approx \frac{\theta_{\alpha}}{Nf(y_{\alpha})} \tag{3}$$

where

$$\theta_{\alpha} = \frac{-1}{2} \frac{\mathrm{d}}{\mathrm{d}u} f(u) E\left[\sigma_{\xi}^{2} | Y = u\right] \bigg|_{u = y_{\alpha}} \tag{4}$$

and where  $\sigma_{\varepsilon}^2$  denotes the conditional variance of the mean-zero pricing error  $(\tilde{Y}(\xi) - Y(\xi))$  (conditioned on  $\xi$ ) and f is the density of Y. A result parallel to equation (4) appears in the literature on "granularity adjustment" of credit VaR to adjust asymptotic approximations of VaR for undiversified idiosyncratic risk [10, 14]. Except in contrived pathological cases, we will generally find  $\theta_{\alpha} > 0$ , so that  $\tilde{Y}_{\lceil \alpha M \rceil}$  is biased upward as an estimate of VaR.

We suppose that the overall computational budget is fixed at  $B$ , and choose  $(N, M)$  to minimize the MSE of the estimator  $\tilde{Y}_{\lceil \alpha M \rceil}$  subject to the budget constraint. Letting  $B \to \infty$ , we find that  $N^*$  grows in proportion to  $(\theta_{\alpha}^2 B)^{1/3}$  and  $M^*$  grows in proportion to  $(B/\theta_{\alpha})^{2/3}$ . That is, for large computational budgets,  $M^*$  grows with the square of  $N^*$ . Thus, marginal increments to  $B$  are allocated mainly to the outer step. It is easy to intuitively see the imbalance between  $N^*$  and  $M^*$ . Note that when N and M are of the same order  $\sqrt{B}$ , the squared bias term contributes much less to the MSE compared to the variance term. By increasing  $M$  at the expense of  $N$ , we reduce the variance till it matches up in contribution to the squared bias term.

As we increase the number of positions  $K$ , the conditional variance  $\sigma_{\varepsilon}^2$  falls, and  $\theta_{\alpha}$  falls proportionately. If the computation budget grows in proportion to K, then the optimal  $N^*$  falls to one, for largeenough  $K$  [11]. The intuition is that idiosyncratic pricing error is diversified away at the portfolio level, so a single inner step trial suffices.

#### **Importance Sampling**

By recentering the outer step simulation on the region of the state space in which large losses are more common, importance sampling (see Variance Reduction) can lead to orders of magnitude improvement in performance. Importance sampling has been applied to structural models of portfolio credit risk [2, 9], but the existing literature does not yet offer a welldeveloped importance sampling theory for reducedform models.<sup>d</sup> Bassamboo and Jain [1] did some initial work in this direction. They consider the case where the intensity  $\Lambda_t$  is an affine process. Observing that the intensity remains affine under a constant exponential twist, they developed an asymptotically efficient importance sampling change-of-measure to estimate the probability of a large number of defaults in a portfolio.

We now discuss two specifications for correlated intensity processes. Duffie and Gârleanu [5] consider the case where each intensity is the sum of common and idiosyncratic affine processes, that is,

$$\lambda_{i,t} = \lambda_t^* + \zeta_{i,t} \tag{5}$$

Here  $\lambda_t^*$  and  $\zeta_{i,t}$  for each *i* are mutually independent nonnegative affine processes. Under suitable parameter restrictions, the resultant process  $\lambda_{i,t}$  is also a nonnegative affine process. For moderately large pools, one expects that large pool losses are most likely to occur when the compensator of the common intensity process, that is,  $\int_0^H \lambda_t^* dt$ , is unusually large. When an exponential twist of  $\theta > 0$  is applied to this variable, the processes remain affine under the new measure and the likelihood ratio has a straightforward representation using results from [7]. If we wish to estimate the  $\alpha$  quantile of the overall loss distribution, we choose  $\theta$  so that the mean of  $\int_0^H \lambda_t^* dt$ under the new measure equals its  $\alpha$  quantile under the original measure. The latter is unknown at the beginning of the simulation, but may be adaptively learnt as the simulation proceeds.

The Black-Karasinski specification is increasingly popular for modeling single-name default intensities [15] and is readily extended to multiname models. Let

$$d \log \lambda_i(t) = \kappa_i(\mu_i - \log \lambda_i(t)) dt + \sigma_i dW_i(t) \quad (6)$$

where  $\kappa_i$ ,  $\mu_i$ , and  $\sigma_i$  are constants. The diffusion  $W_i(t)$  is decomposed into common and idiosyncratic components

$$W_i(t) = \sqrt{\rho_i} W^*(t) + \sqrt{1 - \rho_i} V_i(t) \tag{7}$$

where  $W^*(t)$  and  $V_i(t)$  for each *i* are mutually independent standard Brownian motions and  $\rho_i$  is a constant. An importance sampling scheme that alters the drift of  $W^*(t)$  is straightforward to implement.

#### Discussion

These methods for nested simulation apply in a much more general setting. We could have structural or ratings-based pricing models in place of the stochastic intensity models, which could introduce stochastic interest rates, and allow for long and short positions. Indeed, we can allow for general derivative portfolios (not just portfolios of CDOs), in which case the intensities  $\Lambda(t)$  are replaced by a vector of state variables that could include interest rates, commodity prices, equity prices, and so on. The asymptotic allocation of workload between outer and inner steps remains unchanged. Furthermore, the same analysis applies to estimation of expected shortfall and large loss probabilities, to pricing of compound options, and to the credit rating of structured products under parameter uncertainty.

Optimal allocation schemes and important sampling can be combined and extended in a variety of ways. First, for senior tranches of CDOs in particular, importance sampling within the inner step pricing (as well as the outer step) may offer large efficiency gains. Second, a *jackknife* procedure in the inner step simulation can be used to eliminate the  $1/N$  order term in the bias [11]. Third, a scheme for dynamic *allocation* [11] can easily be implemented. An initial estimate of loss  $\tilde{Y}^n(\xi)$  is obtained for a given  $\xi$ from a small inner step sample of *n* trials. If  $\tilde{Y}^n(\xi)$  is large (and therefore in the neighborhood of VaR), the estimate is refined through drawing additional samples of the inner step. Similar to this is a *screening* and restarting scheme [3, 13] developed for expected shortfall and other coherent risk measures. In the first phase, an initial sample of *M* outer step trials is obtained using a small number of inner step samples. These samples are screened or filtered to pick out the large loss samples that are likely to contribute to the expected shortfall. In the second phase, for the shortlisted samples, inner steps are resampled to improve the statistical properties of the resultant estimator.

These various refinements can alter the trade-off between bias and variance, so that optimal *N*<sup>∗</sup> may grow at a slower or faster rate with the budget. Nonetheless, the essential lesson of the analysis is robust: in a large portfolio VaR setting, inner step pricing simulations can be run with few trials. Despite the high likelihood of grotesque pricing errors at the instrument level, the impact on estimated VaR is small and can be controlled.

#### **Acknowledgments**

The opinions expressed here are our own, and do not reflect the views of the Board of Governors or its staff.

#### **End Notes**

a*.* Eckner [8] develops a semianalytic algorithm for [5] under somewhat restrictive assumptions.

b*.* Observe that a single-name CDS can be represented as a special case of such a tranche. With some additional notation, it would be straightforward to accommodate corporate bonds, cash flow CDOs, and other credit instruments.

c*.* The implicit assumption here is that interim cash flows are reinvested in the money market until time *H*, but other conventions are easily accommodated.

d*.* Chiang *et al.* [4] develop an efficient importance sampling technique for pricing basket-default swaps in a Gaussian copula model of times to default.

#### **References**

- [1] Bassamboo, A. & Jain, S. (2006). Efficient importance sampling for reduced form models in credit risk, in L.F. Perrone, B.G. Lawson, J. Liu & F.P. Wieland, eds, *Proceedings of the 2006 Winter Simulation Conference*, IEEE Press, Piscataway, NJ, pp. 741–748.
- [2] Bassamboo, A., Juneja, S. & Zeevi, A. (2008). Portfolio credit risk with extremal dependence: asymptotic analysis and efficient simulation, *Operations Research* **56**(3), 593–606.
- [3] Boesel, J., Nelson, B.L. & Kim, S.-H. (2003). Using ranking and selection to "clean up" after simulation optimization, *Operations Research* **51**(5), 814–825.

- [4] Chiang, M.-H., Yueh, M.-L. & Hsieh, M.-H. (2007). An efficient algorithm for basket default swap valuation, *Journal of Derivatives* Winter, 8–19.
- [5] Duffie, D. & Garleanu, N. (2001). Risk and valuation ˆ of collateralized debt obligations, *Financial Analysts Journal* **57**(1), 41–59.
- [6] Duffie, D., Eckner, A., Horel, G. & Saita, L. (2009). Frailty correlated default, *Journal of Finance* **64**(5), 2087–2122.
- [7] Duffie, D., Pan, J. & Singleton, K.J. (2000). Transform analysis and asset pricing for affine jump diffusions, *Econometrica* **68**, 1343–1376.
- [8] Eckner, A. (2009). Computational techniques for basic affine models of portfolio credit risk, *Journal of Computational Finance* **13**(1), 1–35.
- [9] Glasserman, P. & Li, J. (2005). Importance sampling for portfolio credit risk, *Management Science* **51**(11), 1643–1656.
- [10] Gordy, M.B. (2004). Granularity adjustment in portfolio credit risk measurement, in G.P. Szego, ed., Risk Measures for the 21st Century, John Wiley & Sons.
- [11] Gordy, M.B. & Juneja, S. (2008). *Nested Simulation in Portfolio Risk Measurement*. FEDS 2008-21, Federal Reserve Board, April 2008.
- [12] Lando, D., Jarrow, R.A. & Yu, F. (2005). Default risk and diversification: theory and empirical implications, *Mathematical Finance* **15**(1), 1–26.
- [13] Lesnevski, V., Nelson, B.L. & Staum, J. (2008). An adaptive procedure for estimating coherent risk measures based on generalized scenarios, *Journal of Computational Finance* **11**(4), 1–31.
- [14] Martin, R. & Wilde, T. (2002). Unsystematic credit risk, *Risk* **15**(11), 123–128.
- [15] Pan, J. & Singleton, K.J. (2008). Default and recovery implicit in the term structure of sovereign CDS spreads, *Journal of Finance* **63**(5), 2345–2384.
- [16] Shing-Hoi, L. (1998). *Monte Carlo Computation of Conditional Expectation Quantiles*. PhD thesis, Stanford University.

### **Further Reading**

Gordy, M.B. & Juneja, S. (2006). Efficient simulation for risk measurement in a portfolio of CDOs, in L.F. Perrone, B.G. Lawson, J. Liu & F.P. Wieland, eds, *Proceedings of the 2006 Winter Simulation Conference*, IEEE Press, Piscataway, NJ.

#### **Related Articles**

**Credit Portfolio Simulation**; **Credit Risk**; **Large Pool Approximations**; **Monte Carlo Simulation**; **Multiname Reduced Form Models**; **Value-at-Risk**; **Variance Reduction**.

MICHAEL B. GORDY & SANDEEP JUNEJA