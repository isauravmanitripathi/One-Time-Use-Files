<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Nested Simulation
   </b>
  </h1>
  <p block-type="Text">
   Stochastic default-intensity ("reduced-form") models are widely applied in the pricing of single-name credit instruments such as credit default swap (CDS) and corporate bonds (
   <i>
    see
   </i>
   <b>
    Duffie–Singleton Model
   </b>
   ;
   <b>
    Intensity-based Credit Risk Models
   </b>
   ). Duffie and Garleanu [5] demonstrate how collateralized ˆ debt obligations (CDO) tranches may be priced using a multiname extension of the stochastic intensity framework (
   <i>
    see
   </i>
   <b>
    Multiname Reduced Form Models
   </b>
   ). This article remains influential as a conceptual benchmark, but practitioners generally find the computational burden of this model prohibitive for real-time trading.a
  </p>
  <p block-type="Text">
   Risk-management applications introduce additional challenges. Time constraints are less pressing than in trading applications, but the computational task may appear more formidable. When loss is measured on a mark-to-market basis, estimation
   <i>
    via
   </i>
   simulation of VaR and other risk measures calls for a nested procedure: In the outer step, one draws realizations of all default intensities up to the horizon, and in the inner step one uses simulation to reprice each instrument in the portfolio at the horizon conditional on the realized intensities. At first glance, simulation-based pricing algorithms would seem to be impractical in the inner step, because the inner pricing simulation must be executed for each trial in the outer step. This intuition is misleading because a relatively small number of trials in the inner step can suffice, particularly when the portfolio contains a large number of positions.
  </p>
  <h4>
   <b>
    Model Framework
   </b>
  </h4>
  <p block-type="Text">
   For simplicity in exposition, we consider a portfolio of
   <i>
    K
   </i>
   unfunded, synthetic CDO tranches.b The reference names in the CDO collateral pools are drawn from a universe consisting of
   <i>
    m
   </i>
   obligors. Each CDO tranche is defined by a set of variables
   <i>
    (At, Lt, , -, s, T )
   </i>
   where
  </p>
  <p block-type="Text">
   •
   <i>
    At
   </i>
   is a vector
   <i>
    At
   </i>
   =
   <i>
    (a
   </i>
   1
   <i>
    t,...,amt)
   </i>
   of exposures in the CDO pool to each name in the underlying universe, expressed in currency units. Exposure is zero for names not included in a pool.
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     <i>
      Lt
     </i>
     is the cumulative default loss since origination as of time
     <i>
      t
     </i>
     .
    </li>
    <li block-type="ListItem">
     and
     <i>
     </i>
     are the original attachment and detachment points, respectively, for a tranche. These too are expressed in currency units. The residual face value of the tranche at time
     <i>
      t
     </i>
     is
     <i>
      Ft
     </i>
     ≡ max{
     <i>
      -
     </i>
     −
     <i>
      Lt,
     </i>
     0} − max{ −
     <i>
      Lt,
     </i>
     0}.
    </li>
    <li block-type="ListItem">
     <i>
      s
     </i>
     is the spread on the tranche. We assume that the CDO issuer pays the tranche holder a continuous stochastic premium of
     <i>
      sFt
     </i>
     .
    </li>
    <li block-type="ListItem">
     <i>
      T
     </i>
     is the maturity of the CDO.
    </li>
   </ul>
  </p>
  <p block-type="TextInlineMath">
   Let
   <i>
    λjt
   </i>
   denote the stochastic default intensity for obligor
   <i>
    j
   </i>
   . The vector of default intensities is denoted by
   <i>
    t
   </i>
   . In models of multiname derivatives, such as CDOs and basket-default swaps, cross-sectional dependence in
   <i>
    t
   </i>
   is a central concern. For now, we simply assume that the joint process for
   <i>
    t
   </i>
   is specified under risk-neutral and physical measures and defaults are independent conditional on
   <i>
    t
   </i>
   (i.e., we rule out contagion and "frailty" in the sense of what is discussed in [6]). Conditional independence implies that default event risk is diversifiable, and so it should attract no risk premium in a large and efficient market. In this case, the risk-neutral intensity ˜
   <i>
    <sup>
     t
    </sup>
   </i>
   equals the empirical intensity
   <i>
    t
   </i>
   at each moment
   <i>
    t
   </i>
   , even though the two processes evolve into the future under different laws [12].
  </p>
  <p block-type="TextInlineMath">
   To keep the focus on credit risk, we assume that risk-free interest rates are constant at
   <i>
    r
   </i>
   . In this case, the price of position
   <i>
    k
   </i>
   at time
   <i>
    t
   </i>
   is a memory-less function of
   <i>
    ((t), Ak (t), Lk(t), k, k, sk, Tk
   </i>
   −
   <i>
    t)
   </i>
   .
  </p>
  <h2>
   <b>
    Simulation of Value-at-Risk
   </b>
  </h2>
  <p block-type="Text">
   We now develop notation related to the simulation process. The simulation is nested: There is an "outer step" in which we draw histories up to the horizon
   <i>
    H
   </i>
   . For each trial in the outer step, there is an "inner step" simulation needed for repricing at the horizon. Loss is measured on a mark-to-market basis, inclusive of interim cash flows. We normalize the present time to zero and the model horizon is
   <i>
    H
   </i>
   .
  </p>
  <p block-type="Text">
   Let
   <i>
    M
   </i>
   be the number of trials in the outer step. In each of these trials, we perform the following:
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     1. Draw a path for
     <i>
      (t )
     </i>
     for
     <i>
      t
     </i>
     =
     <i>
      (
     </i>
     0
     <i>
      , H
     </i>
     ] under the physical measure.
    </li>
    <li block-type="ListItem">
     2. Conditional on the {
     <i>
      λj (t)
     </i>
     }, draw default times
     <i>
      τj
     </i>
     up to
     <i>
      H
     </i>
     and sort them in increasing order. For
    </li>
   </ul>
  </p>
  <p block-type="Text">
   each default in
   <math display="inline">
    (0, H]
   </math>
   , the following steps are followed:
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     Draw a recovery rate
     <math display="inline">
      R_i
     </math>
     for defaulted (a) obligor
     <math display="inline">
      i
     </math>
     .
    </li>
    <li block-type="ListItem">
     For each CDO with exposure to
     <math display="inline">
      j
     </math>
     , incre-
     <math display="inline">
      (b)
     </math>
     ment cumulative loss
     <math display="inline">
      L_k(t)
     </math>
     at
     <math display="inline">
      t = \tau_i
     </math>
     by
     <math display="inline">
      (1 - R_i)a_{k,i}(\tau_i)
     </math>
     and decrement residual face value
     <math display="inline">
      F_k(\tau_i)
     </math>
     accordingly. Record the accrued value at
     <math display="inline">
      H
     </math>
     of the default-leg payment, that is,
     <math display="inline">
      e^{r(H-\tau_j)}dF_k(\tau_j)
     </math>
     .
    </li>
    <li block-type="ListItem">
     Adjust exposure vector
     <math display="inline">
      A_k(t)
     </math>
     for termina-(c) tion of exposure to the defaulted obligor.
    </li>
   </ul>
  </p>
  <p block-type="TextInlineMath">
   The information set generated by the outer step trial, denoted by
   <math display="inline">
    \xi
   </math>
   , consists of
   <math display="inline">
    \{\tau_i, R_i\}
   </math>
   for obligors defaulting before time H and
   <math display="inline">
    \{\lambda_i(H)\}\
   </math>
   for the survivors.
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     3. Evaluate the accrued value at
     <math display="inline">
      H
     </math>
     of the premiumleg cash flows.
     <math display="inline">
      ^{\rm c}
     </math>
    </li>
    <li block-type="ListItem">
     4. Evaluate the price of each position at
     <math display="inline">
      H
     </math>
     with
     <math display="inline">
      N
     </math>
     "inner step" trials for each position. Paths for
     <math display="inline">
      \Lambda(t)
     </math>
     for
     <math display="inline">
      t = (H, T_k]
     </math>
     are simulated under the risk-neutral measure.
    </li>
    <li block-type="ListItem">
     5. Discount prices and cash flows back to time zero and subtract from current prices to get portfolio loss
     <math display="inline">
      Y(\xi)
     </math>
     .
    </li>
   </ul>
  </p>
  <p block-type="Text">
   Observe that the full dependence structure across the portfolio is captured in the period up to the model horizon. Inner step simulations, in contrast, are run independently across positions. This is because the value of position
   <math display="inline">
    k
   </math>
   at time
   <math display="inline">
    H
   </math>
   is simply a conditional expectation (given
   <math display="inline">
    \xi_H
   </math>
   and under the risk-neutral measure) of its own subsequent cash flows, and does not depend on the future cash flows of other positions. Independent repricing implies that pricing errors are independent across the positions, and so they tend to diversify away at the portfolio level. Furthermore, when the positions are priced sequentially, to price tranche
   <math display="inline">
    k
   </math>
   we need to only draw joint paths for the obligors in the collateral pool for CDO
   <math display="inline">
    k
   </math>
   . This may greatly reduce the memory footprint of the simulation.
  </p>
  <p block-type="TextInlineMath">
   We now consider the problem of efficient estimation of VaR for
   <math display="inline">
    Y
   </math>
   . For a target insolvency probability
   <math display="inline">
    \alpha
   </math>
   , VaR is the value
   <math display="inline">
    y_{\alpha}
   </math>
   given by
  </p>
  <p block-type="Equation">
   <math display="block">
    y_{\alpha} = \text{VaR}_{\alpha}[Y] = \inf\{y : P(Y \le y) \ge 1 - \alpha\} \quad (1)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Under mild regularity conditions,
   <math display="inline">
    Y
   </math>
   is a continuous random variable so that
   <math display="inline">
    P(Y \ge y_\alpha) = \alpha
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   If we had analytical pricing formulae, then
   <math display="inline">
    Y(\xi)
   </math>
   would be a nonstochastic function of
   <math display="inline">
    \xi
   </math>
   , and simulation would involve generating i.i.d. samples
   <math display="inline">
    Y(\xi_1), Y(\xi_2), \ldots, Y(\xi_M)
   </math>
   . We would sort these draws as
   <math display="inline">
    Y_{[1]} \geq \ldots \geq Y_{[M]}
   </math>
   , so that
   <math display="inline">
    Y_{\lceil \alpha M \rceil}
   </math>
   provides an estimate of
   <math display="inline">
    y_{[a]}
   </math>
   , where
   <math display="inline">
    [a]
   </math>
   denotes the integer ceiling of the real number
   <math display="inline">
    a
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   In the absence of an analytical pricing formula,
   <math display="inline">
    Y(\xi)
   </math>
   is replaced by a noisy estimate
   <math display="inline">
    \tilde{Y}(\xi)
   </math>
   , which is obtained via the inner step simulations. In place of
   <math display="inline">
    Y_{\lceil \alpha M \rceil}
   </math>
   , we have the empirical quantile
   <math display="inline">
    \tilde{Y}_{\lceil \alpha M \rceil}
   </math>
   as our estimate of
   <math display="inline">
    y_{\alpha}
   </math>
   . Our interest is in characterizing the mean square error (MSE)
   <math display="inline">
    E(Y_{\lceil \alpha M \rceil} - y_{\alpha})^2
   </math>
   , and then minimizing it. We decompose MSE into variance and squared bias:
  </p>
  <p block-type="Equation">
   <math display="block">
    E[(\tilde{Y}_{\lceil \alpha M \rceil} - y_{\alpha})^2] = V[\tilde{Y}_{\lceil \alpha M \rceil}] + E[\tilde{Y}_{\lceil \alpha M \rceil} - y_{\alpha}]^2 \tag{2}
   </math>
  </p>
  <p block-type="Text">
   The variance is proportional to
   <math display="inline">
    1/M
   </math>
   , whereas the bias vanishes with
   <math display="inline">
    1/N
   </math>
   . It can be shown [11, 16] that
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\tilde{Y}_{\lceil \alpha M \rceil}\right] - y_{\alpha} \approx \frac{\theta_{\alpha}}{Nf(y_{\alpha})} \tag{3}
   </math>
  </p>
  <p block-type="Text">
   where
  </p>
  <p block-type="Equation">
   <math display="block">
    \theta_{\alpha} = \frac{-1}{2} \frac{\mathrm{d}}{\mathrm{d}u} f(u) E\left[\sigma_{\xi}^{2} | Y = u\right] \bigg|_{u = y_{\alpha}} \tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   and where
   <math display="inline">
    \sigma_{\varepsilon}^2
   </math>
   denotes the conditional variance of the mean-zero pricing error
   <math display="inline">
    (\tilde{Y}(\xi) - Y(\xi))
   </math>
   (conditioned on
   <math display="inline">
    \xi
   </math>
   ) and f is the density of Y. A result parallel to equation (4) appears in the literature on "granularity adjustment" of credit VaR to adjust asymptotic approximations of VaR for undiversified idiosyncratic risk [10, 14]. Except in contrived pathological cases, we will generally find
   <math display="inline">
    \theta_{\alpha} &gt; 0
   </math>
   , so that
   <math display="inline">
    \tilde{Y}_{\lceil \alpha M \rceil}
   </math>
   is biased upward as an estimate of VaR.
  </p>
  <p block-type="TextInlineMath">
   We suppose that the overall computational budget is fixed at
   <math display="inline">
    B
   </math>
   , and choose
   <math display="inline">
    (N, M)
   </math>
   to minimize the MSE of the estimator
   <math display="inline">
    \tilde{Y}_{\lceil \alpha M \rceil}
   </math>
   subject to the budget constraint. Letting
   <math display="inline">
    B \to \infty
   </math>
   , we find that
   <math display="inline">
    N^*
   </math>
   grows in proportion to
   <math display="inline">
    (\theta_{\alpha}^2 B)^{1/3}
   </math>
   and
   <math display="inline">
    M^*
   </math>
   grows in proportion to
   <math display="inline">
    (B/\theta_{\alpha})^{2/3}
   </math>
   . That is, for large computational budgets,
   <math display="inline">
    M^*
   </math>
   grows with the square of
   <math display="inline">
    N^*
   </math>
   . Thus, marginal increments to
   <math display="inline">
    B
   </math>
   are allocated mainly to the outer step. It is easy to intuitively see the imbalance between
   <math display="inline">
    N^*
   </math>
   and
   <math display="inline">
    M^*
   </math>
   . Note that when N and M are of the same order
   <math display="inline">
    \sqrt{B}
   </math>
   , the squared bias term contributes much less to the MSE compared to the variance term. By increasing
   <math display="inline">
    M
   </math>
   at the expense of
   <math display="inline">
    N
   </math>
   , we reduce the variance till it matches up in contribution to the squared bias term.
  </p>
  <p block-type="TextInlineMath">
   As we increase the number of positions
   <math display="inline">
    K
   </math>
   , the conditional variance
   <math display="inline">
    \sigma_{\varepsilon}^2
   </math>
   falls, and
   <math display="inline">
    \theta_{\alpha}
   </math>
   falls proportionately. If the computation budget grows in proportion to K, then the optimal
   <math display="inline">
    N^*
   </math>
   falls to one, for largeenough
   <math display="inline">
    K
   </math>
   [11]. The intuition is that idiosyncratic pricing error is diversified away at the portfolio level, so a single inner step trial suffices.
  </p>
  <h4>
   <b>
    Importance Sampling
   </b>
  </h4>
  <p block-type="Text">
   By recentering the outer step simulation on the region of the state space in which large losses are more common, importance sampling (see Variance Reduction) can lead to orders of magnitude improvement in performance. Importance sampling has been applied to structural models of portfolio credit risk [2, 9], but the existing literature does not yet offer a welldeveloped importance sampling theory for reducedform models.&lt;sup&gt;d&lt;/sup&gt; Bassamboo and Jain [1] did some initial work in this direction. They consider the case where the intensity
   <math display="inline">
    \Lambda_t
   </math>
   is an affine process. Observing that the intensity remains affine under a constant exponential twist, they developed an asymptotically efficient importance sampling change-of-measure to estimate the probability of a large number of defaults in a portfolio.
  </p>
  <p block-type="Text">
   We now discuss two specifications for correlated intensity processes. Duffie and Gârleanu [5] consider the case where each intensity is the sum of common and idiosyncratic affine processes, that is,
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda_{i,t} = \lambda_t^* + \zeta_{i,t} \tag{5}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Here
   <math display="inline">
    \lambda_t^*
   </math>
   and
   <math display="inline">
    \zeta_{i,t}
   </math>
   for each
   <i>
    i
   </i>
   are mutually independent nonnegative affine processes. Under suitable parameter restrictions, the resultant process
   <math display="inline">
    \lambda_{i,t}
   </math>
   is also a nonnegative affine process. For moderately large pools, one expects that large pool losses are most likely to occur when the compensator of the common intensity process, that is,
   <math display="inline">
    \int_0^H \lambda_t^* dt
   </math>
   , is unusually large. When an exponential twist of
   <math display="inline">
    \theta &gt; 0
   </math>
   is applied to this variable, the processes remain affine under the new measure and the likelihood ratio has a straightforward representation using results from [7]. If we wish to estimate the
   <math display="inline">
    \alpha
   </math>
   quantile of the overall loss distribution, we choose
   <math display="inline">
    \theta
   </math>
   so that the mean of
   <math display="inline">
    \int_0^H \lambda_t^* dt
   </math>
   under the new measure equals its
   <math display="inline">
    \alpha
   </math>
   quantile under the original measure. The latter is unknown at the beginning of the simulation, but may be adaptively learnt as the simulation proceeds.
  </p>
  <p block-type="Text">
   The Black-Karasinski specification is increasingly popular for modeling single-name default intensities [15] and is readily extended to multiname models. Let
  </p>
  <p block-type="Equation">
   <math display="block">
    d \log \lambda_i(t) = \kappa_i(\mu_i - \log \lambda_i(t)) dt + \sigma_i dW_i(t) \quad (6)
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    \kappa_i
   </math>
   ,
   <math display="inline">
    \mu_i
   </math>
   , and
   <math display="inline">
    \sigma_i
   </math>
   are constants. The diffusion
   <math display="inline">
    W_i(t)
   </math>
   is decomposed into common and idiosyncratic components
  </p>
  <p block-type="Equation">
   <math display="block">
    W_i(t) = \sqrt{\rho_i} W^*(t) + \sqrt{1 - \rho_i} V_i(t) \tag{7}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    W^*(t)
   </math>
   and
   <math display="inline">
    V_i(t)
   </math>
   for each
   <i>
    i
   </i>
   are mutually independent standard Brownian motions and
   <math display="inline">
    \rho_i
   </math>
   is a constant. An importance sampling scheme that alters the drift of
   <math display="inline">
    W^*(t)
   </math>
   is straightforward to implement.
  </p>
  <h4>
   Discussion
  </h4>
  <p block-type="Text">
   These methods for nested simulation apply in a much more general setting. We could have structural or ratings-based pricing models in place of the stochastic intensity models, which could introduce stochastic interest rates, and allow for long and short positions. Indeed, we can allow for general derivative portfolios (not just portfolios of CDOs), in which case the intensities
   <math display="inline">
    \Lambda(t)
   </math>
   are replaced by a vector of state variables that could include interest rates, commodity prices, equity prices, and so on. The asymptotic allocation of workload between outer and inner steps remains unchanged. Furthermore, the same analysis applies to estimation of expected shortfall and large loss probabilities, to pricing of compound options, and to the credit rating of structured products under parameter uncertainty.
  </p>
  <p block-type="Text" class="has-continuation">
   Optimal allocation schemes and important sampling can be combined and extended in a variety of ways. First, for senior tranches of CDOs in particular, importance sampling within the inner step pricing (as well as the outer step) may offer large efficiency gains. Second, a
   <i>
    jackknife
   </i>
   procedure in the inner step simulation can be used to eliminate the
   <math display="inline">
    1/N
   </math>
   order term in the bias [11]. Third, a scheme for dynamic
   <i>
    allocation
   </i>
   [11] can easily be implemented. An initial estimate of loss
   <math display="inline">
    \tilde{Y}^n(\xi)
   </math>
   is obtained for a given
   <math display="inline">
    \xi
   </math>
   from a small inner step sample of
   <i>
    n
   </i>
   trials. If
   <math display="inline">
    \tilde{Y}^n(\xi)
   </math>
   is large (and therefore in the neighborhood of VaR), the estimate is refined through drawing additional samples of the inner step. Similar to this is a
   <i>
    screening
   </i>
   and restarting scheme [3, 13] developed for expected
  </p>
  <p block-type="Text">
   shortfall and other coherent risk measures. In the first phase, an initial sample of
   <i>
    M
   </i>
   outer step trials is obtained using a small number of inner step samples. These samples are screened or filtered to pick out the large loss samples that are likely to contribute to the expected shortfall. In the second phase, for the shortlisted samples, inner steps are resampled to improve the statistical properties of the resultant estimator.
  </p>
  <p block-type="Text">
   These various refinements can alter the trade-off between bias and variance, so that optimal
   <i>
    N
   </i>
   <sup>
    ∗
   </sup>
   may grow at a slower or faster rate with the budget. Nonetheless, the essential lesson of the analysis is robust: in a large portfolio VaR setting, inner step pricing simulations can be run with few trials. Despite the high likelihood of grotesque pricing errors at the instrument level, the impact on estimated VaR is small and can be controlled.
  </p>
  <h4>
   <b>
    Acknowledgments
   </b>
  </h4>
  <p block-type="Text">
   The opinions expressed here are our own, and do not reflect the views of the Board of Governors or its staff.
  </p>
  <h4>
   <b>
    End Notes
   </b>
  </h4>
  <p block-type="Text">
   a
   <i>
    .
   </i>
   Eckner [8] develops a semianalytic algorithm for [5] under somewhat restrictive assumptions.
  </p>
  <p block-type="Text">
   b
   <i>
    .
   </i>
   Observe that a single-name CDS can be represented as a special case of such a tranche. With some additional notation, it would be straightforward to accommodate corporate bonds, cash flow CDOs, and other credit instruments.
  </p>
  <p block-type="Text">
   c
   <i>
    .
   </i>
   The implicit assumption here is that interim cash flows are reinvested in the money market until time
   <i>
    H
   </i>
   , but other conventions are easily accommodated.
  </p>
  <p block-type="Text">
   d
   <i>
    .
   </i>
   Chiang
   <i>
    et al.
   </i>
   [4] develop an efficient importance sampling technique for pricing basket-default swaps in a Gaussian copula model of times to default.
  </p>
  <h4>
   <b>
    References
   </b>
  </h4>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [1] Bassamboo, A. &amp; Jain, S. (2006). Efficient importance sampling for reduced form models in credit risk, in L.F. Perrone, B.G. Lawson, J. Liu &amp; F.P. Wieland, eds,
     <i>
      Proceedings of the 2006 Winter Simulation Conference
     </i>
     , IEEE Press, Piscataway, NJ, pp. 741–748.
    </li>
    <li block-type="ListItem">
     [2] Bassamboo, A., Juneja, S. &amp; Zeevi, A. (2008). Portfolio credit risk with extremal dependence: asymptotic analysis and efficient simulation,
     <i>
      Operations Research
     </i>
     <b>
      56
     </b>
     (3), 593–606.
    </li>
    <li block-type="ListItem">
     [3] Boesel, J., Nelson, B.L. &amp; Kim, S.-H. (2003). Using ranking and selection to "clean up" after simulation optimization,
     <i>
      Operations Research
     </i>
     <b>
      51
     </b>
     (5), 814–825.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [4] Chiang, M.-H., Yueh, M.-L. &amp; Hsieh, M.-H. (2007). An efficient algorithm for basket default swap valuation,
     <i>
      Journal of Derivatives
     </i>
     Winter, 8–19.
    </li>
    <li block-type="ListItem">
     [5] Duffie, D. &amp; Garleanu, N. (2001). Risk and valuation ˆ of collateralized debt obligations,
     <i>
      Financial Analysts Journal
     </i>
     <b>
      57
     </b>
     (1), 41–59.
    </li>
    <li block-type="ListItem">
     [6] Duffie, D., Eckner, A., Horel, G. &amp; Saita, L. (2009). Frailty correlated default,
     <i>
      Journal of Finance
     </i>
     <b>
      64
     </b>
     (5), 2087–2122.
    </li>
    <li block-type="ListItem">
     [7] Duffie, D., Pan, J. &amp; Singleton, K.J. (2000). Transform analysis and asset pricing for affine jump diffusions,
     <i>
      Econometrica
     </i>
     <b>
      68
     </b>
     , 1343–1376.
    </li>
    <li block-type="ListItem">
     [8] Eckner, A. (2009). Computational techniques for basic affine models of portfolio credit risk,
     <i>
      Journal of Computational Finance
     </i>
     <b>
      13
     </b>
     (1), 1–35.
    </li>
    <li block-type="ListItem">
     [9] Glasserman, P. &amp; Li, J. (2005). Importance sampling for portfolio credit risk,
     <i>
      Management Science
     </i>
     <b>
      51
     </b>
     (11), 1643–1656.
    </li>
    <li block-type="ListItem">
     [10] Gordy, M.B. (2004). Granularity adjustment in portfolio credit risk measurement, in G.P. Szego, ed., Risk Measures for the 21st Century, John Wiley &amp; Sons.
    </li>
    <li block-type="ListItem">
     [11] Gordy, M.B. &amp; Juneja, S. (2008).
     <i>
      Nested Simulation in Portfolio Risk Measurement
     </i>
     . FEDS 2008-21, Federal Reserve Board, April 2008.
    </li>
    <li block-type="ListItem">
     [12] Lando, D., Jarrow, R.A. &amp; Yu, F. (2005). Default risk and diversification: theory and empirical implications,
     <i>
      Mathematical Finance
     </i>
     <b>
      15
     </b>
     (1), 1–26.
    </li>
    <li block-type="ListItem">
     [13] Lesnevski, V., Nelson, B.L. &amp; Staum, J. (2008). An adaptive procedure for estimating coherent risk measures based on generalized scenarios,
     <i>
      Journal of Computational Finance
     </i>
     <b>
      11
     </b>
     (4), 1–31.
    </li>
    <li block-type="ListItem">
     [14] Martin, R. &amp; Wilde, T. (2002). Unsystematic credit risk,
     <i>
      Risk
     </i>
     <b>
      15
     </b>
     (11), 123–128.
    </li>
    <li block-type="ListItem">
     [15] Pan, J. &amp; Singleton, K.J. (2008). Default and recovery implicit in the term structure of sovereign CDS spreads,
     <i>
      Journal of Finance
     </i>
     <b>
      63
     </b>
     (5), 2345–2384.
    </li>
    <li block-type="ListItem">
     [16] Shing-Hoi, L. (1998).
     <i>
      Monte Carlo Computation of Conditional Expectation Quantiles
     </i>
     . PhD thesis, Stanford University.
    </li>
   </ul>
  </p>
  <h3>
   <b>
    Further Reading
   </b>
  </h3>
  <p block-type="Text">
   Gordy, M.B. &amp; Juneja, S. (2006). Efficient simulation for risk measurement in a portfolio of CDOs, in L.F. Perrone, B.G. Lawson, J. Liu &amp; F.P. Wieland, eds,
   <i>
    Proceedings of the 2006 Winter Simulation Conference
   </i>
   , IEEE Press, Piscataway, NJ.
  </p>
  <h4>
   <b>
    Related Articles
   </b>
  </h4>
  <p block-type="Text">
   <b>
    Credit Portfolio Simulation
   </b>
   ;
   <b>
    Credit Risk
   </b>
   ;
   <b>
    Large Pool Approximations
   </b>
   ;
   <b>
    Monte Carlo Simulation
   </b>
   ;
   <b>
    Multiname Reduced Form Models
   </b>
   ;
   <b>
    Value-at-Risk
   </b>
   ;
   <b>
    Variance Reduction
   </b>
   .
  </p>
  <p block-type="Text">
   MICHAEL B. GORDY &amp; SANDEEP JUNEJA
  </p>
 </body>
</html>
