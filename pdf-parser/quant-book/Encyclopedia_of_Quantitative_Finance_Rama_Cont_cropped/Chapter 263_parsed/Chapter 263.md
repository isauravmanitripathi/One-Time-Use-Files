# Pseudorandom Number Generators

Stochastic models of quantitative finance are defined in the abstract framework of probability theory. To apply the Monte Carlo method to these models, it suffices, in principle, to sample independent realizations of the underlying random variables or random vectors. This can be achieved by sampling independent random variables uniformly distributed over the interval  $(0, 1)$  (i.i.d.  $\mathcal{U}(0, 1)$ , for short) and applying appropriate transformations to these uniform random variables. Nonuniform variate generation techniques develop such transformations and provide efficient algorithms that implement them [3, 6]. A simple general way to obtain independent random variables  $X_1, X_2, \ldots$  with distribution function F from a sequence of i.i.d.  $\mathcal{U}(0, 1)$  random variables  $U_1, U_2, \ldots$ is to define

$$X_i = F^{-1}(U_i) \stackrel{\text{def}}{=} \min\{x \mid F(x) \ge U_i\}; \quad (1)$$

this is the inversion method. This technique can provide a sequence of independent standard normal random variables, for example, which can, in turn, be used to generate the sample path of a geometric Brownian motion or other similar type of stochastic process. There is no closed-form expression for the inverse standard normal distribution function, but very accurate numerical approximations are available.

But how do we get the i.i.d.  $\mathcal{U}(0, 1)$  random variables? Realizing these random variables exactly is very difficult, perhaps, practically impossible. With current knowledge, this can be realized only approx*imately*. Fortunately, the approximation seems good enough for all practical applications of the Monte Carlo method in financial engineering and in other areas as well.

A first class of methods to realize approximations of these random variables are based on real physical noise coming from hardware devices. There is a large variety of such devices; they include gamma ray counters, fast oscillators sampled at low and slightly random frequencies, amplifiers of heat noise produced in electric resistances, photon counting and photon trajectory detectors, and so on. Some of these devices sample a signal at successive epochs and return 0 if the signal is below a given threshold, and 1 if it is above the threshold, at each sampling epoch. Others return the parity of a counter. Most of them produce sequences of bits that are slightly correlated and often slightly biased, but the bias and correlation can be reduced to a negligible amount, that becomes practically undetectable by statistical tests in reasonable time, by combining the bits in a clever way. For example, a simple technique to eliminate the bias when there is no correlation. proposed long ago by John von Neumann, places the successive bits in nonoverlapping pairs, discards all the pairs 00 and 11, and replaces the pairs 01 and  $10$  by  $1$  and  $0$ , respectively. Generalizations of this technique can eliminate both the bias and correlation [2]. Simpler techniques such as Xoring (adding modulo 2) the bits by blocks of 2 or more, or Xoring several bit streams from different sources, are often used in practice. Reliable devices to generate random bits and numbers, based on these techniques, are available on the market. These types of devices are needed for applications such as cryptography, lotteries, and gambling machines, for example, where some amount of real randomness (or entropy) is essential to provide the required unpredictability and security.

For Monte Carlo methods, however, these devices are unnecessary and unpractical. They are unnecessary because simple *deterministic algorithms* are available that require no other hardware than a standard computer and provide good enough imitations of i.i.d.  $U(0, 1)$  random variables from a statistical viewpoint, in the sense that the statistical behavior of the simulation output is pretty much the same (for all practical purposes) if we use *(pseudo)random numbers* produced by these algorithms in place of true i.i.d.  $U(0, 1)$  random variables. These deterministic algorithmic methods are much more convenient than hardware devices.

A (pseudo)random number generator (RNG, for short) can be defined as a structure comprised of the following ingredients [9]: a finite set of states  $S$ ; a probability distribution on  $S$  to select the *initial* state  $s_0$  (also called the *seed*); a transition function  $f: \mathcal{S} \to \mathcal{S}$ ; an *output space*  $\mathcal{U}$ ; and an *output function*  $g: \mathcal{S} \to \mathcal{U}$ . Here, we assume that  $\mathcal{U}$  is the interval  $(0, 1)$ . The state evolves according to the recurrence  $s_i = f(s_{i-1})$ , for  $i \ge 1$ , and the *output* at step i is  $u_i = g(s_i) \in \mathcal{U}$ . These  $u_i$ 's are the successive *random* numbers produced by the RNG. (Following common usage in the simulation community, here we leave out the qualifier "pseudo". In the area of cryptology, the term *pseudo-RNG* refers to a stronger notion, with polynomial-time unpredictability properties [20]).

Because S is finite, the RNG must eventually return to a previously visited state, that is, *sl*<sup>+</sup>*<sup>j</sup>* = *sl* for some *l* ≥ 0 and *j >* 0. Then, *si*<sup>+</sup>*<sup>j</sup>* = *si* and *ui*<sup>+</sup>*<sup>j</sup>* = *ui* for all *i* ≥ *l*; that is, the output sequence eventually repeats itself. The smallest *j >* 0 for which this happens is the *period length ρ*. Clearly, *ρ* cannot exceed |S|, the number of distinct states. If the state can be represented with *b* bits of memory, then *ρ* ≤ 2*<sup>b</sup>*. For good RNGs, *ρ* is usually close to 2*<sup>b</sup>*, as it is not difficult to construct recurrences with this property. Typical values of *b* range from 31 to around 20 000 or even higher [18]. In our opinion, *ρ* should never be less than 2<sup>100</sup> and preferably more than 2200. Values of *b* that exceed 1000 are unnecessary if the RNG satisfies the quality criteria described in what follows.

A key advantage of algorithmic RNGs is their ability to repeat exactly the same sequence of random numbers without storing them. Repeating the same sequence several times is essential for the proper implementation of variance reduction techniques such as using common random numbers for comparing similar systems, sensitivity analysis, sample-path optimization, external control variates, antithetic variates, and so on [1, 5] (*see also* **Variance Reduction**). It is also handy for program verification and debugging. On the other hand, some real randomness can be used for selecting the seed *s*<sup>0</sup> of the RNG.

# **Streams and Substreams**

Modern high-quality simulation software often offers the possibility to declare and create virtual RNGs just like for any other type of variable or object, in practically unlimited amount. In an implementation adopted by several simulation software vendors, these virtual RNGs are called *streams*, and each stream is split into multiple *substreams* long enough to prevent potential overlap [14, 19]. For any given stream, there are methods to generate the next number, to rewind to the beginning of the stream or to the beginning of the current substream, or to the beginning of the next substream.

To illustrate its usefulness, consider a simple model of a financial option whose payoff is a function of a geometric Brownian motion observed at fixed points in time. We want to estimate *d* = *Ɛ*[*X*<sup>2</sup> − *X*1] where *X*<sup>1</sup> and *X*<sup>2</sup> are the payoffs with two slightly different parameter settings, such as different volatilities or different strike prices, for example. This is often useful for sensitivity analysis (estimating the *greeks*; *see* **Monte Carlo Greeks**). To estimate *d*, we would simulate the model with the two different settings using *common random numbers* across the two versions [1, 5] (*see also* **Variance Reduction**), repeat this *n* times independently and compute a confidence interval on *d* from the *n* independent copies of *X*<sup>2</sup> − *X*1. To implement this, we take a stream of random numbers that contains multiple substreams, use the same substream to simulate both *X*<sup>1</sup> and *X*<sup>2</sup> for each replication, and *n* different substreams for the *n* replications. At the beginning of a replication, the stream is placed to the beginning of a new substream and the model is simulated to compute *X*1. Then the stream is reset to the beginning of its current substream before simulating the model again to compute *X*2. This ensures that exactly the same random numbers are used to generate the Brownian motion increments at the same time points for both *X*<sup>1</sup> and *X*2. Then the stream is moved to the beginning of the next substream for the next pair of runs.

There are many situations where the number of calls to the RNG during a simulation depends on the model parameters and may not be the same for *X*<sup>1</sup> and *X*2. Even in that case, the above scheme ensures that the RNG restarts at the same place for both parameter settings, for each replication. In more complicated models, to ensure a good synchronization of the random numbers across the two settings (i.e., make sure that the same random numbers are used for the same purposes in both cases), it is typically convenient to have several different streams, each stream being dedicated to one specific aspect of the model. For instance, in the previous example, if we also need to simulate external events that occur according to a Poisson process and influence the payoff in some way (e.g., they could trigger jumps in the Brownian motion), it is better to use a separate stream to simulate this process, to guarantee that no random number is used for the Brownian motion increment in one setting and for the Poisson process in the other setting.

#### **Ouality Criteria and Testing**

A good RNG must obviously have a very long period, to make sure that there is no chance of wrapping around. It should also be repeatable (able to reproduce exactly the same sequence several times),  $p$ ortable (be easy to implement and behave the same way in different software/hardware environments), and it should be easy to split its sequence into several disjoint streams and substreams, and implement efficient tools to move between those streams and substreams. The latter requires the availability of efficient jump-ahead methods, that can quickly compute  $s_{i+v}$  given  $s_i$ , for any large v. The number b of bits required to store the state should not be too large, because the computing time for jumping ahead typically increases faster than linearly with  $b$ , and also because there can be a large number of streams and substreams in a given simulation, especially for large complex models. Another key performance measure is the speed of the generator itself. Fast generators can produce up to 100 million  $\mathcal{U}(0, 1)$  random numbers per second on current personal computers [18].

All these nice properties are not sufficient, however. For example, an RNG that returns  $u_i =$  $(i/10^{1000})$  mod 1 at step *i* satisfies these properties but is definitely not recommendable, because its successive output values have an obvious strong correlation. Ideally, if we select a random seed  $s_0$  uniformly in  $S$ , we would like the vector of the first  $s$  output values,  $(u_0, \ldots, u_{s-1})$ , to be uniformly distributed over the *s*-dimensional unit hypercube  $[0, 1]^s$  for each  $s > 0$ . This would guarantee both uniformity and independence. Formally, we cannot have this, because these  $s$ -dimensional vectors must take their values from the finite set  $\Psi_s = \{(u_0, \ldots, u_{s-1}) : s_0 \in \mathcal{S}\}$ , whose cardinality cannot exceed  $|S|$ . If  $s_0$  is random,  $\Psi_s$  can be viewed as the *sample space* from which vectors of successive output values are drawn randomly. Then, to approximate the uniformity and independence, we want the finite set  $\Psi_{s}$  to provide a dense and uniform coverage of the hypercube  $[0, 1]^{s}$ , at least for small and moderate values of  $s$ . This is possible only if  $S$ has large cardinality, and it is, in fact, a more important reason for having a long period than the danger of exhausting the cycle.

Hence, the uniformity of  $\Psi_s$  in  $[0, 1]^s$  is a key quality criterion. But how do we measure it? There are many ways of measuring the uniformity (or the discrepancy from the uniform distribution) for a point set in the unit hypercube [16, 22] (see also Quasi-Monte Carlo Methods). To be practical, the uniformity measure must be selected so that it can be effectively computed without generating explicitly the points of  $\Psi_{s}$ . For this reason, the theoretical figures of merit that measure the uniformity usually depend on the mathematical structure of the RNG. This is also the main reason for having RNGs based on linear recurrences: their point sets  $\Psi_{s}$  are easier to analyze mathematically, because they have a simpler structure. One could argue that nonlinear and more complex structures give rise to point sets  $\Psi_{s}$  that look more random, and some of them behave very well in empirical statistical tests, but their structure is much harder to analyze. They could leave large holes in  $[0, 1]^{s}$  that are difficult to detect.

To design a good RNG, one typically selects an algorithm together with the size of the state space, and constraints on the parameters that ensure a fast implementation. Then one makes a computerized search in the space of parameters to find a set of values that give (i) the maximal period length within this class of generators and then (ii) the largest figure of merit than can be found. RNGs are thus selected and constructed primarily based on theoretical criteria. Then, they are implemented and tested empirically.

A large variety of *empirical statistical tests* have been designed and implemented for RNGs [8, 18]. All these tests try to detect empirical evidence against the hypothesis  $\mathcal{H}_0$  that the  $u_i$  are i.i.d.  $\mathcal{U}[0, 1]$ . A test can be any function Y of a finite set of  $u_i$ 's, which can be computed in reasonable time, and whose distribution under  $\mathcal{H}_0$  can be approximated well enough. There is an unlimited number of such tests. When applying the test, one computes the realization of  $Y$ , say  $y$ , and then the probability  $p^+ = \mathbb{P}[Y \geq y \mid \mathcal{H}_0]$ , called the right  $p$  value. If  $Y$  takes a much larger value than expected, then  $p^+$  will be very close to 0, and we declare that the RNG fails the test. We may also examine the left p value  $p^- = \mathbb{P}[Y \leq y \mid \mathcal{H}_0]$ , or both  $p^+$  and  $p^-$ , depending on the design of the test. When a generator really fails a test, it is not unusual to find p values as small as  $10^{-15}$  or less.

Specific batteries that contain a variety of standard tests, which detect problems often encountered in poorly designed or too simple RNGs, have been proposed and implemented [18]. The bad news is that a majority of the RNGs available in popular commercial software fail these tests unequivocally, with *p* values smaller than 10<sup>−</sup>15. These generators should be discarded, unless we have very good reasons to believe that for our specific simulation models, the problems detected by these failed tests will not affect the results. The good news is that some freely available high-quality generators pass all the tests in these batteries. Of course, passing all these tests is not a proof that the RNG is reliable for all the possible simulations, but it certainly improves our confidence in the generator. In fact, no RNG can pass all conceivable statistical tests. In some sense, the good RNGs fail only very complicated tests that are hard to find and implement, whereas bad RNGs fail simple tests.

# **Linear Recurrences**

Most RNGs used for simulation are based on linear recurrences of the general form

$$x_i = (a_1 x_{i-1} + \dots + a_k x_{i-k}) \bmod m$$
 (2)

where *k* and *m* are positive integers, and the coefficients *a*1*,...,ak* are in {0*,* 1*,...,m* − 1}, with *ak* = 0. Some use a large value of *m*, preferably a prime number, and define the output as *ui* = *xi/m*, so the state at step *i* can be viewed as *si* = **x***<sup>i</sup>* = *(xi*<sup>−</sup>*k*+1*,...,xi)*. The RNG is then called a *multiple recursive generator* (MRG). For *k* = 1, we obtain the classical linear congruential generator (LCG). In practice, the output transformation is modified slightly to make sure that *ui* is always strictly between 0 and 1, for example, by taking *ui* = *(xi* + 1*)/(m* + 1*)* or *ui* = *(xi* + 1*/*2*)/m*. Jumping ahead from **x***<sup>i</sup>* to **x***i*+*<sup>ν</sup>* for an arbitrary large *ν* can be implemented easily: because of the linearity, one can write **x***i*+*<sup>ν</sup>* = **A***ν***x***<sup>i</sup>* mod *m*, where **A***<sup>ν</sup>* is a *k* × *k* matrix that can be precomputed once for all [13]. When *m* is prime, one can choose the coefficients *aj* so that the period length reaches *mk* − 1, its maximum [8].

The point set *s* produced by an MRG is known to have a *lattice structure*, and its uniformity is measured *via* a figure of merit for the quality of that lattice, for several values of *s*. This is known as the *spectral test* [4, 8, 10].

Typically, *m* is chosen as one of the largest prime integers representable on the target computer, for example, *m* = 2<sup>31</sup> − 1 on a 32-bit computer. Then, a direct implementation of equation (2) with integer numbers would cause overflow, so more clever implementation techniques are needed. These techniques require that we impose additional conditions on the coefficients *aj* . We have to be careful that these conditions do not oversimplify the structure of the point set *s*. One extreme example of this is to take only two nonzero coefficients, say *ar* and *ak* , both equal to ±1. Implementation is then easy and fast. However, all triples of the form *(ui, ui*−*r, ui*−*k)* produced by such a generator, for *i* = 0*,* 1*,...*, lie in only two planes in the three-dimensional unit cube. Despite this awful behavior, these types of generators (or variants thereof) can be found in many popular software products [18]. They should be avoided. All simple LCGs, say with *m* ≤ 264, should be discarded; they have too much structure and their period length is too short for present computers.

One effective way of implementing high-quality MRGs is to combine two (or more) of them by adding their outputs modulo 1. (There are also other slightly different ways of combining.) If the components have distinct prime moduli, the combination turns out to be just another MRG with (nonprime) modulus *m* equal to the product of the moduli of the components, and the period can be up to half the product of the component's periods when we combine two of them. The idea is to select the components so that (i) a fast implementation is easy to construct for each individual component and (ii) the combined MRG has a more complicated structure and highly uniform sets *s*, as measured by the spectral test [10]. Specific MRG constructions can be found in [10, 13, 18] and the references given therein.

A different approach uses a linear recurrence as in equation (2), but with *m* = 2. All operations are then performed modulo 2, that is, in the finite field <sup>2</sup> with elements {0*,* 1}. This allows very fast implementations by exploiting the binary nature of computers. A general framework for this is the matrix linear recurrence [13, 17]:

$$\mathbf{x}_i = \mathbf{A}\mathbf{x}_{i-1} \tag{3}$$

$$\mathbf{y}_i = \mathbf{B}\mathbf{x}_i \tag{4}$$

$$u_i = \sum_{\ell=1}^{w} y_{i,\ell-1} 2^{-\ell} \tag{5}$$

where **x***<sup>i</sup>* = *(xi,*0*,...,xi,k*<sup>−</sup>1*)*<sup>t</sup> is the *k*-bit *state vector* at step *i*, **y***<sup>i</sup>* = *(yi,*0*,...,yi,w*<sup>−</sup>1*)*<sup>t</sup> is the *w*-bit *output* *vector* at step *i*, *k*, and *w* are the positive integers, **A** is a *k* × *k* binary *transition matrix*, **B** is a *w* × *k* binary *output transformation matrix*, and *ui* ∈ [0*,* 1*)* is the *output* at step *i*. All operations in equations (3) and (4) are performed in 2. These RNGs are called 2-*linear generators*.

The theoretical analysis usually assumes the simple output definition (5), but, in practice, this definition is modified slightly to avoid returning 0 or 1. This framework covers several types of generators, including the Tausworthe, polynomial LCG, generalized feedback shift register (GFSR), twisted GFSR, Mersenne twister, Well, Xorshift, linear cellular automaton, and combinations of these [13, 17, 21]. With a carefully selected matrix **A** (its characteristic polynomial must be a primitive polynomial over 2), the period length can reach 2*<sup>k</sup>* − 1. In practice, the matrices **A** and **B** are chosen so that the products (3) and (4) can be implemented very efficiently on a computer by a few simple binary operations such as or, exclusive-or, shift, and rotation, on blocks of bits. The idea is to find a compromise between the number of such operations (which affects the speed) and a good uniformity of the point sets *s* (which is easier to reach with more operations). The uniformity of these point sets is measured *via* their *equidistribution*; essentially, the hypercube [0*,* 1]*<sup>s</sup>* is partitioned into small subcubes (or subrectangles) of equal sizes, and for several such partitions, we check if all the subcubes contain exactly the same number of points from *s*. This can be computed efficiently by computing the ranks of certain binary matrices [17]. Combined generators of this type, defined by Xoring the output vectors **y***<sup>i</sup>* of the components, are equivalent to yet another 2-linear generator. Such combinations have the same motivation as for MRGs.

# **Nonlinear Generators**

Linear RNGs have many nice properties, but they also fail certain specialized statistical tests focused at detecting linearity. When the simulation itself applies nonlinear transformations to the uniform random numbers, which is typical, one should not worry about the linearity, unless the structure of *s* is not very good. However, there are cases where the linearity can matter. For example, to generate a large random binary matrix, one should not use an 2 linear generator, because the rank of the matrix is likely to be much smaller than expected, due to the excessive linear dependence [18].

There are many ways of constructing nonlinear generators. For example, one can simply add a nonlinear output transformation to a linear RNG, or permute (shuffle) the output values with the help of another generator. Another way is to combine an MRG with an 2-linear generator, either by addition modulo 1 or by Xoring the outputs. An important advantage of this technique is that the uniformity of the resulting combined generator can be assessed theoretically, at least to a certain extent [15]. They can also be fast.

When combining generators, it is important to understand what we do and we should be careful to examine the structure not only of the combination but also of the quality of the components. By blindly combining two good components, it is indeed possible (and not too difficult) to obtain a bad (worst) RNG.

Generators whose underlying recurrence is nonlinear are generally harder to analyze and are slower. These are the types of generators used for cryptographic applications. Empirically, well-designed nonlinear generators tend to perform better in statistical tests than the linear ones [18], but from the theoretical perspective, their structure is not understood as well. RNGs based on chaotic dynamical systems have often been proposed in the literature, but these generators have several major drawbacks, including the fact that their *s*-dimensional uniformity is often very poor [7].

# **What to Look For and What to Avoid**

A quick look at the empirical results in [12, 18] shows that many widely used RNGs are seriously deficient, including the default generators of several highly popular software products. So before running important simulation experiments, one should always check what is the default RNG, and be ready to replace it if needed. Note that the generators that pass the tests in [18] are not all recommended. Before adoption, one should verify that the RNG has solid theoretical support, that it is fast enough, and that multiple streams and substreams are available, for example. Convenient software packages with multiple streams and substreams are described in [14, 19] and are available freely from the web page of this author. These packages are based on combined MRGs of [10], combined Tausworthe generators of [11], the Well generators [23] (which are improvements over the Mersenne twister in terms of equidistribution), and some additional nonlinear generators, among others. No uniform RNG can be guaranteed against all possible defects, but one should at least avoid those that fail simple statistical tests miserably and go for the more robust ones, for which no serious problem has been detected after years of usage and testing.

# **Acknowledgments**

This work has been supported by the Natural Sciences and Engineering Research Council of Canada Grant No. ODGP0110050 and a Canada Research Chair to the author.

# **References**

- [1] Bratley, P., Fox, B.L. & Schrage, L.E. (1987). *A Guide to Simulation*, 2nd Edition, Springer-Verlag, New York.
- [2] Chor, B. & Goldreich, O. (1988). Unbiased bits from sources of weak randomness and probabilistic communication complexity, *SIAM Journal on Computation* **17**(2), 230–261.
- [3] Devroye, L. (1986). *Non-Uniform Random Variate Generation*, Springer-Verlag, New York.
- [4] Fishman, G.S. (1996). *Monte Carlo: Concepts, Algorithms, and Applications*, *Series in Operations Research*, Springer-Verlag, New York.
- [5] Glasserman, P. (2004). *Monte Carlo Methods in Financial Engineering*, Springer-Verlag, New York.
- [6] Hormann, W., Leydold, J. & Derflinger, G. (2004). ¨ *Automatic Nonuniform Random Variate Generation*, Springer-Verlag, Berlin.
- [7] Jackel, P. (2002). ¨ *Monte Carlo Methods in Finance*, John Wiley & Sons, Chichester.
- [8] Knuth, D.E. (1998). *The Art of Computer Programming, Volume 2: Seminumerical Algorithms*, 3rd Edition, Addison-Wesley, Reading.
- [9] L'Ecuyer, P. (1994). Uniform random number generation, *Annals of Operations Research* **53**, 77–120.
- [10] L'Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators, *Operations Research* **47**(1), 159–164.
- [11] L'Ecuyer, P. (1999). Tables of maximally equidistributed combined LFSR generators, *Mathematics of Computation* **68**(225), 261–269.
- [12] L'Ecuyer, P. (2001). Software for uniform random number generation: distinguishing the good and the

bad, in *Proceedings of the 2001 Winter Simulation Conference*, IEEE Press, Piscataway, pp. 95–105.

- [13] L'Ecuyer, P. (2006). Uniform random number generation, in *Simulation, Handbooks in Operations Research and Management Science*, S.G. Henderson & B.L. Nelson, eds, Elsevier, Amsterdam, Chapter 3, pp. 55–81.
- [14] L'Ecuyer, P. & Buist, E. (2005). Simulation in Java with SSJ, in *Proceedings of the 2005 Winter Simulation Conference*, IEEE Press, pp. 611–620.
- [15] L'Ecuyer, P. & Granger-Piche, J. (2003). Combined ´ generators with components from different families, *Mathematics and Computers in Simulation* **62**, 395–404.
- [16] L'Ecuyer, P. & Lemieux, C. (2002). Recent advances in randomized quasi-Monte Carlo methods, in *Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and Applications*, M. Dror & P. L'Ecuyer, F. Szidarovszky, eds, Kluwer Academic, Boston, pp. 419–474.
- [17] L'Ecuyer, P. & Panneton, F. (2009). *F*2-Linear random number generators, *Advancing the Frontiers of Simulation: A Festschrift in Honor of George S. Fishman*, Springer-Verlag.
- [18] L'Ecuyer, P. & Simard, R. (2007). TestU01: A C library for empirical testing of random number generators, *ACM Transactions on Mathematical Software* **33**(4), Article 22, 5.
- [19] L'Ecuyer, P., Simard, R., Chen, E.J. & Kelton, W.D. (2002). An object-oriented random-number package with many long streams and substreams, *Operations Research* **50**(6), 1073–1075.
- [20] Luby, M. (1996). *Pseudorandomness and Cryptographic Applications*, Princeton University Press, Princeton.
- [21] Matsumoto, M. & Nishimura, T. (1998). Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator, *ACM Transactions on Modeling and Computer Simulation* **8**(1), 3–30.
- [22] Niederreiter, H. (1992). Random number generation and quasi-Monte Carlo methods, in *SIAM CBMS-NSF Regional Conference Series in Applied Mathematics*, SIAM, Philadelphia, Vol. 63.
- [23] Panneton, F., L'Ecuyer, P. & Matsumoto, M. (2006). Improved long-period generators based on linear recurrences modulo 2, *ACM Transactions on Mathematical Software* **32**(1), 1–16.

# **Related Articles**

**Monte Carlo Greeks**; **Monte Carlo Simulation for Stochastic Differential Equations**; **Quasi-Monte Carlo Methods**; **Stochastic Differential Equations: Scenario Simulation**; **Variance Reduction**.

PIERRE L'ECUYER