<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h2>
   <b>
    Phase-type Distribution
   </b>
  </h2>
  <p block-type="Text">
   A phase-type distribution is a probability distribution that results from a system of one or more interrelated Poisson processes occurring in sequence or phases. Such distributions represent one of the most general classes of distributions permitting a probabilistic (i.e., Markovian) interpretation, as they are essentially based on the method of exponential stages technique that was introduced by Erlang in the early twentieth century and later generalized by Neuts in the mid-to-late 1970s. Phase-type distributions also possess a variety of desirable analytical properties that often facilitate their use in terms of yielding exact, algorithmically tractable, solution procedures. Phasetype distributions have been used extensively in many areas of applied probability, particularly in queueing theory (e.g., see [3]), actuarial risk theory (e.g., see [9]), and finance (e.g., see [4]). In addition, thorough treatments of phase-type distributions and their applications can be found in several reference texts including
   <math display="inline">
    [1, 5, 7, 8, 10]
   </math>
   . In what follows, we present only a brief overview of phase-type distributions and some of their key distributional properties.
  </p>
  <p block-type="TextInlineMath">
   In essence, phase-type distributions consist of a "general" mixture of exponentials and are characterized by an absorbing finite-state Markov chain. Specifically, let
   <math display="inline">
    \{X(t), t &gt; 0\}
   </math>
   represent a homogeneous, continuous-time Markov chain with transient states
   <math display="inline">
    \{1, 2, \ldots, n\}
   </math>
   and single absorbing state labeled
   <math display="inline">
    n+1
   </math>
   . Define the row vector
   <math display="inline">
    \alpha = (\alpha_1, \alpha_2, \dots, \alpha_n)
   </math>
   , wherein
   <math display="inline">
    \alpha_i
   </math>
   represents the probability of beginning (at time 0) in a transient state
   <math display="inline">
    j, j = 1, 2, \dots, n
   </math>
   . The infinitesimal generator
   <math display="inline">
    Q
   </math>
   associated with this Markov process can be represented as
  </p>
  <p block-type="Equation">
   <math display="block">
    Q = \begin{bmatrix} S &amp; \underline{s}_0 \\ \underline{0} &amp; 0 \end{bmatrix} \tag{1}
   </math>
  </p>
  <p block-type="TextInlineMath">
   In equation (1), S is an
   <math display="inline">
    n \times n
   </math>
   nonsingular matrix containing the transition rates among the
   <math display="inline">
    n
   </math>
   transient states. The diagonal entries of
   <math display="inline">
    S
   </math>
   are necessarily negative, while the remaining entries of
   <math display="inline">
    S
   </math>
   are nonnegative. Furthermore,
   <math display="inline">
    \underline{s}_0
   </math>
   is the
   <math display="inline">
    n \times 1
   </math>
   column vector of absorption rates into state
   <math display="inline">
    n + 1
   </math>
   from the individual transient states. Necessarily,
   <math display="inline">
    \underline{s}_0 = -S\underline{e}
   </math>
   , where
   <math display="inline">
    \underline{e}
   </math>
   denotes an
   <math display="inline">
    n \times 1
   </math>
   column vector of ones. In addition, 0 represents a
   <math display="inline">
    1 \times n
   </math>
   row vector of zeros.
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    Y
   </math>
   represent the time to absorption into state
   <math display="inline">
    n + 1
   </math>
   , namely
   <math display="inline">
    Y = \inf\{t &gt; 0 \mid X(t) = n + 1\}
   </math>
   . The random variable
   <math display="inline">
    Y
   </math>
   is said to have a continuous phase-type distribution with (parametric) representation
   <math display="inline">
    (\alpha, S)
   </math>
   of dimension
   <i>
    n
   </i>
   , often denoted as
   <math display="inline">
    Y \sim
   </math>
   <math display="inline">
    PH_n(\alpha, S)
   </math>
   . The cumulative distribution function and density function of
   <math display="inline">
    Y
   </math>
   are given by
  </p>
  <p block-type="Equation">
   <math display="block">
    F(y) = 1 - \underline{\alpha} \exp(Sy)\underline{e} \quad \text{and}
   </math>
   <br/>
   <math display="block">
    f(y) = \underline{\alpha} \exp(Sy)\underline{s}_0 \quad \text{for} \quad y \ge 0 \tag{2}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where the matrix exponential in equation (2) is defined by
   <math display="inline">
    \exp(Sy) = \sum_{m=0}^{\infty} y^m S^m / m!
   </math>
   Moreover, Y has Laplace-Stieltjes transform and integer moments given by
  </p>
  <p block-type="Equation">
   <math display="block">
    E\{e^{-tY}\} = 1 - \underline{\alpha} \underline{e} + \underline{\alpha}(tI - S)^{-1}\underline{s}_0 \quad \text{and}
   </math>
   <math display="block">
    E\{Y^k\} = (-1)^k k! \underline{\alpha} S^{-k} \underline{e} \tag{3}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where I denotes an
   <math display="inline">
    n \times n
   </math>
   identity matrix. Computation of the above matrix-based quantities is a routine task with the aid of most mathematical software packages in use today. A good reference for the calculation of matrix exponentials is [6].
  </p>
  <p block-type="Text">
   One of the most important and useful features of the phase-type family of distributions is that the phase-type distribution is dense in the set of all positive-valued distributions, implying that it is (theoretically) possible to represent any positive-valued distribution with a "suitable" phasetype approximation. However, we emphasize the fact that the phase-type distribution is a lighttailed distribution (including combinations/mixtures of exponential and Erlang distributions as special cases), and thus it cannot be expected to fit a heavy-tailed distribution in the extreme tail. For an excellent treatment regarding phase-type fitting of arbitrary distributions, see [2] and references therein.
  </p>
  <p block-type="Text">
   Numerous operations on phase-type distributions lead again to distributions that are phase-type with readily calculable parameters. This is a very appealing quality of phase-type distributions. In particular, convolutions, finite mixtures, and geometric compounds of phase-type distributions are again phasetype distributed. See
   <math display="inline">
    [1, 7]
   </math>
   for proofs of these results and further closure properties.
  </p>
  <p block-type="Text" class="has-continuation">
   In conclusion, we state that a discrete analog to the continuous phase-type distribution exists by
  </p>
  <p block-type="Text">
   considering the time to absorption in an equivalent discrete-time Markov chain. We direct interested readers to [5, 7] for further details on discrete phasetype distributions.
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [1] Asmussen, S. (2000).
     <i>
      Ruin Probabilities
     </i>
     , World Scientific, Singapore.
    </li>
    <li block-type="ListItem">
     [2] Asmussen, S. (2000). Matrix-analytic models and their analysis,
     <i>
      Scandinavian Journal of Statistics
     </i>
     <b>
      27
     </b>
     , 193–226.
    </li>
    <li block-type="ListItem">
     [3] Asmussen, S. (2003).
     <i>
      Applied Probability and Queues
     </i>
     , 2nd Edition, Springer-Verlag, New York.
    </li>
    <li block-type="ListItem">
     [4] Asmussen, S., Avram, F. &amp; Pistorius, M. (2004). Russian and American put options under exponential phase-type Levy models, `
     <i>
      Stochastic Processes and their Applications
     </i>
     <b>
      109
     </b>
     , 79–111.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [5] Latouche, G. &amp; Ramaswami, V. (1999).
     <i>
      Introduction to Matrix Analytic Methods in Stochastic Modeling
     </i>
     , ASA SIAM, Philadelphia.
    </li>
    <li block-type="ListItem">
     [6] Moler, C. &amp; Van Loan, C. (2003). Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later,
     <i>
      SIAM Review
     </i>
     <b>
      45
     </b>
     , 3–49.
    </li>
    <li block-type="ListItem">
     [7] Neuts, M. (1981).
     <i>
      Matrix-geometric Solutions in Stochastic Models: An Algorithmic Approach
     </i>
     , Johns Hopkins University Press, Baltimore.
    </li>
    <li block-type="ListItem">
     [8] Neuts, M. (1989).
     <i>
      Structured Stochastic Matrices of M/G/1 Type and Their Applications
     </i>
     , Marcel Dekker, New York.
    </li>
    <li block-type="ListItem">
     [9] Rolski, T., Schmidli, H., Schmidt, V. &amp; Teugels, J. (1999).
     <i>
      Stochastic Processes for Insurance and Finance
     </i>
     , John Wiley &amp; Sons, Chichester.
    </li>
    <li block-type="ListItem">
     [10] Wolff, R. (1989).
     <i>
      Stochastic Modeling and the Theory of Queues
     </i>
     , Prentice-Hall, NJ.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   STEVE DREKIC
  </p>
 </body>
</html>
