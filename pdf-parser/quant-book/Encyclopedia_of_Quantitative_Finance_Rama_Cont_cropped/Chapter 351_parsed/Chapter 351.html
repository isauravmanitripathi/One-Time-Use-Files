<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Generalized Method of Moments (GMM)
   </b>
  </h1>
  <p block-type="Text">
   The generalized method of moments (GMM) provides a computationally convenient method of obtaining consistent and asymptotically normally distributed estimators of the parameters of statistical models. The method has been applied in many areas of economics but has arguably been most frequently applied in finance. In fact, it was in empirical finance that the power of the method was first illustrated: for while Hansen [7] introduced GMM and presented its fundamental statistical theory, Hansen and Hodrick [9] and Hansen and Singleton [10] showed the potential of the GMM approach to estimation through their empirical analyses of, respectively, foreign exchange markets and asset pricing. This article briefly describes the GMM framework for estimation and inference in the case where time-series data are used. The reader is referred to [6] for a comprehensive treatment of GMM that presents both a rigorous statistical analysis of the method and empirical illustrations in finance. For applications of GMM to panel data, see [15].
  </p>
  <p block-type="Text" class="has-continuation">
   The popularity of GMM can be understood by comparing the requirements for the method to those for maximum likelihood (ML). While ML is the best available estimator within the paradigm of classical statistics, its optimality stems from its basis on the joint probability distribution of the data. However, in many scenarios of interest in finance, this dependence on the probability distribution can become a weakness for two main reasons. First, in some models, the joint probability distribution is only implicitly specified and is not available in a convenient closed form with the result that ML is computationally infeasible, for example, in stochastic volatility models [11]. Secondly, in other cases, the underlying theoretical model places restrictions on the distribution of the data but does not completely specify its form, with the result that ML is infeasible unless the researcher imposes an arbitrary assumption about the distribution. The latter is an unattractive strategy because if the assumed distribution is incorrect, then the optimality of ML is lost, and the resulting estimator may even be inconsistent; for example, in nonlinear Euler equation models [10]. In contrast, GMM estimation is based on population moment conditions. It turns
  </p>
  <p block-type="Text">
   out that in many cases where the financial-theoretic model does not specify the complete distribution, it does specify population moment conditions. Therefore, in these settings, GMM can be preferred to ML because it offers a way to estimate the parameters solely on the basis of information deduced from the underlying financial model.
  </p>
  <p block-type="Text">
   The cornerstone of GMM estimation is the population moment condition:
  </p>
  <h4>
   <b>
    Definition 1 Population Moment Condition.
   </b>
  </h4>
  <p block-type="Text">
   <i>
    Let θ
   </i>
   <sup>
    0
   </sup>
   <i>
    be a vector of unknown parameters that are to be estimated,
   </i>
   <b>
    v
   </b>
   <i>
    <sup>
     t
    </sup>
    be a vector of random variables, and
   </i>
   <b>
    f
   </b>
   <i>
    (.) a vector of functions. Then a population moment condition takes the form
   </i>
  </p>
  <p block-type="Equation">
   <math display="block">
    E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}_0)] = 0 \tag{1}
   </math>
  </p>
  <p block-type="Text">
   <i>
    for all t.
   </i>
  </p>
  <p block-type="Text">
   To illustrate the types of population moment conditions that can arise in finance models, we consider the specific examples in the seminal papers [9] and [10].
  </p>
  <p block-type="Text">
   <b>
    Example 1
   </b>
   The efficiency of foreign exchange markets
  </p>
  <p block-type="Text">
   Consider the following regression model for the relationship between the forward and spot exchange rates:
  </p>
  <p block-type="Equation">
   <math display="block">
    s_{t+k} = \beta_{0,1} + \beta_{0,2} f_{t,k} + u_t \tag{2}
   </math>
  </p>
  <p block-type="Text">
   where
   <i>
    ft,k
   </i>
   is the log of the
   <i>
    k
   </i>
   − period forward exchange rate at time
   <i>
    t
   </i>
   ,
   <i>
    st
   </i>
   <sup>
    +
   </sup>
   <i>
    <sup>
     k
    </sup>
   </i>
   is the log of the spot exchange rate in period
   <i>
    t
   </i>
   +
   <i>
    k
   </i>
   ,
   <i>
    β
   </i>
   0
   <i>
    ,
   </i>
   <sup>
    1
   </sup>
   and
   <i>
    β
   </i>
   0
   <i>
    ,
   </i>
   <sup>
    2
   </sup>
   are unknown parameters, and
   <i>
    ut
   </i>
   is an error term. Hansen and Hodrick [9] demonstrate that under the simple efficient-markets hypothesis, the following population moment condition holds
  </p>
  <p block-type="Equation">
   <math display="block">
    E[f_{t,k}(s_{t+k} - \beta_{0,1} - \beta_{0,2}f_{t,k})] = 0 \qquad (3)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <i>
    (β
   </i>
   0
   <i>
    ,
   </i>
   1
   <i>
    , β
   </i>
   0
   <i>
    ,
   </i>
   2
   <i>
    )
   </i>
   =
   <i>
    (
   </i>
   0
   <i>
    ,
   </i>
   1
   <i>
    )
   </i>
   .
  </p>
  <p block-type="Text">
   <b>
    Example 2
   </b>
   The consumption-based asset-pricing model
  </p>
  <p block-type="Text" class="has-continuation">
   The consumption-based asset-pricing model involves a representative agent who makes investment and consumption decisions at time
   <i>
    t
   </i>
   to maximize his/her discounted lifetime utility subject to a budget constraint. Assuming that the agent's utility function is of the constant relative risk aversion form and that the
  </p>
  <p block-type="Text">
   only asset available as a possible investment yields a pay-off in the following period, Hansen and Singleton
   <math display="inline">
    [10]
   </math>
   show that the model implies the following population moment condition:
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\mathbf{z}_{t}\left\{\delta_{0}\left(\frac{c_{t+1}}{c_{t}}\right)^{\gamma_{0}-1}\left(\frac{g_{t+1}}{p_{t}}\right)-1\right\}\right]=0\qquad(4)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    c_t
   </math>
   is consumption in period t,
   <math display="inline">
    g_{t+1}
   </math>
   is the gain (cash flow) on the asset in period
   <math display="inline">
    t + 1
   </math>
   ,
   <math display="inline">
    p_t
   </math>
   is the price of the asset in period t,
   <math display="inline">
    \mathbf{z}_t
   </math>
   is a vector of variables in the agent's information set at time
   <math display="inline">
    t
   </math>
   , and the unknown parameters
   <math display="inline">
    1 - \gamma_0
   </math>
   and
   <math display="inline">
    \delta_0
   </math>
   are, respectively, the agent's coefficient of relative risk aversion and discount factor.
  </p>
  <p block-type="TextInlineMath">
   The population moment condition states that
   <math display="inline">
    E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta})]
   </math>
   equals zero when evaluated at
   <math display="inline">
    \boldsymbol{\theta}_0
   </math>
   . For the GMM estimation to be successful, this must be a unique property of
   <math display="inline">
    \theta_0
   </math>
   , that is,
   <math display="inline">
    E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta})]
   </math>
   is not equal to zero when evaluated at any other value of
   <math display="inline">
    \theta
   </math>
   . If the latter holds, then
   <math display="inline">
    \theta_0
   </math>
   is said to be
   <i>
    identified
   </i>
   by
   <math display="inline">
    E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}_0)] = 0
   </math>
   . A first-order condition for identification (often referred to as a local
   <i>
    condition
   </i>
   ) is that
   <math display="inline">
    rank\{G(\theta_0)\} = p
   </math>
   , where
   <math display="inline">
    G(\theta) =
   </math>
   <math display="inline">
    E[\partial \mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta})/\partial \boldsymbol{\theta}']
   </math>
   , and this plays a crucial role in standard asymptotic distribution theory for GMM. By definition, the moment condition involves
   <math display="inline">
    q
   </math>
   pieces of information about
   <math display="inline">
    p
   </math>
   unknowns; therefore identification can only hold if
   <math display="inline">
    q \geq p
   </math>
   . For reasons that emerge below, it is convenient to split this scenario into two parts:
   <math display="inline">
    q = p
   </math>
   , in which case
   <math display="inline">
    \theta_0
   </math>
   is said to be
   <i>
    just identified
   </i>
   , and
   <math display="inline">
    q &gt; p
   </math>
   , in which case
   <math display="inline">
    \theta_0
   </math>
   is said to be
   <i>
    overidentified
   </i>
   .
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    \mathbf{g}_T(\boldsymbol{\theta})
   </math>
   be the analogous sample moment to
   <math display="inline">
    E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta})]
   </math>
   , that is,
   <math display="inline">
    \mathbf{g}_T(\boldsymbol{\theta}) = T^{-1} \sum_{t=1}^T \mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta})
   </math>
   where
   <math display="inline">
    T
   </math>
   is the sample size. The GMM estimator is then as follows.
  </p>
  <h2>
   <b>
    Definition 2 Generalized Method of Moments
   </b>
   Estimator.
  </h2>
  <p block-type="Text">
   The generalized method of moments estimator based on equation (1) is the value of
   <math display="inline">
    \boldsymbol{\theta}
   </math>
   that minimizes
  </p>
  <p block-type="Equation">
   <math display="block">
    Q_T(\boldsymbol{\theta}) = \mathbf{g}_T(\boldsymbol{\theta})' \mathbf{W}_T \mathbf{g}_T(\boldsymbol{\theta}) \tag{5}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{W}_T
   </math>
   is known as the weighting matrix and is restricted to be a positive semidefinite matrix that converges in probability to
   <math display="inline">
    \mathbf{W}
   </math>
   , some positive definite matrix of constants.
  </p>
  <p block-type="TextInlineMath">
   The restrictions on the weighting matrix are required to ensure that
   <math display="inline">
    O_T(\theta)
   </math>
   is a meaningful measure of the distance the sample moment is from zero at different values of
   <math display="inline">
    \theta
   </math>
   . The choice of weighting matrix is discussed further below.
  </p>
  <p block-type="Text">
   The first-order conditions for this minimization are
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{G}_T(\hat{\boldsymbol{\theta}}_T)'\mathbf{W}_T\mathbf{g}_T(\hat{\boldsymbol{\theta}}_T) = 0 \tag{6}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{G}_T(\boldsymbol{\theta}) = T^{-1} \sum_{t=1}^T \partial \mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}) / \partial \boldsymbol{\theta}'
   </math>
   . The structure of these conditions reveals some interesting insights into GMM estimation. Since
   <math display="inline">
    G_T(\theta)
   </math>
   is
   <math display="inline">
    q \times p
   </math>
   , it follows that equation (6) involves calculating
   <math display="inline">
    \hat{\boldsymbol{\theta}}_T
   </math>
   as the value of
   <math display="inline">
    \boldsymbol{\theta}
   </math>
   that sets the
   <i>
    p
   </i>
   linear combinations of
   <math display="inline">
    \mathbf{g}_T(.)
   </math>
   to zero. Therefore, if
   <math display="inline">
    p = q
   </math>
   (and
   <math display="inline">
    \mathbf{G}_T(\hat{\boldsymbol{\theta}}_T)'\mathbf{W}_T
   </math>
   is nonsingular), then
   <math display="inline">
    \hat{\boldsymbol{\theta}}_T
   </math>
   satisfies the analogous sample moment condition to equation (1),
   <math display="inline">
    \mathbf{g}_T(\hat{\boldsymbol{\theta}}_T) = 0
   </math>
   , and is, thus, the method of moments estimator based on the original moment condition. However, if
   <math display="inline">
    q &gt; p
   </math>
   , then the first-order conditions are not equivalent to solving the sample moment condition. Instead,
   <math display="inline">
    \hat{\theta}_T
   </math>
   is equivalent to the method of moments estimator based on
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathbf{G}(\boldsymbol{\theta}_0)' \mathbf{W} E[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}_0)] = 0 \tag{7}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Although equation
   <math display="inline">
    (1)
   </math>
   implies equation
   <math display="inline">
    (7)
   </math>
   , the reverse does not hold because
   <math display="inline">
    q &gt; p
   </math>
   ; therefore, in this case, the estimation is actually based on only part of the original information. As a result, if
   <math display="inline">
    q &gt; p
   </math>
   , then GMM can be viewed as decomposing the original moment condition into two parts, the
   <i>
    identify
   </i>
   ing restrictions, which contain the information actually used in the estimation, and the
   <i>
    overidentifying
   </i>
   restrictions, which represent a remainder. Furthermore, GMM estimation produces two fundamental statistics each of which is associated with a particular component: the estimator
   <math display="inline">
    \hat{\theta}_T
   </math>
   is a function of the information in the identifying restrictions, and the estimated sample moment,
   <math display="inline">
    \mathbf{g}_T(\hat{\boldsymbol{\theta}}_T)
   </math>
   , is a function of the information in the overidentifying restrictions. While unused in estimation, the overidentifying restrictions play a crucial role in inference about the validity of the model as discussed below.
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   To establish the consistency and asymptotic normality of the GMM estimator, it is necessary to place restrictions on
   <math display="inline">
    \mathbf{v}_t
   </math>
   and
   <math display="inline">
    \mathbf{f}(.)
   </math>
   . In the standard asymptotic framework (see [7]), it is assumed that
   <math display="inline">
    \mathbf{v}_t
   </math>
   is stationary and ergodic,
   <math display="inline">
    \mathbf{f}(.)
   </math>
   is continuous and differentiable and that its derivative is continuous in
   <math display="inline">
    \theta
   </math>
   . Various expectations of
   <math display="inline">
    \mathbf{f}(.)
   </math>
   and its derivative must also exist.
  </p>
  <p block-type="TextInlineMath">
   These conditions permit the applications of laws of large numbers and central limit theorems to the requisite sample averages. Chief among the restrictions on expectations of
   <math display="inline">
    \mathbf{f}()
   </math>
   are that the population moment condition is valid and identifies
   <math display="inline">
    \theta_0
   </math>
   . Under the above conditions, it can be shown that the
   <math display="inline">
    \hat{\boldsymbol{\theta}}_T \stackrel{p}{\rightarrow} \boldsymbol{\theta}_0
   </math>
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    T^{1/2}(\hat{\boldsymbol{\theta}}_T - \boldsymbol{\theta}_0) \stackrel{d}{\to} N(0, V) \tag{8}
   </math>
  </p>
  <p block-type="Text">
   where
  </p>
  <p block-type="Equation">
   <math display="block">
    V = [\mathbf{G}(\boldsymbol{\theta}_0)' \mathbf{W} \mathbf{G}(\boldsymbol{\theta}_0)]^{-1} \mathbf{G}(\boldsymbol{\theta}_0)' \mathbf{W} \mathbf{S}(\boldsymbol{\theta}_0) \mathbf{W} \mathbf{G}(\boldsymbol{\theta}_0)
   </math>
   <math display="block">
    \times [\mathbf{G}(\boldsymbol{\theta}_0)' \mathbf{W} \mathbf{G}(\boldsymbol{\theta}_0)]^{-1}
   </math>
  </p>
  <p block-type="TextInlineMath">
   and
   <math display="inline">
    \mathbf{S}(\boldsymbol{\theta}) = \lim_{T \to \infty} Var[T^{1/2}\mathbf{g}_T(\boldsymbol{\theta})].
   </math>
  </p>
  <p block-type="TextInlineMath">
   At this point, we return to the issue of the choice of weighting matrix. In the discussion of the firstorder conditions above, it is noted that if
   <math display="inline">
    p = q
   </math>
   , then the GMM estimator can be found by solving
   <math display="inline">
    \mathbf{g}_T(\hat{\boldsymbol{\theta}}_T)
   </math>
   . As a result, the estimator does not depend on
   <math display="inline">
    \mathbf{W}_T
   </math>
   . The asymptotic properties must also be invariant to
   <math display="inline">
    \mathbf{W}_T
   </math>
   and it can be shown that
   <math display="inline">
    \mathbf{V}
   </math>
   reduces to
   <math display="inline">
    \{\mathbf{G}(\boldsymbol{\theta}_0)'\mathbf{S}(\boldsymbol{\theta}_0)^{-1}\mathbf{G}(\boldsymbol{\theta}_0)\}^{-1}
   </math>
   in this case. However, if
   <math display="inline">
    q &gt; p
   </math>
   , then the first-order conditions depend on
   <math display="inline">
    \mathbf{W}_T
   </math>
   and therefore so does
   <math display="inline">
    \hat{\theta}_T
   </math>
   , in general. This dependence is unattractive because it raises the possibility that subsequent inferences can be affected by the choice of weighting matrix. However, in terms of asymptotic properties, the choice of weighting matrix only manifests itself in
   <math display="inline">
    V
   </math>
   , the asymptotic variance of the estimator. Since this is the case, it is natural to choose
   <math display="inline">
    \mathbf{W}_T
   </math>
   such that
   <math display="inline">
    \mathbf{V}
   </math>
   is minimized in a matrix sense. Hansen [7] shows that this can be achieved by setting
   <math display="inline">
    \mathbf{W}_T = \hat{\mathbf{S}}_T^{-1}
   </math>
   where
   <math display="inline">
    \hat{\mathbf{S}}_T
   </math>
   is a consistent estimator of
   <math display="inline">
    S(\theta_0)
   </math>
   . The resulting asymptotic variance is
   <math display="inline">
    \mathbf{V} = \mathbf{V}^0 = \{ \mathbf{G}(\boldsymbol{\theta}_0)' S(\boldsymbol{\theta}_0)^{-1} \mathbf{G}(\boldsymbol{\theta}_0) \}^{-1}; \text{Chamberlain [4]}
   </math>
   shows
   <math display="inline">
    \mathbf{V}^0
   </math>
   is a semiparametric efficiency bound for estimation of
   <math display="inline">
    \theta_0
   </math>
   based on equation (1).
  </p>
  <p block-type="TextInlineMath">
   In practical terms, two issues arise in the implementation of GMM with this choice of weighting matrix: (i) how to construct
   <math display="inline">
    \hat{\mathbf{S}}_T
   </math>
   so that it is a consistent estimator of
   <math display="inline">
    S(\theta_0)
   </math>
   ; (ii) how to handle the dependence of
   <math display="inline">
    \hat{\mathbf{S}}_T
   </math>
   on
   <math display="inline">
    \hat{\boldsymbol{\theta}}_T
   </math>
   . We treat each in turn.
  </p>
  <h4>
   (i) Estimation of
   <math display="inline">
    S(\theta_0)
   </math>
   .
  </h4>
  <p block-type="TextInlineMath" class="has-continuation">
   Under stationarity and ergodicity and certain other technical restrictions, it can be shown that
   <math display="inline">
    S(\theta_0)
   </math>
   =
   <math display="inline">
    \Gamma_0(\boldsymbol{\theta}_0) + \sum_{i=1}^{\infty} \{ \Gamma_i(\boldsymbol{\theta}_0) + \Gamma_i(\boldsymbol{\theta}_0)' \}
   </math>
   where
   <math display="inline">
    \Gamma_i(\boldsymbol{\theta}_0) =
   </math>
   <math display="inline">
    \text{Cov}[\mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}_0), \mathbf{f}(\mathbf{v}_{t-i}, \boldsymbol{\theta}_0)]
   </math>
   is known as the
   <i>
    i
   </i>
   -lag autocovariance matrix of
   <math display="inline">
    \mathbf{f}(\mathbf{v}_t, \boldsymbol{\theta}_0)
   </math>
   [2]. In some cases, the
  </p>
  <p block-type="TextInlineMath">
   structure of the model implies
   <math display="inline">
    \Gamma_i(\theta_0) = 0
   </math>
   for all
   <math display="inline">
    i &gt; k
   </math>
   for some
   <math display="inline">
    k
   </math>
   , and this simplifies the estimation problem ([6], Section 3.5). In the absence of such a restriction
   <math display="inline">
    \mathbf{a}
   </math>
   on the autocovariance matrices, the long-run variance can be estimated by a member of the class of
   <i>
    heteroscedasticity autocorrelation covariance (HAC)
   </i>
   estimators defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \hat{\mathbf{S}}_{\text{HAC}} = \hat{\mathbf{\Gamma}}_0 + \sum_{i=1}^{T-1} \omega(i; b_T) (\hat{\mathbf{\Gamma}}_i + \hat{\mathbf{\Gamma}}'_i) \qquad (9)
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \hat{\mathbf{\Gamma}}_j = T^{-1} \sum_{t=j+1}^T \hat{\mathbf{f}}_t \hat{\mathbf{f}}_{t-j}^v, \ \hat{\mathbf{f}}_t = \mathbf{f}(v_t, \hat{\boldsymbol{\theta}}_T), \ \omega(.)
   </math>
   is known as the
   <i>
    kernel
   </i>
   , and
   <math display="inline">
    b_T
   </math>
   is known as the bandwidth. The kernel and bandwidth must satisfy certain restrictions to ensure
   <math display="inline">
    \hat{\mathbf{S}}_{\text{HAC}}
   </math>
   is both consistent and positive semidefinite. As an illustration, Newey and West [13] propose the use of the kernel
   <math display="inline">
    \omega(i, b_T) = \{1 - i/(b_T + 1)\}\mathcal{I}\{i \leq b_T\}
   </math>
   where
   <math display="inline">
    \mathcal{I}\{i \leq t_T\}
   </math>
   <math display="inline">
    b_T
   </math>
   is an indicator variable that takes the value 1 if
   <math display="inline">
    i &lt; b_T
   </math>
   and 0, otherwise. This choice is an example of a truncated kernel estimator because the number of included autocovariances is determined by
   <math display="inline">
    b_T
   </math>
   . For consistency, we require
   <math display="inline">
    b_T \rightarrow
   </math>
   <math display="inline">
    \infty
   </math>
   with
   <math display="inline">
    T \to \infty
   </math>
   but
   <math display="inline">
    b_T = o(T^{1/2})
   </math>
   . Various choices of kernel have been proposed and their properties analyzed: while theoretical rankings are possible, the evidence suggests that the choice of
   <math display="inline">
    b_T
   </math>
   is a far more important determinant of finite sample performance. Andrews [2] and Newey and West [14] propose data-based methods for the selection of
   <math display="inline">
    b_T
   </math>
   . See ([6], Section 3.5) for further discussion.
  </p>
  <h4>
   (ii) Dependence of
   <math display="inline">
    \hat{S}_T
   </math>
   on
   <math display="inline">
    \hat{\theta}_T
   </math>
   .
  </h4>
  <p block-type="TextInlineMath">
   As is apparent from the above discussion, the calculation of a consistent estimator for
   <math display="inline">
    S(\theta_0)
   </math>
   requires knowledge of a (consistent) estimator of
   <math display="inline">
    \theta_0
   </math>
   . Therefore, to calculate a GMM estimator that attains the efficiency bound, a multistep procedure is used. In the first step, GMM is performed with an arbitrary weighting matrix; this preliminary estimator is then used in the calculation of
   <math display="inline">
    \hat{\mathbf{S}}_T
   </math>
   . In the second step, GMM estimation is performed with
   <math display="inline">
    \mathbf{W}_T = \hat{\mathbf{S}}_T^{-1}
   </math>
   . For obvious reasons, the resulting estimator is commonly referred to as the two-step GMM estimator. Instead of stopping after just two steps, the procedure can be continued so that on the
   <math display="inline">
    i
   </math>
   th step the GMM estimation is performed using
   <math display="inline">
    \mathbf{W}_T = \hat{\mathbf{S}}_T^{-1}
   </math>
   , where
   <math display="inline">
    \hat{\mathbf{S}}_T
   </math>
   is based on the estimator from the
   <math display="inline">
    (i-1)
   </math>
  </p>
  <p block-type="Text">
   th step. This yields the so-called iterated GMM estimator.
  </p>
  <p block-type="TextInlineMath">
   While two steps are sufficient to attain the efficiency bound, simulation evidence suggests that there are often considerable gains to iteration in the sense of improvements in the quality of asymptotic theory as an approximation to finite sample behavior ([6], Chapter 6). Notwithstanding these improvements, there is evidence that the quality of the asymptotic approximation may be poor in some circumstances of interest and this motivated the development of alternative methods for attaining the efficiency bound. One such method is the
   <i>
    continuous-updating gener
   </i>
   alized method of moments (CUGMM) estimator proposed by Hansen et al. [8]. The CUGMM estimator is the value of
   <math display="inline">
    \boldsymbol{\theta}
   </math>
   that minimizes
   <math display="inline">
    \mathbf{g}_T(\boldsymbol{\theta})'\mathbf{S}_T(\boldsymbol{\theta})^{-1}\mathbf{g}_T(\boldsymbol{\theta})
   </math>
   where
   <math display="inline">
    S_T(\theta)
   </math>
   is a (matrix) function of
   <math display="inline">
    \theta
   </math>
   such that
   <math display="inline">
    \mathbf{S}_T(\boldsymbol{\theta}) \stackrel{p}{\rightarrow} \mathbf{S}(\boldsymbol{\theta})
   </math>
   uniformly in
   <math display="inline">
    \boldsymbol{\theta}
   </math>
   . While the CUGMM estimator has the same limiting distribution as the iterated GMM estimator, Newey and Smith [12] and Anatolyev [1] demonstrate analytically that the continuous-updating estimator can be expected to exhibit lower finite sample bias than its two-step counterpart; see also [3].
  </p>
  <p block-type="TextInlineMath">
   The foregoing results are predicated on the assumption that equation (1) represents valid information about
   <math display="inline">
    \theta_0
   </math>
   , and, in this sense, that the model is correctly specified. In practice, it is desirable to assess whether the data appear consistent with the restriction implied by the population moment condition. As noted above, if
   <math display="inline">
    p = q
   </math>
   , then the first-order conditions force
   <math display="inline">
    \mathbf{g}_T(\hat{\boldsymbol{\theta}}_T) = 0
   </math>
   irrespective of whether equation (1) holds, and so the latter cannot be tested directly using the estimated sample moment,
   <math display="inline">
    \mathbf{g}_T(\hat{\boldsymbol{\theta}}_T)
   </math>
   . However, if
   <math display="inline">
    q &gt; p
   </math>
   , then
   <math display="inline">
    \mathbf{g}_T(\boldsymbol{\theta}_T) \neq 0
   </math>
   because GMM estimation only imposes the identifying restrictions and ignores the overidentifying restrictions. The latter represent
   <math display="inline">
    q - p
   </math>
   restrictions which are true if equation (1) is itself true. To motivate the most commonly applied test statistic, it is useful to recall two aspects of our discussion above: (i) the GMM minimand measures the distance of
   <math display="inline">
    \mathbf{g}_T(\boldsymbol{\theta})
   </math>
   from zero; and (ii) the estimated sample moment contains information about overidentifying restrictions. Combining (i) and (ii), it can be shown that GMM minimand evaluated at
   <math display="inline">
    \hat{\theta}_T
   </math>
   is a measure of how far the sample is from satisfying the overidentifying restrictions. This leads to the overidentifying restrictions test
  </p>
  <p block-type="Text">
   statistic,
  </p>
  <p block-type="Equation">
   <math display="block">
    J_T = T \mathbf{g}_T (\hat{\boldsymbol{\theta}}_T)' \hat{\mathbf{S}}_T^{-1} \mathbf{g}_T (\hat{\boldsymbol{\theta}}_T) \tag{10}
   </math>
  </p>
  <p block-type="TextInlineMath">
   which converges to a
   <math display="inline">
    \chi_{q-p}^2
   </math>
   distribution under the null hypothesis that equation (1) holds. Notice that
   <math display="inline">
    J_T
   </math>
   is conveniently calculated as the sample size times the two-step (or iterated) GMM minimand evaluated at the associated estimator. The overidentifying restrictions test is the standard model diagnostic within the GMM framework. Nevertheless,
   <math display="inline">
    J_T
   </math>
   does not have power against all alternatives of interest: Ghysels and Hall [5] show that
   <math display="inline">
    J_T
   </math>
   has no (local) power against parameter variation and so it is prudent to complement the overidentifying restrictions test with tests of structural stability ([6], Section 5.4).
  </p>
  <p block-type="Text">
   As noted above, there is evidence that in some cases of interest, the aforementioned (standard) asymptotic theory can provide a poor approximation to the finite sample performance of the GMM estimator and its associated inference techniques. This has prompted researchers to develop ways of ameliorating these deficiencies. The proposed methods fall into three basic categories: (i) methods for
   <i>
    moment selection
   </i>
   , which are designed to determine the "best" set of moments to use with a view to then basing inference on the standard asymptotic theory described above; (ii) resampling methods such as the
   <i>
    bootstrap
   </i>
   that are valid under similar assumptions to the standard theory but aim to provide more accurate approximations to finite sample behavior; and (iii) alternative asymptotic theories that are based on different assumptions to the standard framework and are designed to provide more accurate approximations in certain scenarios of interest, such as weak identifi
   <i>
    cation.
   </i>
   For further discussion of
   <math display="inline">
    (i)
   </math>
   –
   <math display="inline">
    (iii)
   </math>
   , see
   <math display="inline">
    (6]
   </math>
   , Chapters 6, 7, and 8).
  </p>
  <h3>
   References
  </h3>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [1] Anatolyev, S. (2005). GMM, GEL, Serial correlation, and asymptotic bias,
     <i>
      Econometrica
     </i>
     <b>
      73
     </b>
     , 983–1002.
    </li>
    <li block-type="ListItem">
     [2] Andrews, D. (1991). Heteroscedasticity and autocorrelation consistent covariance matrix estimation, Econometrica 59, 817-858.
    </li>
    <li block-type="ListItem">
     Antoine, B., Bonnal, H. &amp; Renault, E. (2007). On the [3] efficient use of the informational content of estimating equations: implied probabilities and Euclidean Empirical Likelihood, Journal of Econometrics 138, 461-487.
    </li>
    <li block-type="ListItem">
     [4] Chamberlain, G. (1987). Asymptotic efficiency in estimation with conditional moment restrictions, Journal of Econometrics 34, 305-334.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [5] Ghysels, E. &amp; Hall, A. (1990). Are consumption based intertemporal asset pricing models structural?
     <i>
      Journal of Econometrics
     </i>
     <b>
      45
     </b>
     , 121–139.
    </li>
    <li block-type="ListItem">
     [6] Hall, A. (2005).
     <i>
      Generalized Method of Moments
     </i>
     , Oxford University Press, Oxford, UK.
    </li>
    <li block-type="ListItem">
     [7] Hansen, L. (1982). Large sample properties of Generalized Method of Moments estimators,
     <i>
      Econometrica
     </i>
     <b>
      50
     </b>
     , 1029–1054.
    </li>
    <li block-type="ListItem">
     [8] Hansen, L., Heaton, J. &amp; Yaron, A. (1996). Finite sample properties of some alternative GMM estimators obtained from financial market data,
     <i>
      Journal of Business and Economic Statistics
     </i>
     <b>
      14
     </b>
     , 262–280.
    </li>
    <li block-type="ListItem">
     [9] Hansen, L. &amp; Hodrick, R. (1980). Forward exchange rates as optimal predictors of future spot rates,
     <i>
      Journal of Political Economy
     </i>
     <b>
      87
     </b>
     , 829–853.
    </li>
    <li block-type="ListItem">
     [10] Hansen, L. &amp; Singleton, K. (1982). Generalized instrumental variables estimation of nonlinear rational expectations models,
     <i>
      Econometrica
     </i>
     <b>
      50
     </b>
     , 1269–1286.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
     [11] Melino, A. &amp; Turnbull, S. (1990). Pricing foreign currency options with stochastic volatility,
     <i>
      Journal of Econometrics
     </i>
     <b>
      45
     </b>
     , 239–266.
    </li>
    <li block-type="ListItem">
     [12] Newey, W. &amp; Smith, R. (2004). Higher order properties of GMM and generalized empirical likelihood estimators,
     <i>
      Econometrica
     </i>
     <b>
      72
     </b>
     , 219–255.
    </li>
    <li block-type="ListItem">
     [13] Newey, W. &amp; West, K. (1987). A simple positive semi–definite heteroscedasticity and autocorrelation consistent covariance matrix,
     <i>
      Econometrica
     </i>
     <b>
      55
     </b>
     , 703–708.
    </li>
    <li block-type="ListItem">
     [14] Newey, W. &amp; West, K. (1994). Automatic lag selection in covariance matrix estimation,
     <i>
      Review of Economic Studies
     </i>
     <b>
      61
     </b>
     , 631–653.
    </li>
    <li block-type="ListItem">
     [15] Wooldridge, J. (2002).
     <i>
      Econometric Analysis of Cross Section and Panel Data
     </i>
     , MIT Press, Cambridge, MA.
    </li>
   </ul>
  </p>
  <p block-type="Text">
   ALASTAIR R. HALL
  </p>
 </body>
</html>
