<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
 </head>
 <body>
  <h1>
   <b>
    Stochastic Control
   </b>
  </h1>
  <p block-type="Text">
   The theory of stochastic optimal control concerns the control of a dynamical system in the presence of random noises so as to optimize a certain performance criterion. The early development of stochastic optimal control theory began in the period between the late 1950s and early 1960s. The earlier stage of the development focused on the quadratic performance criterion. Around the same period, the classic work of Bellman [1] first introduced one of the main approaches in stochastic optimal control theory, namely, the dynamic programming principle. The dynamic programming approach plays a significant role in modern finance, in particular, continuous-time finance. The key idea of the dynamic programming principle is to consider a family of stochastic optimal control problems with different starting times and states and to relate the problems through the Hamilton–Jacobi–Bellman (HJB) equation. An HJB equation is a nonlinear second-order partial differential equation (
   <i>
    see
   </i>
   <b>
    Partial Differential Equations
   </b>
   ) that describes the local behavior of the performance criterion evaluated at the optimal control. For detailed discussion of the dynamic programming approach, we refer to [11, 12, 21].
  </p>
  <p block-type="Text">
   Together with the HJB approach, the stochastic maximum principle provides the second main approach to stochastic control. The key idea of the stochastic maximum principle is to derive a set of necessary conditions satisfied by any optimal control. The stochastic maximum principle basically states that any optimal control must satisfy forward–backward stochastic differential equations (SDEs), called the
   <i>
    optimality system
   </i>
   , and a maximum condition of a functional, called the
   <i>
    Hamiltonian
   </i>
   . The novelty of the stochastic maximum principle is to make the stochastic optimal control problem, which is infinite dimensional, more tractable. It leads to explicit solutions for the optimal controls in some cases. References [2] and [21] provided excellent discussions on the stochastic maximum principle.
  </p>
  <p block-type="Text" class="has-continuation">
   Merton [16, 17] pioneered the study of an optimal consumption–investment problem in a continuoustime economy. He first explored the state of the art of the stochastic optimal control theory to develop an elegant (closed-form) solution to the problem (
   <i>
    see
   </i>
   <b>
    Merton Problem
   </b>
   ). The stochastic control approach adopted by Merton uses the HJB equation. Another
  </p>
  <p block-type="Text">
   approach is the martingale approach, which uses the martingale method for risk-neutral valuation of options to provide an elegant solution to the optimal consumption–investment problem. The martingale approach was pioneered by the important contributions of Cox and Huang [4] and Karatzas
   <i>
    et al.
   </i>
   [13]. It was then extended by a number of authors, (see, e.g., [14]).
  </p>
  <p block-type="Text">
   Each of the three main approaches in stochastic optimal control has its own merits. For example, dynamic programming works well in the case when (i) the state processes and optimal controls are Markov (
   <i>
    see
   </i>
   <b>
    Markov Processes
   </b>
   ), (ii) the state processes have deterministic coefficients, and (iii) state constraints are absent. The stochastic maximum principle can deal with the situations when the state processes have random coefficients and state constraints are present. The martingale approach is applicable when one considers a general state process, for example, when the state process is not Markov. It works well when the market is complete though there are some works that consider the case when the market is incomplete (see [14]). The martingale approach is suitable for the situations when there are nonnegative constraints on consumption and wealth. It is difficult to say which one is uniformly better or more general than the other two. However, the three approaches are related to each other in some way. For example, the relationship between the dynamic programming approach and the stochastic maximum principle can be established by relating the solutions of the forward–backward SDEs associated with the stochastic maximum principle to those of the HJB equation from the dynamic programming ([21], Chapter 5). The relationship between the martingale approach and the stochastic maximum principle stems from two facts. Firstly, the solutions of the adjoint equations can be related to the density process for the change of measures in the martingale approach. Secondly, the first-order condition of the constrained maximization problem in the martingale approach is related to that of the first-order condition of the Hamiltonian in the stochastic maximum principle [3]. It is interesting to note that the three approaches end up with the same result of the optimal consumption–investment problem in some cases.
  </p>
  <p block-type="Text" class="has-continuation">
   We discuss three methods, namely, the martingale method, the HJB Method, and the stochastic maximum principle to solve the optimal consumption–investment problem. Here, we focus on the case
  </p>
  <p block-type="Text">
   of a power utility function. For general cases, refer to
   <math display="inline">
    [10, 14]
   </math>
   .
  </p>
  <h4>
   The Martingale Approach
  </h4>
  <p block-type="Text">
   The development here is based on the contributions of Karaztazs, Lehoczky, Sethi, Shreve, and Xu [13]. Here, we just present some main results and highlight some key steps. For a more comprehensive discussion, we refer to [10; Chapter 10].
  </p>
  <p block-type="TextInlineMath">
   We consider a popular model for a financial market consisting of one risk-free asset and
   <math display="inline">
    n
   </math>
   risky assets. These assets are tradable over a finite time horizon
   <math display="inline">
    [0, T^*]
   </math>
   , where
   <math display="inline">
    T^* &lt; \infty
   </math>
   . Fix a complete probability space
   <math display="inline">
    (\Omega, \mathcal{F}, \mathcal{P})
   </math>
   , where
   <math display="inline">
    \mathcal{P}
   </math>
   is a real-world probability measure.
  </p>
  <p block-type="TextInlineMath">
   The dynamics of the risk-free asset or bond,
   <math display="inline">
    B
   </math>
   , and the risky assets
   <math display="inline">
    S_1, S_2, \ldots, S_n
   </math>
   , under
   <math display="inline">
    \mathcal{P}
   </math>
   , are governed by
  </p>
  <p block-type="Equation">
   <math display="block">
    dB(t) = r(t)B(t) dt
   </math>
   ,
   <math>
    B(0) = 1
   </math>
   (1)
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathrm{d}S_i(t) = S_i(t) \left( \mu_i(t) \,\mathrm{d}t + \sum_{j=1}^n \sigma_{ij}(t) \,\mathrm{d}W_j(t) \right) \tag{2}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    S_i(0) = s_i
   </math>
   ,
   <math>
    i = 1, 2, ..., n
   </math>
   (3)
  </p>
  <p block-type="TextInlineMath">
   Here
   <math display="inline">
    W(t) := (W_1(t), W_2(t), \ldots, W_n(t))^T
   </math>
   is an
   <math display="inline">
    n
   </math>
   -dimensional Brownian motion defined on
   <math display="inline">
    (\Omega, \mathcal{F}, \mathcal{P})
   </math>
   , where
   <math display="inline">
    y^T
   </math>
   is the transpose of a vector y. Write
   <math display="inline">
    \{\mathcal{F}(t)\}\
   </math>
   for the right-continuous and complete filtration generated by
   <math display="inline">
    \{W(t)\}\
   </math>
   . For a treatment of SDEs, see [8].
  </p>
  <p block-type="TextInlineMath">
   The market interest rate
   <math display="inline">
    r(t)
   </math>
   , the vector of appreciation rates
   <math display="inline">
    \mu(t) := (\mu_1(t), \mu_2(t), \dots, \mu_n(t))^T
   </math>
   , and the volatility matrix
   <math display="inline">
    \sigma(t) := [\sigma_{ij}(t)]_{i,j=1,2,...,n}
   </math>
   of the risky assets are supposed to be measurable,
   <math display="inline">
    \{\mathcal{F}(t)\}
   </math>
   adapted, and bounded processes. The market is complete.
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    a(t) := \sigma(t)\sigma^{T}(t)
   </math>
   . Suppose there is an
   <math display="inline">
    \epsilon &gt; 0
   </math>
   such that
  </p>
  <p block-type="Equation">
   <math display="block">
    \xi^T a(t)\xi \ge \epsilon |\xi|^2, \quad \forall \xi \in \mathbb{R}^n, \quad (t, \omega) \in [0, T^*] \times \Omega,
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    |\cdot|
   </math>
   denotes the Euclidean norm in
   <math display="inline">
    \mathbb{R}^n
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   Then, the inverses of
   <math display="inline">
    \sigma
   </math>
   and
   <math display="inline">
    \sigma^T
   </math>
   exist and are bounded, and the market is complete. The filtration
   <math display="inline">
    \{\mathcal{F}(t)\}\
   </math>
   is equivalent to the
   <math display="inline">
    \mathcal{P}
   </math>
   -completion of the filtration generated by the price process
   <math display="inline">
    \{S(t)\}.
   </math>
  </p>
  <p block-type="Text">
   We define the market price of risk by
  </p>
  <p block-type="Equation">
   <math display="block">
    \theta(t) := \sigma^{-1}(t)(\mu(t) - r(t)\mathbf{1})\tag{4}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \mathbf{1} := (1, 1, \dots, 1)^T \in \mathbb{R}^n
   </math>
   ;
   <math display="inline">
    \theta
   </math>
   is bounded and
   <math display="inline">
    \{\mathcal{F}(t)\}
   </math>
   -progressively measurable.
  </p>
  <p block-type="Text">
   Now, we introduce an exponential process:
  </p>
  <p block-type="Equation">
   <math display="block">
    \Lambda(t) := \exp\left(-\int_0^t \theta^T(s) \, \mathrm{d}W(s) - \frac{1}{2} \int_0^t |\theta^T(s)|^2 \mathrm{d}s\right) \tag{5}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Define a new probability measure
   <math display="inline">
    \mathcal{P}^{\theta} \sim \mathcal{P}
   </math>
   on
   <math display="inline">
    \mathcal{F}(T^*)
   </math>
   by setting
  </p>
  <p block-type="Equation">
   <math display="block">
    \frac{\mathrm{d}\mathcal{P}^{\theta}}{\mathrm{d}P} := \Lambda(T^*) \tag{6}
   </math>
  </p>
  <p block-type="Text">
   By Girsanov's theorem,
  </p>
  <p block-type="Equation">
   <math display="block">
    W^{\theta}(t) := W(t) + \int_0^t \theta(s) \, \mathrm{d}s \tag{7}
   </math>
  </p>
  <p block-type="Text">
   is an
   <math display="inline">
    n
   </math>
   -dimensional standard Brownian motion under
   <math display="inline">
    \mathcal{P}^{\theta}
   </math>
   .
  </p>
  <p block-type="Text">
   Also, under
   <math display="inline">
    \mathcal{P}^{\theta}
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathrm{d}S_i(t) = S_i(t) \left( r(t) \,\mathrm{d}t + \sum_{j=1}^n \sigma_{ij}(t) \,\mathrm{d}W_j^{\theta}(t) \right) \tag{8}
   </math>
  </p>
  <p block-type="Text">
   Here
   <math display="inline">
    \mathcal{P}^{\theta}
   </math>
   is called the
   <i>
    risk-neutral
   </i>
   or
   <i>
    equivalent martingale measure.
   </i>
  </p>
  <p block-type="Text">
   Consider a power utility as below:
  </p>
  <p block-type="Equation">
   <math display="block">
    U(c) = \frac{c^{\gamma}}{\gamma} \ , \quad 0 &lt; \gamma &lt; 1 \tag{9}
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    \gamma
   </math>
   represents the risk-aversion parameter. The relative degree of risk aversion is
   <math display="inline">
    1 - \gamma
   </math>
   , which indicates how risk averse the investor is. The higher
   <math display="inline">
    1 - \gamma
   </math>
   is, the more risk averse the investor is.
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    U'
   </math>
   denote the first derivative of
   <math display="inline">
    U(c)
   </math>
   with respect to c. Write
   <math display="inline">
    I(\cdot)
   </math>
   for the inverse of
   <math display="inline">
    U'(\cdot)
   </math>
   . Then, for any
   <math display="inline">
    y \in (0, \infty)
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    I(y) = y^{1/(\gamma - 1)} \tag{10}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Consider a measurable
   <math display="inline">
    \mathbb{R}^n
   </math>
   -valued,
   <math display="inline">
    \{\mathcal{F}(t)\}
   </math>
   -adapted process
   <math display="inline">
    \pi := (\pi_1, \pi_2, \dots, \pi_n)^T
   </math>
   and a consumption
  </p>
  <p block-type="TextInlineMath">
   process
   <math display="inline">
    \{c(t)\}\
   </math>
   as a nonnegative, measurable,
   <math display="inline">
    \{\mathcal{F}(t)\}\
   </math>
   adapted process such that
  </p>
  <p block-type="Equation">
   <math display="block">
    \int_0^{T^*} (c(t) + |\pi(t)|^2) dt &lt; \infty, \quad \mathcal{P} \text{ a.s.} \quad (11)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Here
   <math display="inline">
    \pi_i(t)S_i(t)
   </math>
   represents the amount invested in the
   <i>
    i
   </i>
   th risky asset, for
   <math display="inline">
    i = 1, 2, \ldots, n
   </math>
   . So,
   <math display="inline">
    \pi
   </math>
   is called a portfolio process or a trading strategy. Note that the adapted condition of
   <math display="inline">
    (\pi, c)
   </math>
   implies that the investor cannot anticipate the future. One financial implication is that "insider trading" is not allowed.
  </p>
  <p block-type="TextInlineMath">
   We assume that
   <math display="inline">
    \pi
   </math>
   is self-financing. A trading strategy is said to be self-financing if the changes in the value of the wealth result entirely from net gains or losses from the investments in the risk-free asset and the risky assets. In other words, there is no net inflow or outflow of funds. Let
   <math display="inline">
    \{V(t)\}
   </math>
   denote the wealth process of the investor, where
   <math display="inline">
    V(t) := \sum_{i=1}^{n} \pi_i(t) S_i(t) + \left(1 - \sum_{i=1}^{n} \pi_i(t)\right) B(t) - \int_0^t c(s) \, ds.
   </math>
   Then, under
   <math display="inline">
    \mathcal{P}
   </math>
   , the evolution of
   <math display="inline">
    \{V(t)\}\
   </math>
   is governed by
  </p>
  <p block-type="Equation">
   <math display="block">
    dV(t) = \sum_{i=1}^{n} \pi_i(t) dS_i(t) + \left(1 - \sum_{i=1}^{n} \pi_i(t)\right) dB(t) - c(t) dt \qquad (12)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    \beta(t) := B^{-1}(t) = \exp\left(-\int_0^t r(u) \, \mathrm{d}u\right)
   </math>
   . Then, under
   <math display="inline">
    \mathcal{P}^{\theta}
   </math>
   , the evolution of the discounted wealth process
   <math display="inline">
    \{\beta(t)V(t)\}\
   </math>
   is governed by
  </p>
  <p block-type="Equation">
   <math display="block">
    \beta(t)V(t) = v - \int_0^t \beta(s)c(s) \, \mathrm{d}s
   </math>
   <math display="block">
    + \int_0^t \beta(s)\pi^T(s)\sigma(s) \, \mathrm{d}W^\theta(t) \quad (13)
   </math>
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    v = V(0)
   </math>
   represents the initial wealth of the investor
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    \mathcal{A}(v)
   </math>
   denote the class of control processes
   <math display="inline">
    (\pi, c)
   </math>
   , for the initial wealth v, with wealth process satisfying equation
   <math display="inline">
    (13)
   </math>
   and that the wealth process
   <math display="inline">
    \{V(t)\}\
   </math>
   is nonnegative at all times in
   <math display="inline">
    [0, T^*]
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   It can be shown that for any
   <math display="inline">
    (\pi, c) \in \mathcal{A}(v)
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    E^{\theta} \left[ \int_0^{T^*} \beta(t) c(t) \, \mathrm{d}t \right] \le v \tag{14}
   </math>
  </p>
  <p block-type="TextInlineMath">
   The utility maximization problem is to select
   <math display="inline">
    (\pi, c) \in \mathcal{A}(v)
   </math>
   so as to maximize the expected discounted utility see Expected Utility Maximization,
   <b>
    Expected Utility Maximization: Duality Methods
   </b>
   from consumption over
   <math display="inline">
    [0, T^*]
   </math>
   :
  </p>
  <p block-type="Equation">
   <math display="block">
    J(v,\pi,c) := E\left[\int_0^{T^*} \frac{(c(t))^{\gamma}}{\gamma} dt\right] \qquad (15)
   </math>
  </p>
  <p block-type="TextInlineMath">
   Note that the utility only depends on the consumption level. So, in order to maximize the utility from consumption, we should increase the consumption level up to a certain bound. In other words, we only consider the consumption processes
   <math display="inline">
    c
   </math>
   such that
   <math display="inline">
    E^{\theta} \left[ \int_{0}^{T^*} \beta(t) c(t) dt \right] = v
   </math>
   . The utility maximization problem is to solve the following maximization problem:
  </p>
  <p block-type="Equation">
   <math display="block">
    J(v, \hat{\pi}, \hat{c}) = \sup_{(\pi, c) \in \mathcal{A}(v)} J(v, \pi, c) \tag{16}
   </math>
  </p>
  <p block-type="Text">
   subject to the budget constraint
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\int_{0}^{T^{*}} \Lambda(t)\beta(t)c(t) dt\right] = v \qquad (17)
   </math>
  </p>
  <p block-type="TextInlineMath">
   To solve the problem, we need to find the optimal portfolio process
   <math display="inline">
    \hat{\pi}
   </math>
   , the optimal consumption process
   <math display="inline">
    \hat{c}
   </math>
   , and the value function defined by
   <math display="inline">
    \Phi(v) :=
   </math>
   <math display="inline">
    J(v, \hat{\pi}, \hat{c}).
   </math>
  </p>
  <p block-type="Text">
   Let
   <math display="inline">
    \lambda
   </math>
   denote the Lagrange multiplier of the constrained maximization problem (3). Then, the first-order conditions of the maximization problem imply that the optimal consumption rate
   <math display="inline">
    c^*(t)
   </math>
   satisfy
  </p>
  <p block-type="Equation">
   <math display="block">
    c^*(t) = (\lambda \Lambda(t)\beta(t))^{1/(\gamma - 1)}
   </math>
   (18)
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\int_{0}^{T^{*}} \Lambda(t)\beta(t)c^{*}(t) dt\right] = v \qquad (19)
   </math>
  </p>
  <p block-type="Text">
   So, the optimal consumption process is
  </p>
  <p block-type="Equation">
   <math display="block">
    c^*(t) = (\lambda \Lambda(t)\beta(t))^{1/(\gamma - 1)}, \quad \forall t \in [0, T^*] \quad (20)
   </math>
  </p>
  <p block-type="Text">
   with the Lagrange multiplier
   <math display="inline">
    \lambda
   </math>
   determined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \lambda = \left(\frac{v}{E\left[\int_0^{T^*} \left(\Lambda(t)\beta(t)\right)^{\gamma/(\gamma-1)} \mathrm{d}t\right]}\right)^{\gamma-1} \tag{21}
   </math>
  </p>
  <p block-type="Text">
   Since the market is complete, the optimal wealth process
   <math display="inline">
    \{V^*(t)\}\
   </math>
   is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    V^*(t) = E^{\theta} \left[ \int_0^{T^*} c^*(s) \beta(s) \, \mathrm{d}s |\mathcal{F}(t) \right]
   </math>
   <br/>
   =
   <math display="block">
    \frac{1}{\Lambda(t)} E \left[ \int_0^{T^*} \Lambda(s) c^*(s) \beta(s) \, \mathrm{d}s |\mathcal{F}(t) \right]
   </math>
   (22)
  </p>
  <h2>
   The HJB Method
  </h2>
  <p block-type="Text">
   In this section, we illustrate the use of the dynamic programming approach, also called the
   <i>
    HJB
   </i>
   method, to solve the optimal consumption-investment problem described in the section The Martingale Approach. We consider the asset price dynamics in the section The Martingale Approach with timedependent coefficients replaced by constant coefficients. We impose the same assumptions and notation for control policies, utility function, and probability measures as those in the section The Martingale Approach, unless otherwise stated.
  </p>
  <p block-type="TextInlineMath">
   We consider the same problem as that in the last section on the interval
   <math display="inline">
    [t, T^*]
   </math>
   instead of
   <math display="inline">
    [0, T^*]
   </math>
   . For any
   <math display="inline">
    t \in [0, T^*]
   </math>
   , we consider admissible policies
   <math display="inline">
    (\pi, c) \in \mathcal{A}(t, v)
   </math>
   for which the wealth process
   <math display="inline">
    \{V(t)\}\
   </math>
   satisfies
  </p>
  <p block-type="Equation">
   <math display="block">
    e^{-ru}V(u)
   </math>
   <br/>
   =
   <math>
    ve^{-rt} - \int_{t}^{u} e^{-rs}c(s) ds
   </math>
   <br/>
   +
   <math>
    \int_{t}^{u} e^{-rs}\pi^{T}(s)\sigma dW^{\theta}(s) \qquad \forall u \in [t, T^{*}]
   </math>
   (23)
  </p>
  <p block-type="Text">
   Here we require that the wealth process
   <math display="inline">
    \{V(t)\}\
   </math>
   is nonnegative at all times in
   <math display="inline">
    [0, T^*]
   </math>
   .
  </p>
  <p block-type="Text">
   The value function, which is an indirect utility see
   <b>
    Expected Utility Maximization, Expected Utility
   </b>
   Maximization: Duality Methods function, is defined by
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi(t,v) := \sup_{(\pi,c)\in\mathcal{A}(t,v)} E\left[\int_t^{T^*} \frac{(c(s))^\gamma}{\gamma} \, \mathrm{d}s|\mathcal{F}(t)\right] \tag{24}
   </math>
  </p>
  <p block-type="TextInlineMath" class="has-continuation">
   Let
   <math display="inline">
    \Phi_t
   </math>
   ,
   <math display="inline">
    \Phi_v
   </math>
   , and
   <math display="inline">
    \Phi_{vv}
   </math>
   denote the derivative of
   <math display="inline">
    \Phi
   </math>
   with respect to
   <i>
    t
   </i>
   , the first and second derivatives of
   <math display="inline">
    \Phi
   </math>
   with
  </p>
  <p block-type="Text">
   respect to
   <math display="inline">
    v
   </math>
   respectively. Then, the value function satisfies the following HJB equation:
  </p>
  <p block-type="Equation">
   <math display="block">
    0 = \Phi_t + \sup_{\pi \in \mathbb{R}^n, c \in [0, \infty)} \left[ \frac{1}{2} |\pi^T \sigma|^2 \Phi_{vv} + [(rv - c) + \pi^T (\mu - r\mathbf{1})] \Phi_v + \frac{c^{\gamma}}{\gamma} \right]
   </math>
   (25)
  </p>
  <p block-type="Text">
   with the boundary and terminal conditions
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi(t,0+) = 0, \quad \forall t \in [0,T^*] \tag{26}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi(T^*, v) = 0, \quad \forall v \in (0, \infty) \tag{27}
   </math>
  </p>
  <p block-type="Text">
   With the power utility, the solution of the HJB equation is
  </p>
  <p block-type="Equation">
   <math display="block">
    \Phi(t,v) = \frac{(g(t))^{1-\gamma}}{\gamma} v^{\gamma} \tag{28}
   </math>
  </p>
  <p block-type="Text">
   where
  </p>
  <p block-type="Equation">
   <math display="block">
    g(t) = \frac{1}{K} (1 - e^{-K(T-t)})
   </math>
   (29)
  </p>
  <p block-type="Text">
   with
  </p>
  <p block-type="Equation">
   <math display="block">
    K = -\frac{\gamma}{1-\gamma} \left(r + \frac{|\theta|^2}{2(1-\gamma)}\right) \tag{30}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    \theta = \sigma^{-1}(\mu - r\mathbf{1})\tag{31}
   </math>
  </p>
  <p block-type="Text">
   In this case, the optimal consumption and portfolio processes are, respectively, given by
  </p>
  <p block-type="Equation">
   <math display="block">
    c^*(t,v) = \frac{v}{g(t)}\tag{32}
   </math>
  </p>
  <p block-type="Text">
   and
  </p>
  <p block-type="Equation">
   <math display="block">
    \pi^*(t,v) = (\sigma^T)^{-1} \left(\frac{v}{1-\gamma}\right) \theta \tag{33}
   </math>
  </p>
  <p block-type="Text" class="has-continuation">
   The HJB method for the optimal consumption-investment problem is discussed in different monographs, such as [6, 15, 18, 19], and others. These monographs focus on discussing the verification theorem for the HJB solution to the problem. Loosely speaking, if a bounded, continuous, and smooth enough function satisfies the HJB equation with associated boundary conditions, the function
  </p>
  <p block-type="Text">
   is identical to the value function. The verification theorem also provides a sufficient condition for an optimal control. For detailed discussion on the verification theorem, we refer to [18] for the diffusion case and [19] for the jump-diffusion case.
  </p>
  <p block-type="Text">
   One fundamental result behind the HJB method is called the
   <i>
    principle of optimality
   </i>
   . Informally speaking, the principle of optimality states that if you do not know the optimal expected reward at the current time
   <math display="inline">
    t
   </math>
   , but have knowledge of how well you can achieve at some later time, say
   <math display="inline">
    t + h
   </math>
   , you can evaluate the expected reward associated with the policy of adopting the control u during
   <math display="inline">
    (t, t + h)
   </math>
   , acting optimally from
   <math display="inline">
    t + h
   </math>
   onward and minimizing over the set of controls. Indeed, the HJB equation for a stochastic control problem follows "in principle" from the principle of optimality for dynamic programming. By the principle of optimality and Itô's differentiation rule, it can be shown that the value function of a stochastic optimal control problem satisfies the HJB equation if the value function satisfies certain differentiability or smoothness conditions see Monotone Schemes. However, the principle of optimality and its derivation are often overlooked in some recent literature on the stochastic optimal control theory and its financial applications, but they are certainly important. Some fundamental contributions to these aspects were due to Davis and Varaiya [5] and Elliott [7], (see also [8]). In these works, the martingale method was used to deduce the principle of optimality.
  </p>
  <h3>
   The Stochastic Maximum Principle
  </h3>
  <p block-type="TextInlineMath">
   Firstly, we suppose that the state
   <math display="inline">
    X(t) := X^{(u)}(t)
   </math>
   of a controlled diffusion in
   <math display="inline">
    \mathbb{R}^n
   </math>
   is
  </p>
  <p block-type="Equation">
   <math display="block">
    dX(t) = b(t, X(t), u(t)) dt
   </math>
   <br/>
   +
   <math>
    \sigma(t, X(t), u(t)) dW(t)
   </math>
   (34)
  </p>
  <p block-type="Text">
   where the coefficients b and
   <math display="inline">
    \sigma
   </math>
   satisfy some regularity conditions.
  </p>
  <p block-type="TextInlineMath">
   Here, the control
   <math display="inline">
    u
   </math>
   enters both the drift coefficient b and the diffusion coefficient
   <math display="inline">
    \sigma
   </math>
   . We assume that (i) the control
   <math display="inline">
    u(t) := u(t, \omega)
   </math>
   takes value in
   <math display="inline">
    U \subset \mathbb{R}^k
   </math>
   , for some positive integer k; (ii) u is
   <math display="inline">
    \{\mathcal{F}(t)\}
   </math>
   -progressively measurable and right continuous with left-hand limit (RCLL); and (iii) the controlled diffusion has a unique solution
   <math display="inline">
    \{X^{(u)}(t)\}\
   </math>
   . These controls are called
  </p>
  <p block-type="Text">
   <i>
    admissible
   </i>
   and we write
   <math display="inline">
    \mathcal{U}
   </math>
   for the set of admissible controls.
  </p>
  <p block-type="Text">
   We consider the same performance criterion as the one in the principle of optimality in the section The HJB Method and impose the same set of assumptions for the performance criterion as those in that section.
  </p>
  <p block-type="TextInlineMath">
   Let
   <math display="inline">
    H: [0, T^*] \times \mathbb{R}^n \times U \times \mathbb{R}^n \times \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) \to
   </math>
   <math display="inline">
    \Re
   </math>
   denote the Hamiltonian given by
  </p>
  <p block-type="Equation">
   <math display="block">
    H(t, X, u, p, q)
   </math>
   <br/>
   :=
   <math>
    g(t, X, u) + b^{T}(t, X, u)p + tr(\sigma^{T}(t, X, u)q)
   </math>
   <br/>
   (35)
  </p>
  <p block-type="Text">
   where
   <math display="inline">
    tr(M)
   </math>
   represents the trace of a square matrix
   <math display="inline">
    M
   </math>
   ; we suppose that
   <math display="inline">
    H
   </math>
   is differentiable in
   <math display="inline">
    X
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   The adjoint equation corresponding to
   <math display="inline">
    u
   </math>
   and
   <math display="inline">
    X^{(u)}
   </math>
   for the unknown processes
   <math display="inline">
    \{p(t)\}\
   </math>
   and
   <math display="inline">
    \{q(t)\}\
   </math>
   is given by the following backward stochastic differential equation:
  </p>
  <p block-type="Equation">
   <math display="block">
    \begin{aligned} \mathrm{d}p(t) &amp;= -\nabla H(t, X(t), u(t), p(t), q^T(t)) \,\mathrm{d}t \\ &amp;+ q^T(t) \,\mathrm{d}W(t) \end{aligned} \tag{36}
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    p(T^*) = \nabla h(X(T^*)) \tag{37}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \nabla G
   </math>
   is the gradient of a function G with respect to
   <math display="inline">
    X
   </math>
   .
   <math display="inline">
    h
   </math>
   is a concave function of
   <math display="inline">
    X
   </math>
   .
  </p>
  <p block-type="Text">
   Then, we present a sufficient maximum principle in the following proposition.
  </p>
  <p block-type="TextInlineMath">
   <b>
    Proposition 1
   </b>
   Let
   <math display="inline">
    u^* \in \mathcal{U}
   </math>
   and the corresponding controlled state process be
   <math display="inline">
    X^* := X^{(u^*)}
   </math>
   . Suppose there exists a solution
   <math display="inline">
    (p^*(t), q^*(t))
   </math>
   of the corresponding adjoint equation,
   <math display="inline">
    (36)
   </math>
   and
   <math display="inline">
    (37)
   </math>
   satisfying
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\int_0^{T^*} q^*(t)(q^*(t))^T \mathrm{d}t\right] &lt; \infty \tag{38}
   </math>
  </p>
  <p block-type="Text">
   Suppose, further, that
  </p>
  <p block-type="Text">
   1.
   <i>
    for each
   </i>
   <math display="inline">
    t \in [0, T^*]
   </math>
   ,
  </p>
  <p block-type="Equation">
   <math display="block">
    H(t, X^*(t), u^*(t), p^*(t), q^*(t))
   </math>
   <br/>
   =
   <math display="block">
    \sup_{u \in U} H(t, X^*(t), u, p^*(t), q^*(t)) \quad (39)
   </math>
  </p>
  <p block-type="Text">
   2.
   <math display="inline">
    h(X)
   </math>
   is a concave function of
   <math display="inline">
    X
   </math>
   ,
  </p>
  <p block-type="TextInlineMath">
   <i>
    3. for each
   </i>
   <math display="inline">
    t \in [0, T^*]
   </math>
   <i>
    ,
   </i>
  </p>
  <p block-type="Equation">
   <math display="block">
    H^*(X) := \max_{u \in U} H(t, X, u, p^*(t), q^*(t)) \quad (40)
   </math>
  </p>
  <p block-type="Text">
   exists and is a concave function of
   <math display="inline">
    X
   </math>
   . Then,
   <math display="inline">
    u^*
   </math>
   is an optimal control.
  </p>
  <p block-type="Text">
   For the necessary condition of the stochastic maximum principle, we refer to
   <math display="inline">
    [2, 3, 7-9, 20]
   </math>
   .
  </p>
  <p block-type="Text">
   Now, we illustrate the application of the stochastic maximum principle to the optimal consumption-investment problem presented. Here, we just present some heuristic arguments. For detailed discussions and proofs, we refer to The Martingale Approach [3]. We assume the same asset price dynamics and adopt the same notation as those in the section. As in that section, the market is complete here, so the market price of risk is uniquely determined. We consider the following utility maximization problem for both consumption and terminal wealth:
  </p>
  <p block-type="Equation">
   <math display="block">
    J_1(v) := \sup_{(\pi,c)\in\mathcal{A}(v)} E\left[\int_0^{T^*} \frac{(c(t))^{\gamma_1}}{\gamma_1} \mathrm{d}t + \frac{(V(T^*))^{\gamma_2}}{\gamma_2}\right] \tag{41}
   </math>
  </p>
  <p block-type="Text">
   In this case, the Hamiltonian is
  </p>
  <p block-type="Equation">
   <math display="block">
    H(t, V, (\pi, c), p, q)
   </math>
   <math display="block">
    = \frac{c^{\gamma_1}}{\gamma_1} + p \left[ r(t)V - c + \pi^T (\mu(t) - r(t)\mathbf{1}) \right]
   </math>
   <math display="block">
    + q^T \sigma^T (t)\pi \tag{42}
   </math>
  </p>
  <p block-type="Text">
   and the adjoint equation has the following form:
  </p>
  <p block-type="Equation">
   <math display="block">
    dp(t) = -r(t)p(t) dt + qT(t) dW(t) \quad (43)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    p(T) = (V^*(T))^{\gamma_2 - 1} \tag{44}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Define an exponential process
   <math display="inline">
    \{Z_{\bar{\theta}}(t)\}
   </math>
   by
  </p>
  <p block-type="Equation">
   <math display="block">
    Z_{\bar{\theta}}(t) := \exp\left(-\int_0^t \bar{\theta}^T(s) \, \mathrm{d}W(s) - \frac{1}{2} \int_0^t |\bar{\theta}(s)|^2 \mathrm{d}s\right) \tag{45}
   </math>
  </p>
  <p block-type="TextInlineMath">
   where
   <math display="inline">
    \{\bar{\theta}(t)\}\
   </math>
   is a measurable,
   <math display="inline">
    \{\mathcal{F}(t)\}\
   </math>
   -adapted process, which is uniformly bounded in
   <math display="inline">
    (t, \omega) \in [0, T^*] \times
   </math>
   Ω. Write
   <math display="inline">
    \xi_{\bar{\theta}}(t) := \beta(t) Z_{\bar{\theta}}(t)
   </math>
   , for each
   <math display="inline">
    t \in [0, T^*]
   </math>
   .
  </p>
  <p block-type="Text">
   It can be shown that an adapted solution to the adjoint equation
   <math display="inline">
    (42)
   </math>
   and
   <math display="inline">
    (43)
   </math>
   is given by the processes
  </p>
  <p block-type="Equation">
   <math display="block">
    p(t) = p(0)\xi_{\bar{\theta}}(t), \quad q(t) = -p(0)\xi_{\bar{\theta}}(t)\theta(t) \quad (46)
   </math>
  </p>
  <p block-type="Text">
   From the first-order conditions of maximizing the Hamiltonian
   <math display="inline">
    (5)
   </math>
   , one obtains
  </p>
  <p block-type="Equation">
   <math display="block">
    c^*(t) = (p(0)\xi_{\bar{\theta}}(t))^{1/(\gamma_1 - 1)}
   </math>
   (47)
  </p>
  <p block-type="Equation">
   <math display="block">
    \bar{\theta}(t) = \sigma^{-1}(\mu(t) - r(t)\mathbf{1}) = \theta(t) \qquad (48)
   </math>
  </p>
  <p block-type="TextInlineMath">
   To simplify the notation, write
   <math display="inline">
    \xi(t) := \xi_{\bar{\theta}}(t)
   </math>
   , for each
   <math display="inline">
    t \in [0, T^*]
   </math>
   . Define a function
   <math display="inline">
    \mathcal{X} : (0, \infty) \to
   </math>
   <math display="inline">
    (0, \infty)
   </math>
   by
  </p>
  <p block-type="Equation">
   <math display="block">
    \mathcal{X}(y)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    :=E\left[\int_{0}^{T^{*}}\xi(s)(y\xi(s))^{\frac{1}{\gamma_{1}-1}}\mathrm{d}s+\xi(T^{*})(y\xi(T^{*}))^{\frac{1}{\gamma_{2}-1}}\right]
   </math>
   (49)
  </p>
  <p block-type="TextInlineMath">
   Since
   <math display="inline">
    \mathcal{X}
   </math>
   is strictly decreasing and surjective, its inverse
   <math display="inline">
    \mathcal{Y} := \mathcal{X}^{-1} : (0, \infty) \to (0, \infty)
   </math>
   exists and is strictly decreasing. We then conjecture that the optimal controls
   <math display="inline">
    (\pi^*, c^*)
   </math>
   satisfy
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\int_0^{T^*} \xi(s)c^*(s) \, \mathrm{d}s + \xi(T^*)V^*(T^*)\right] = v \tag{50}
   </math>
  </p>
  <p block-type="TextInlineMath">
   Under this conjecture,
   <math display="inline">
    p(0) = \mathcal{Y}(v)
   </math>
   .
  </p>
  <p block-type="TextInlineMath">
   By the martingale representation theorem, there exists a progressively measurable process
   <math display="inline">
    \eta
   </math>
   :
   <math display="inline">
    [0, T^*] \times \Omega \to \mathbb{R}^n \text{ with } \int_0^{T^*} |\eta(s)|^2 \mathrm{d}s &lt; \infty, \ \mathcal{P} \text{ a.s.}
   </math>
   such that
  </p>
  <p block-type="Equation">
   <math display="block">
    E\left[\int_{0}^{T^{*}} \xi(s)(\mathcal{Y}(v)\xi(s))^{\frac{1}{\gamma_{1}-1}} \mathrm{d}s
   </math>
   <br/>
   +
   <math display="block">
    \xi(T^{*})(\mathcal{Y}(v)\xi(T^{*}))^{\frac{1}{\gamma_{2}-1}} |\mathcal{F}(t)\right]
   </math>
   <br/>
   =
   <math display="block">
    v + \int_{0}^{t} \eta^{T}(s) \, \mathrm{d}W(s) \tag{51}
   </math>
  </p>
  <p block-type="Text">
   Then, it can be shown that
  </p>
  <p block-type="Equation">
   <math display="block">
    \pi^*(t) = \sigma^{-1}(t) \left[ \frac{\eta(t)}{\xi(t)} + V^*(t)\theta(t) \right] \qquad (52)
   </math>
  </p>
  <p block-type="Equation">
   <math display="block">
    c^*(t) = (\mathcal{Y}(v)\xi(t))^{\frac{1}{\gamma_1 - 1}}
   </math>
   (53)
  </p>
  <p block-type="TextInlineMath">
   where the optimal wealth process {
   <i>
    V
   </i>
   <sup>
    ∗
   </sup>
   <i>
    (t)
   </i>
   } is given by
  </p>
  <p block-type="Equation">
   <math display="block">
    V^*(t) = \frac{1}{\xi(t)} E\left[\int_t^{T^*} \xi(s) (\mathcal{Y}(v)\xi(s))^{\frac{1}{\gamma_1 - 1}} ds + \xi(T^*)(\mathcal{Y}(v)\xi(T^*))^{\frac{1}{\gamma_2 - 1}} |\mathcal{F}(t)\right]
   </math>
   (54)
  </p>
  <h2>
   <b>
    References
   </b>
  </h2>
  <p block-type="ListGroup" class="has-continuation">
   <ul>
    <li block-type="ListItem">
     [1] Bellman, R.S. (1957).
     <i>
      Dynamic Programming
     </i>
     , Princeton University Press, Princeton.
    </li>
    <li block-type="ListItem">
     [2] Bensoussan, A. (1981).
     <i>
      Lectures on Stochastic Control
     </i>
     , Lecture Notes in Mathematics, 972, Springer-Verlag, Berlin, pp. 1–62.
    </li>
    <li block-type="ListItem">
     [3] Cadenillas, A. &amp; Karatzas, I. (1995). The stochastic maximum principle for linear, convex optimal control with random coefficients,
     <i>
      SIAM Journal of Control and Optimization
     </i>
     <b>
      33
     </b>
     (2), 590–624.
    </li>
    <li block-type="ListItem">
     [4] Cox, J.C. &amp; Huang, C.-F. (1989). Optimal consumption and portfolio policies when asset prices follow a diffusion process,
     <i>
      Journal of Economic Theory
     </i>
     <b>
      49
     </b>
     , 33–83.
    </li>
    <li block-type="ListItem">
     [5] Davis, M.H.A. &amp; Varaiya, P.P. (1973). Dynamic programming conditions for partially observable stochastic systems,
     <i>
      SIAM Journal on Control and Optimization
     </i>
     <b>
      11
     </b>
     , 226–261.
    </li>
    <li block-type="ListItem">
     [6] Duffie, D. (1996).
     <i>
      Dynamic Asset Pricing Theory
     </i>
     , 2nd Edition, Princeton University Press, Princeton.
    </li>
    <li block-type="ListItem">
     [7] Elliott, R.J. (1977). The optimal control of a stochastic system,
     <i>
      SIAM Journal on Control and Optimization
     </i>
     <b>
      15
     </b>
     (5), 756–778.
    </li>
    <li block-type="ListItem">
     [8] Elliott, R.J. (1982).
     <i>
      Stochastic Calculus and Applications
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [9] Elliott, R.J. &amp; Kohlmann, M. (1994). The second order minimum principle and adjoint process,
     <i>
      Stochastics and Stochastics Reports
     </i>
     <b>
      46
     </b>
     , 25–39.
    </li>
    <li block-type="ListItem">
     [10] Elliott, R.J. &amp; Kopp, P.E. (2005).
     <i>
      Mathematics of Financial Markets
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
   </ul>
  </p>
  <p block-type="ListGroup">
   <ul>
    <li block-type="ListItem">
    </li>
    <li block-type="ListItem">
     [11] Fleming, W.H. &amp; Rishel, R.W. (1975).
     <i>
      Deterministic and Stochastic Optimal Control
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [12] Fleming, W.H. &amp; Soner, H.M. (1993).
     <i>
      Controlled Markov Processes and Viscosity Solutions
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [13] Karatzas, I., Lehoczky, J.P. &amp; Shreve, S.E. (1987). Optimal portfolio and consumption decisions for a "small investor" on a finite horizon,
     <i>
      SIAM Journal of Control and Optimization
     </i>
     <b>
      25
     </b>
     , 1557–1586.
    </li>
    <li block-type="ListItem">
     [14] Karatzas, I. &amp; Shreve, S.E. (1998).
     <i>
      Methods of Mathematical Finance
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [15] Korn, R. (1997).
     <i>
      Optimal Portfolios: Stochastic Models for Optimal Investment and Risk Management in Continuous Time
     </i>
     , World Scientific, Singapore.
    </li>
    <li block-type="ListItem">
     [16] Merton, R.C. (1969). Lifetime portfolio selection under uncertainty: the continuous-time model,
     <i>
      Review of Economics and Statistics
     </i>
     <b>
      51
     </b>
     , 247–257.
    </li>
    <li block-type="ListItem">
     [17] Merton, R.C. (1971). Optimal consumption and portfolio rules in a continuous time model,
     <i>
      Journal of Economic Theory
     </i>
     <b>
      3
     </b>
     , 373–413.
    </li>
    <li block-type="ListItem">
     [18] Øksendal, B. (2003).
     <i>
      Stochastic Differential Equations: An Introduction with Applications
     </i>
     , 6th Edition, Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [19] Øksendal, B. &amp; Sulem, A. (2004).
     <i>
      Applied Stochastic Control of Jump Diffusions
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
    <li block-type="ListItem">
     [20] Peng, S. (1990). A general stochastic maximum principle for optimal control problems,
     <i>
      SIAM Journal of Control and Optimization
     </i>
     <b>
      28
     </b>
     , 966–979.
    </li>
    <li block-type="ListItem">
     [21] Yong, J. &amp; Zhou, X.Y. (1999).
     <i>
      Stochastic Control
     </i>
     , Springer, Berlin, Heidelberg, New York.
    </li>
   </ul>
  </p>
  <h2>
   <b>
    Related Articles
   </b>
  </h2>
  <p block-type="Text">
   <b>
    Expected Utility Maximization
   </b>
   ;
   <b>
    Expected Utility Maximization: Duality Methods
   </b>
   ;
   <b>
    Markov Processes
   </b>
   ;
   <b>
    Merton Problem
   </b>
   ;
   <b>
    Monotone Schemes
   </b>
   ;
   <b>
    Partial Differential Equations
   </b>
   .
  </p>
  <p block-type="Text">
   ROBERT J. ELLIOTT &amp; TAK KUEN SIU
  </p>
 </body>
</html>
