{
  "blocks": [
    {
      "id": "/page/0/SectionHeader/0",
      "block_type": "SectionHeader",
      "html": "<h2><b>Measurements Errors </b></h2>",
      "page": 267,
      "polygon": [
        [
          73.4379746835443,
          71.2027833001988
        ],
        [
          249.171875,
          71.2027833001988
        ],
        [
          249.171875,
          88.083251953125
        ],
        [
          73.4379746835443,
          88.083251953125
        ]
      ],
      "bbox": [
        73.4379746835443,
        71.2027833001988,
        249.171875,
        88.083251953125
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Text/1",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">In physical sciences, errors in measurement are present everywhere owing to the unavoidable inaccuracy of the measuring devices, and most often they are thought of as being independent and centered variables, usually with a density, and which add up to the \"true\" values of the quantity of interest. In finance, the quantities of interest are usually prices, which can be exactly observed by virtually any person. Some factual errors, like wrong records, may occur, but those are relatively infrequent and easy to detect (giving rise to the \"cleaning\" of data), and may be modeled with independent centered variables with no density and a rather large mass at <math display=\"inline\">0</math> (= no error).</p>",
      "page": 267,
      "polygon": [
        [
          74.1873417721519,
          111.67594433399603
        ],
        [
          286.31640625,
          111.67594433399603
        ],
        [
          286.31640625,
          258.0830078125
        ],
        [
          74.1873417721519,
          258.0830078125
        ]
      ],
      "bbox": [
        74.1873417721519,
        111.67594433399603,
        286.31640625,
        258.0830078125
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Text/2",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">However, mathematical models in finance are often continuous-time models describing the dynamics of a process <math display=\"inline\">(X_t : t &gt; 0)</math>. This process is the basic underlying \"price\", or log-price, of an asset, or several assets if it is multivariate, and it satisfies the rules of mathematical finance, and, in particular, it is a semimartingale by the first fundamental theorem of asset pricing. This process cannot be exactly observed because (i) any observed price is an integer (a number of elementary units, like a cent or a hundredth of a cent); and (ii) one may think that extraneous reasons, like some erratic behavior of price makers or erratic orders, \"locally\" change the price in a way that does not affect the underlying and should not influence the model for it.</p>",
      "page": 267,
      "polygon": [
        [
          74.1873417721519,
          259.73974609375
        ],
        [
          286.75,
          259.73974609375
        ],
        [
          286.75,
          430.3837890625
        ],
        [
          74.1873417721519,
          430.3837890625
        ]
      ],
      "bbox": [
        74.1873417721519,
        259.73974609375,
        286.75,
        430.3837890625
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/TextInlineMath/3",
      "block_type": "TextInlineMath",
      "html": "<p block-type=\"TextInlineMath\">In practice, one then observes prices <math display=\"inline\">Z_t</math> that are possibly different from the underlying, or \"efficient\", prices <math display=\"inline\">X_t</math>, and at finitely many times <math display=\"inline\">t_0, t_1, \\dots, t_n</math>. This may look like an academic difference and it may appear that, in practice, we ought to consider <math display=\"inline\">Z_t</math>and not <math display=\"inline\">X_t</math> at all. However, currently we have highfrequency or very high-frequency data, like tick data, and it is empirically clear that we need to differentiate between <math display=\"inline\">X_t</math> and <math display=\"inline\">Z_t</math>. For example, when <math display=\"inline\">Z_t = X_t</math>(no error), the so-called realized volatility, which is the sum</p>",
      "page": 267,
      "polygon": [
        [
          74.1873417721519,
          432.46322067594434
        ],
        [
          286.2582278481013,
          432.46322067594434
        ],
        [
          286.2582278481013,
          556.2958984375
        ],
        [
          74.1873417721519,
          556.2958984375
        ]
      ],
      "bbox": [
        74.1873417721519,
        432.46322067594434,
        286.2582278481013,
        556.2958984375
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Equation/4",
      "block_type": "Equation",
      "html": "<p block-type=\"Equation\"><math display=\"block\">V_n = (Z_{t1} - Z_{t_0})^2 + (Z_{t_2} - Z_{t_1})^2</math><br/>+ \\dots + (Z_{t_n} - Z_{t_{n-1}})^2 (1)</p>",
      "page": 267,
      "polygon": [
        [
          119.888671875,
          572.86328125
        ],
        [
          285.3046875,
          572.86328125
        ],
        [
          285.3046875,
          603.052734375
        ],
        [
          119.888671875,
          603.052734375
        ]
      ],
      "bbox": [
        119.888671875,
        572.86328125,
        285.3046875,
        603.052734375
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Text/5",
      "block_type": "Text",
      "html": "<p block-type=\"Text\" class=\"has-continuation\">is a quantity that converges, when the observations become more and more frequent over a fixed time interval <math display=\"inline\">[0, T]</math>, to a given quantity V (the integrated</p>",
      "page": 267,
      "polygon": [
        [
          74.1873417721519,
          612.2568359375
        ],
        [
          286.4609375,
          612.2568359375
        ],
        [
          286.4609375,
          644.6552734375
        ],
        [
          74.1873417721519,
          644.6552734375
        ]
      ],
      "bbox": [
        74.1873417721519,
        612.2568359375,
        286.4609375,
        644.6552734375
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/TextInlineMath/6",
      "block_type": "TextInlineMath",
      "html": "<p block-type=\"TextInlineMath\">volatility over <math display=\"inline\">[0, T]</math> when <math display=\"inline\">X_t</math> is continuous, and, more generally, to the quadratic variation of the process <math display=\"inline\">X_t</math>). Moreover, at least when observation times are equidistant or almost equidistant, the \"rate of convergence\" is <math display=\"inline\">\\sqrt{n}</math>, which means that <math display=\"inline\">\\sqrt{n}</math> (<math display=\"inline\">V_n</math> – <math display=\"inline\">V</math>) does neither explode nor go to 0 (and in fact it usually converges to some limit). Now, all empirical studies show that <math display=\"inline\">V_n</math> increases indefinitely when the frequency of observations increases: this goes back to [15] and [8], or more recently to, for example, [2] or [12]. Hence, the observations <math display=\"inline\">Z_t</math>differ from the underlying <math display=\"inline\">X_t</math> in a nonnegligible way, and the difference <math display=\"inline\">\\epsilon_t = Z_t - X_t</math> is referred to as the microstructure noise, or market frictions, and should be taken into account.</p>",
      "page": 267,
      "polygon": [
        [
          301.24556962025315,
          71.2027833001988
        ],
        [
          513.375,
          71.2027833001988
        ],
        [
          513.375,
          241.515625
        ],
        [
          301.24556962025315,
          241.515625
        ]
      ],
      "bbox": [
        301.24556962025315,
        71.2027833001988,
        513.375,
        241.515625
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Text/7",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">We then have to give a model for this microstructure noise, on top of these mimartingale model for <math display=\"inline\">X_t</math>, like Black-Scholes, possibly with stochastic volatility, or jump diffusion, or others. The microstructure noise models that have been proposed so far in the literature are mostly the following:</p>",
      "page": 267,
      "polygon": [
        [
          301.24556962025315,
          244.33797216699801
        ],
        [
          513.3164556962025,
          244.33797216699801
        ],
        [
          513.3164556962025,
          312.203125
        ],
        [
          301.24556962025315,
          312.203125
        ]
      ],
      "bbox": [
        301.24556962025315,
        244.33797216699801,
        513.3164556962025,
        312.203125
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/Text/267",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">1. The errors <math display=\"inline\">\\epsilon_t</math> are independent, centered, identically distributed variables, and also independent of the underlying <math display=\"inline\">X_t</math>. This is the simplest and most well-understood situation, in which the uncorrected realized volatility <math display=\"inline\">V_n</math> increases to <math display=\"inline\">\\infty</math>at the rate <i>n</i>. When the efficient price <math display=\"inline\">X_t</math> is continuous and one is interested in estimating the integrated volatility <math display=\"inline\">V</math> over <math display=\"inline\">[0, T]</math>, that is, the quadratic variation of <math display=\"inline\">X_t</math> at time T, a variety of variants of <math display=\"inline\">V_n</math> have been proposed, using various smoothing kernels: the best methods give a rate of convergence which is <math display=\"inline\">n^{1/4}</math>, known as the <i>optimal rate</i> when <math display=\"inline\">X_t</math> is a Brownian motion, and asymptotic variances which are nearly optimal. Note that no distributional assumption is made on the errors, and, in particular, the variance of the errors is not supposed to be known.</p>",
      "page": 267,
      "polygon": [
        [
          302.0703125,
          323.0357852882704
        ],
        [
          513.3164556962025,
          323.0357852882704
        ],
        [
          513.3164556962025,
          518.375
        ],
        [
          302.0703125,
          518.375
        ]
      ],
      "bbox": [
        302.0703125,
        323.0357852882704,
        513.3164556962025,
        518.375
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/0/TextInlineMath/9",
      "block_type": "TextInlineMath",
      "html": "<p block-type=\"TextInlineMath\">Such results can be extended to (i) errors that are not independent, but present some sort of \"weak\" dependence, and are still globally independent of <math display=\"inline\">X_t</math>; (ii) errors that are not identically distributed, provided their second moments stay constant; (iii) errors that are multiplicative instead of additive, meaning that the variables <math display=\"inline\">Z_t/X_t</math> are independent instead of the <math display=\"inline\">Z_t - X_t</math>, and \"centered\" is replaced by \"with mean 1\". The literature about this question is huge, and among many articles we can quote <math display=\"inline\">[1, 3-6, 13, 14, 16, 17]</math>.</p>",
      "page": 267,
      "polygon": [
        [
          318.4810126582278,
          519.4055666003976
        ],
        [
          513.3164556962025,
          519.4055666003976
        ],
        [
          513.3164556962025,
          644.287109375
        ],
        [
          318.4810126582278,
          644.287109375
        ]
      ],
      "bbox": [
        318.4810126582278,
        519.4055666003976,
        513.3164556962025,
        644.287109375
      ],
      "section_hierarchy": {
        "2": "/page/0/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/1/SectionHeader/0",
      "block_type": "SectionHeader",
      "html": "<h2><b>2</b> <b>Measurements Errors</b></h2>",
      "page": 511,
      "polygon": [
        [
          78.19140625,
          45.49365234375
        ],
        [
          199.61679077148438,
          45.49365234375
        ],
        [
          199.61679077148438,
          56.70166015625
        ],
        [
          78.19140625,
          56.70166015625
        ]
      ],
      "bbox": [
        78.19140625,
        45.49365234375,
        199.61679077148438,
        56.70166015625
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/1/ListGroup/509",
      "block_type": "ListGroup",
      "html": "<p block-type=\"ListGroup\"><ul><li block-type=\"ListItem\">2. The previous models more or less take care of reason (ii) for microstructure noise. As for reason (i), and if it were the only reason for the noise, one should simply take the observation <i>Zt</i> to be <i>Zt</i> = <i>α</i>[<i>Xt</i> <i>/α</i>], where [<i>x</i>] denotes the integer part of any number <i>x</i>, and <i>α</i> is the precision with which prices are written (1 cent or 1<i>/</i>100 cent for example). Then again some results for the estimation of the integrated volatility <i>V</i> are available, asymptotically when <i>n</i> increases whereas the rounding level <i>α</i> decreases to 0. Very few papers have been devoted so far to this subject; see, for example, [7].</li><li block-type=\"ListItem\">3. Now, with the frequency attained by observations of prices, one cannot satisfactorily consider that the rounding level <i>α</i> is \"small\", and we have a serious problem here: if <i>α</i> is fixed and the frequency increases, then the realized volatility increases to <sup>∞</sup> with the rate <sup>√</sup><i>n</i>, and the integrated volatility is no longer identifiable. The best one can know about the process <i>Xt</i> is the times at which it crosses the level <i>iα</i> for any integer <i>i</i>, or equivalently the family of so-called \"local times\" <i>L(iα)</i> at levels <i>iα</i> for all integers <i>i</i> (and time <i>T</i> , the time horizon). This is very far from <i>V</i> , although <i>V</i> is indeed the limit of <i>α</i> <i><sup>i</sup></i>≥<sup>0</sup> <i>L(iα)</i> as <i>α</i> → 0 (see [9]).</li><li block-type=\"ListItem\">4. In practice, models like (1) above are not fully satisfactory because they do not account for the—obvious and ubiquitous—fact that rounding-off occurs. Models like (2) with a fixed level <i>α</i> are not satisfactory either, on a mathematical level (they do not permit to retrieve the quantities of interest like <i>V</i> ) and on an empirical level they give rise to sequences of prices that stay constant for relatively many successive times and then oscillate wildly between two adjacent multiples of <i>α</i>, then stay constant again: this is not quite observed in practice; usually prices stay constant for a few successive times, with occasionally a jump back and forth of one unit. Probably a more reasonable model consists in a mixture of (1) and (2): we have additive errors <i>t</i> , and then rounding, that is, we observe <i>Zt</i> = <i>α(</i>[<i>(Xt</i> + <i>t)/α</i>]. It turns out—perhaps surprisingly—that if the errors <i>t</i> have a density that is positive on an arbitrarily located interval of length at least <i>α</i>, then it is possible to retrieve the integrated volatility <i>V</i> in pretty much the same way as when rounding</li></ul></p>",
      "page": 511,
      "polygon": [
        [
          78.76953125,
          71.88751220703125
        ],
        [
          292.67578125,
          71.88751220703125
        ],
        [
          292.67578125,
          644.7618103027344
        ],
        [
          78.76953125,
          644.7618103027344
        ]
      ],
      "bbox": [
        78.76953125,
        71.88751220703125,
        292.67578125,
        644.7618103027344
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/1/Text/4",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">is absent, and at the same rate. Some recent attempts in this direction may be found in [11] and [10].</p>",
      "page": 511,
      "polygon": [
        [
          324.11932373046875,
          71.6539306640625
        ],
        [
          518.0,
          71.6539306640625
        ],
        [
          518.0,
          104.46026611328125
        ],
        [
          324.11932373046875,
          104.46026611328125
        ]
      ],
      "bbox": [
        324.11932373046875,
        71.6539306640625,
        518.0,
        104.46026611328125
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/1/Text/511",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">5. One might also look for more microeconomically oriented models. That is, in addition to the rounding effect, which is probably unavoidable, one tries to model the behavior of the various agents so that the observed prices <i>Zt</i> behave according to empirical data, whereas the underlying <i>Xt</i> , which is a sort of \"local limit\" of the <i>Zt</i>'s, satisfies a model that fits the usual requirements of mathematical finance (no-arbitrage, completeness, and so on). This looks like the two-scale diffusions, but so far this has not really been put in use, because of the mathematical difficulties, and even more because the underlying microeconomics is still at a rather rudimentary level.</p>",
      "page": 511,
      "polygon": [
        [
          306.6953125,
          106.206787109375
        ],
        [
          518.2890625,
          106.206787109375
        ],
        [
          518.2890625,
          264.8416748046875
        ],
        [
          306.6953125,
          264.8416748046875
        ]
      ],
      "bbox": [
        306.6953125,
        106.206787109375,
        518.2890625,
        264.8416748046875
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/0"
      },
      "images": {}
    },
    {
      "id": "/page/1/SectionHeader/6",
      "block_type": "SectionHeader",
      "html": "<h2><b>References</b></h2>",
      "page": 511,
      "polygon": [
        [
          306.8399963378906,
          281.29364013671875
        ],
        [
          358.95721435546875,
          281.29364013671875
        ],
        [
          358.95721435546875,
          292.5016174316406
        ],
        [
          306.8399963378906,
          292.5016174316406
        ]
      ],
      "bbox": [
        306.8399963378906,
        281.29364013671875,
        358.95721435546875,
        292.5016174316406
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/1/ListGroup/510",
      "block_type": "ListGroup",
      "html": "<p block-type=\"ListGroup\"><ul><li block-type=\"ListItem\">[1] Ait-Sahalia, Y., Mykland, P.A. &amp; Zhang, L. (2005). How often to sample a continuous-time process in the presence of market microstructure noise, <i>Review of</i> <i>Financial Studies</i> <b>18</b>, 351–416.</li><li block-type=\"ListItem\">[2] Andersen, T.G., Bollerslev, T., Diebold, F.X. &amp; Labys, P. (2000). Great realizations, <i>Risk</i> <b>13</b>, 105–108.</li><li block-type=\"ListItem\">[3] Andersen, T.G., Bollerslev, T., Diebold, F.X. &amp; Labys, P. (2001). The distribution of realized exchange rate volatility, <i>Journal of the American Sta</i><i>tistical Association</i> <b>96</b>, 42–55.</li><li block-type=\"ListItem\">[4] Bandi, F.M. &amp; Russell, J.R. (2006b). Separating microstructure noise from volatility, <i>Journal of Financial</i> <i>Economics</i> <b>79</b>, 655–692.</li><li block-type=\"ListItem\">[5] Barndorff-Nielsen, O.E., Hansen, P.R., Lunde, A. &amp; Shephard, N. (2008). Designing realised kernels to measure ex-post variation of equity prices in the presence of noise, <i>Econometrica</i> <b>76</b>(6), 1481–1536.</li><li block-type=\"ListItem\">[6] Barndorff-Nielsen, O.E. &amp; Shephard, N. (2002). Econometric analysis of realised volatility and its use in estimating stochastic volatility models, <i>Journal of the Royal</i> <i>Statistical Society, B</i> <b>64</b>, 253–280.</li><li block-type=\"ListItem\">[7] Delattre, S. &amp; Jacod, J. (1997). A central limit theorem for normalized functions of the increments of a diffusion process, in the presence of round-off errors, <i>Bernoulli</i> <b>3</b>, 1–28.</li><li block-type=\"ListItem\">[8] Hasbrouck, J. (1993). Assessing the quality of a security market: a new approach to transaction-cost measurement, <i>Review of Financial Studies</i> <b>6</b>, 191–212.</li><li block-type=\"ListItem\">[9] Jacod, J. (1996). La variation quadratique du Brownien en presence d'erreurs d'arrondi, ´ <i>Ast´erisque</i> <b>236</b>, 155–162.</li><li block-type=\"ListItem\">[10] Jacod, J., Li, Y., Mykland, P.A., Podolskij, M. &amp; Vetter, M. (2009). Microstructure noise in the continuous</li></ul></p>",
      "page": 511,
      "polygon": [
        [
          305.25,
          307.44793701171875
        ],
        [
          517.1328125,
          307.44793701171875
        ],
        [
          517.1328125,
          644.6552734375
        ],
        [
          305.25,
          644.6552734375
        ]
      ],
      "bbox": [
        305.25,
        307.44793701171875,
        517.1328125,
        644.6552734375
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/PageHeader/0",
      "block_type": "PageHeader",
      "html": "",
      "page": 133,
      "polygon": [
        [
          390.8125,
          45.49365234375
        ],
        [
          512.3054809570312,
          45.49365234375
        ],
        [
          512.3054809570312,
          56.70166015625
        ],
        [
          390.8125,
          56.70166015625
        ]
      ],
      "bbox": [
        390.8125,
        45.49365234375,
        512.3054809570312,
        56.70166015625
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/Text/1",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">case: the pre-averaging approach, <i>Stochastic Processes</i> <i>and their Applications</i> <b>119</b>(7), 2249–2276.</p>",
      "page": 133,
      "polygon": [
        [
          97.86000061035156,
          71.92462158203125
        ],
        [
          285.1402587890625,
          71.92462158203125
        ],
        [
          285.1402587890625,
          91.028564453125
        ],
        [
          97.86000061035156,
          91.028564453125
        ]
      ],
      "bbox": [
        97.86000061035156,
        71.92462158203125,
        285.1402587890625,
        91.028564453125
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/ListGroup/132",
      "block_type": "ListGroup",
      "html": "<p block-type=\"ListGroup\"><ul><li block-type=\"ListItem\">[11] Li, Y. &amp; Mykland, P.A. (2007). Are volatility estimators robust with respect to modeling assumptions? <i>Bernoulli</i> <b>13</b>, 601–622.</li><li block-type=\"ListItem\">[12] Mykland, P.A. &amp; Zhang, L. (2005). Discussion of paper \"a selective overview of nonparametric methods in financial econometrics\" by J. Fan, <i>Statistical Science</i> <b>20</b>, 347–350.</li><li block-type=\"ListItem\">[13] Oomen, R.A.A. (2005). Properties of bias corrected realized variance in calendar time and business time, <i>Journal of Financial Econometrics</i> <b>3</b>, 258–272.</li><li block-type=\"ListItem\">[14] Podolskij, M. &amp; Vetter, M. (2006). Estimation of volatility functionals in the simultaneous presence of</li></ul></p>",
      "page": 133,
      "polygon": [
        [
          74.8671875,
          93.18792724609375
        ],
        [
          286.75,
          93.18792724609375
        ],
        [
          286.75,
          218.10748291015625
        ],
        [
          74.8671875,
          218.10748291015625
        ]
      ],
      "bbox": [
        74.8671875,
        93.18792724609375,
        286.75,
        218.10748291015625
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/Text/6",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">microstructure noise and jumps, Technical Report, Ruhr-Universitat, Bochum. To appear in ¨ <i>Bernoulli</i>.</p>",
      "page": 133,
      "polygon": [
        [
          325.02142333984375,
          71.97607421875
        ],
        [
          512.2613525390625,
          71.97607421875
        ],
        [
          512.2613525390625,
          91.02716064453125
        ],
        [
          325.02142333984375,
          91.02716064453125
        ]
      ],
      "bbox": [
        325.02142333984375,
        71.97607421875,
        512.2613525390625,
        91.02716064453125
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/ListGroup/133",
      "block_type": "ListGroup",
      "html": "<p block-type=\"ListGroup\"><ul><li block-type=\"ListItem\">[15] Roll, R. (1984). A simple model of the implicit bid-ask spread in an efficient market, <i>Journal of Finance</i> <b>39</b>, 1127–1139.</li><li block-type=\"ListItem\">[16] Zhang, L. (2006). Efficient estimation of stochastic volatility using noisy observations: a multi-scale approach, <i>Bernoulli</i> <b>12</b>, 1019–1043.</li><li block-type=\"ListItem\">[17] Zhang, L., Mykland, P.A. &amp; A¨ıt-Sahalia, Y. (2005). A tale of two time scales: determining integrated volatility with noisy high-frequency data, <i>Journal of the American</i> <i>Statistical Association</i> <b>100</b>, 1394–1411.</li></ul></p>",
      "page": 133,
      "polygon": [
        [
          301.203125,
          93.54791259765625
        ],
        [
          512.322265625,
          93.54791259765625
        ],
        [
          512.322265625,
          198.7274169921875
        ],
        [
          301.203125,
          198.7274169921875
        ]
      ],
      "bbox": [
        301.203125,
        93.54791259765625,
        512.322265625,
        198.7274169921875
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    },
    {
      "id": "/page/2/Text/10",
      "block_type": "Text",
      "html": "<p block-type=\"Text\">JEAN JACOD</p>",
      "page": 133,
      "polygon": [
        [
          450.359375,
          208.74755859375
        ],
        [
          500.3956604003906,
          208.74755859375
        ],
        [
          500.3956604003906,
          218.461181640625
        ],
        [
          450.359375,
          218.461181640625
        ]
      ],
      "bbox": [
        450.359375,
        208.74755859375,
        500.3956604003906,
        218.461181640625
      ],
      "section_hierarchy": {
        "2": "/page/1/SectionHeader/6"
      },
      "images": {}
    }
  ],
  "page_info": {
    "0": {
      "bbox": [
        0.0,
        0.0,
        592.0,
        754.0
      ],
      "polygon": [
        [
          0.0,
          0.0
        ],
        [
          592.0,
          0.0
        ],
        [
          592.0,
          754.0
        ],
        [
          0.0,
          754.0
        ]
      ]
    },
    "1": {
      "bbox": [
        0.0,
        0.0,
        592.0,
        754.0
      ],
      "polygon": [
        [
          0.0,
          0.0
        ],
        [
          592.0,
          0.0
        ],
        [
          592.0,
          754.0
        ],
        [
          0.0,
          754.0
        ]
      ]
    },
    "2": {
      "bbox": [
        0.0,
        0.0,
        592.0,
        754.0
      ],
      "polygon": [
        [
          0.0,
          0.0
        ],
        [
          592.0,
          0.0
        ],
        [
          592.0,
          754.0
        ],
        [
          0.0,
          754.0
        ]
      ]
    }
  }
}