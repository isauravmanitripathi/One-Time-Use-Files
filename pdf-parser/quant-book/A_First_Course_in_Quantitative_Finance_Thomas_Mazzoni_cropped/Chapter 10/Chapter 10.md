✖

# **10 Behavioral Finance**

Expected utility is a normative theory, which means it explains how agents should behave in order to be perceived as rational decision makers. In fact many deviations from this theoretical blueprint can be observed in reality. This is where psychological influences and mechanisms complement the agent's decision process. Incorporation of such mechanisms paves the road to a descriptive theory of actually observed decision behavior.

# **10.1 The Efficient Market Hypothesis**

Everything we have seen so far points in the direction of efficient markets, in the sense advocated by Fama (1970). To be more precise, Fama analyzed the available theoretical and empirical evidence with respect to three versions of his efficient market hypothesis (EMH). Each version breeds an additional implication, worth discussing briefly. In its weak form, the EMH requires that equilibrium returns are martingales with respect to the information F*<sup>t</sup>* , generated by observing solely the price process of the respective asset. In other words, the sequence of excess returns constitutes a "fair game" with respect to the natural filtration F*<sup>t</sup>* . Since equilibrium returns are ultimately determined by discounting expected utility of future cashflows, weak EMH implies that agents behave sufficiently rationally, such that if the market is cleared, asset prices are aligned with their fundamental values. So the first hypothesis is

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

$$H_1: \quad \text{weak EMH} \implies \boxed{\text{rational agents}} + \boxed{S_t = V_t} \,. \tag{10.1}$$

The second part of the implication is hard to verify. On the one hand, as seen in the last chapter, there may be rational bubbles that cannot be detected, even though it is not clear how they could have originated in an efficient market. On the other hand, this so-called "joint hypothesis problem" requires an equilibrium asset pricing model to test for market inefficiency. But a market is called inefficient, if prices systematically deviate from their equilibrium values, which only can be determined by the model. So if market inefficiencies are observed, either the market is indeed not efficient, or the equilibrium asset pricing model is incorrect, or both.

The second hypothesis is called the semi-strong version of the EMH. It requires the agent to incorporate all publicly available information from news and so forth, not only the information contained in the price process, to determine the fundamental value of an asset. Call this information set F *P t* , then clearly F *P <sup>t</sup>* ⊃ F*<sup>t</sup>* holds. This requires the rational agent to apply the Bayes' rule to condense the correct posterior

distribution out of the current prior distribution and the novel incoming information. So the implication of this hypothesis is

$$H_2$$
: semi-strong EMH  $\Rightarrow H_1 + \boxed{\text{Bayes' rule}}$ . (10.2)

If equilibrium returns are martingales with respect to the information  $\mathcal{F}^P_t$ , too, then all required information is instantaneously stored in the price process of the respective asset. That means it is completely unnecessary to watch CNBC or Bloomberg TV all day, all you have to know is encoded in the history of the price process.

The final hypothesis is the strong version of the EMH. It corresponds to another, yet larger information set  $\mathcal{F}_t^I \supset \mathcal{F}_t^P \supset \mathcal{F}_t$ , containing all publicly available information plus some insider information, available only to single persons or limited groups of agents. What does it mean if a market is efficient with respect to this information set  $\mathcal{F}^{I}_{t}$ ? It does not mean that there is no insider information, it means that this additional information simply does not count. Or in other words, market transparency is that high that only irrelevant information is not revealed to the entire community of market participants. That means

$$H_3$$
: strong EMH  $\Rightarrow H_2 + |\text{full transparency}|$ . (10.3)

What Fama concluded is that there is evidence for occasional violation of strong market efficiency. But theses incidences appear to be rather local phenomena that rarely affect the entire market. On the contrary, the analysis strongly supported the assumption of an at least semi-strong efficient market. So why should these results be challenged? On the one hand, there are anomalies like the equity premium puzzle of Mehra and Prescott (1985). On the other hand, we cannot be sure that the joint hypothesis problem did not create a facsimile of an efficient market. So what we can do is to check the implications of  $H_1$  and  $H_2$  to see if they are in alignment with the behavior of actual decision makers.

Let's proceed backwards and first check if agents follow Bayes' rule, when incorporating new information. There is an outstanding example that has become known as the Harvard Medical School test.

# Example 10.1

Casscells et al. (1978) asked 60 people, fourth-year medical students and staff members of Harvard Medical School, the following question: If a test to detect a disease whose prevalence is  $1/1000$  has a false positive rate of 5%, what is the chance that a person found to have a positive result actually has the disease, assuming that you know nothing about the person's symptoms or signs?

## Answers

The estimates varied widely. Nearly half of the test persons answered 95%, incorrectly incorporating the information about the false positive rate. Only 11 out of 60 persons gave the appropriate answer of approximately  $2\%$ .

Let's see why 2% is the right answer. Assume that *D* is the event of contracting the disease, and + is a positive test result. Then the probability of having the disease, conditional on a positive test result is given by Bayes' rule

$$P(D|+) = \frac{P(+|D)P(D)}{P(+)} = \frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|D^C)P(D^C)}.$$
(10.4)

Since nothing was said about a false negative rate of the test, we can assume that it is reliable, which means *P*(+|*D*) = 1. Thus plugging in the numbers, we obtain

$$P(D|+) = \frac{1 \cdot 0.001}{1 \cdot 0.001 + 0.05 \cdot 0.999} \approx \frac{0.001}{0.05} = 2\%. \tag{10.5}$$

Of course we can think of fourth-year medical students and teachers as experts in their field, as well as we might think of participants in a financial market as experts in the field of finance. There is no obvious reason to believe that those agents are better *Bayes*ians. This is a considerable blow for the semi-strong efficient market hypothesis.

Let us now take a look at the second part of the implication of *H*<sup>1</sup> to check if there is evidence against a weakly efficient market, too. This is a delicate peace of reasoning, because as already emphasized, to detect deviations of asset prices from their fundamental values, we need an equilibrium pricing model, which is already part of the joint hypothesis problem. Put the other way around, we need to check the alignment of prices and fundamental values outside an asset pricing model, if such a thing is possible at all. Surprisingly, it turns out that there are rare occasions, where the market provides sufficient conditions for a model free analysis. Let's look at two particularly striking examples.

# **Example 10.2**

In March 2000, the digital electronics manufacturer 3Com (acquired by Hewlett-Packard in 2010) sold 5% of its subsidiary Palm Inc. in an initial public offering (IPO). 3Com announced that it would spin the remainder of Palm off completely by the end of the year and would provide compensation in the form of 1.5 Palm shares per 3Com share at that time.

# Analysis

At the IPO day, Palm closed at \$95.06. Because every 3Com share entailed 1.5 Palm shares, this put a lower limit of \$142.59 on the 3Com share. In fact, 3Com closed at \$81.81, which violates the lower limit by −\$60.78. If additionally 3Com's non Palm assets were taken into account, the "stub value" of 3Com shares would have been −\$70.77. This mispricing lasted for more than 12 months; see Figure 10.1, taken from Hens and Rieger (2010, p. 163).

........................................................................................................................

There is another very disturbing example of mispricing to be observed when examining so-called "Siamese twin companies."

![](_page_3_Figure_1.jpeg)

**Fig. 10.1** Negative stub value of 3Com after IPO of Palm – Figure is reproduced from Hens and Rieger (2010, p. 163) with permission of Springer

# **Example 10.3**

In 1907, the companies Royal Dutch Petroleum and Shell Transport and Trading merged their interests on a 60:40 basis, while remaining separate legal entities, located in the Netherlands and Great Britain, respectively. Internally, all tax adjusted cashflows are effectively split in the proportion 60:40 and information about the linkage between the two companies is publicly available. Royal Dutch and Shell are traded liquidly at major exchanges in Europe and the United States.

# Analysis

If fundamental value equals asset price, the price for Royal Dutch should be always 1.5 times the price of Shell. In reality, this parity is violated considerably over extended periods of time. Figure 10.2, taken from Froot and Dabora (1999), illustrates the effect. ........................................................................................................................

![](_page_3_Figure_8.jpeg)

**Fig. 10.2** Logarithmic deviations from Royal Dutch/Shell parity – Figure is reproduced from Froot and Dabora (1999) with permission of Elsevier

We can conclude that there is substantial evidence for violations of the equality of asset prices and fundamental values. So if we question the *S<sup>t</sup>* = *V<sup>t</sup>* implication of *H*1, we also have to reconsider rationality of agents. This is because our equilibrium asset pricing model was based on the assumption that agents maximize some kind of utility functional, which we in turn called rational behavior. But does that mean that agents behave irrationally? Not necessarily, at least not in the sense we usually use that term. There are indeed systematic decision patterns beyond rationality, leading to alternative decision systems. We will explore two of them in the remainder of this chapter.

# **10.2 Beyond Rationality**

There is a very old paradox challenging one of the axioms of expected utility theory, quite from the start. It is called the *Allais*-paradox (Allais, 1953). It requires people to choose between two lotteries in two different scenarios. There are always three possible outcomes of the lottery in million dollars,

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

$$\mathcal{W} = \{0, 1, 5\}. \tag{10.6}$$

The respective scenarios are:

# **Scenario 1**

| Scenario 1 | Scenario 2 |  |  |
|------------|------------|--|--|
| •          | •          |  |  |
| =          | =          |  |  |
| (0,        | (0.89,     |  |  |
| 1,         | 0.11,      |  |  |
| 0)         | 0)         |  |  |
| LA         | LA         |  |  |
| •          | •          |  |  |
| =          | =          |  |  |
| (0.01,     | (0.9,      |  |  |
| 0.89,      | 0,         |  |  |
| 0.1)       | 0.1)       |  |  |
| LB         | LB         |  |  |

In scenario 1, people were asked to choose between obtaining a safe amount of one million dollars or taking an additional 10% chance of gaining five million dollars, accompanied by a 1% chance of losing everything. Most people prefer the riskless lottery *LA*, because one million dollars is a considerable amount of money and a bird in the hand is worth two in the bush. In scenario 2, people had to choose between an 11% chance of gaining one million dollars and a 10% chance of gaining five million dollars. Because there is not much difference between 10% and 11%, but a considerable difference in the potential gain, people for the most part prefer lottery *LB*. But this switch of preferences compromises the independence axiom (4.5) on page 62 of expected utility theory. To see why this is the case, define the new lotteries

$$L_1 = (0, 1, 0), \quad L_2 = (1, 0, 0), \quad \text{and} \quad L_3 = \left(\frac{1}{11}, 0, \frac{10}{11}\right).$$
 (10.7)

If we choose the probability π = 0.11, then the lotteries *L<sup>A</sup>* and *L<sup>B</sup>* can be written as compound lotteries

$$L_A = \pi L_1 + (1 - \pi)L_n$$
 and  $L_B = \pi L_3 + (1 - \pi)L_n$  (10.8)

in both scenarios *n* = 1, 2.

**Quick calculation 10.1** Check that this statement is correct.

But if *L<sup>A</sup>* is preferred over *L<sup>B</sup>* in scenario 1, then *L*<sup>1</sup> is also preferred over *L*3. This preference should not depend on the mixing lottery *Ln*, whatever scenario is considered. Nevertheless, a flip in the preference order occurs and thus, the independence axiom of expected utility theory is violated.

Even though the effect is clear once *L<sup>A</sup>* and *L<sup>B</sup>* are stated as compound lotteries, it is very hard to explain on a fundamental level. Obviously, the decision maker was distracted from the core problem, and thus did not realize that her preference order was irrational. The mechanism transmitting this distraction is likely to be psychological in nature. Kahneman and Tversky (1981) presented an impressive example of one such psychological mechanism called the **framing effect**. They confronted more than 300 students from Stanford University and the University of British Columbia with what became known as the Asian disease problem.

# **Example 10.4**

One half of the participants were asked the following question: Imagine that the US is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimate of the consequences of the programs are as follows:

- If program A is adopted, 200 people will be saved.
- If program B is adopted, there is 1/3 probability that 600 people will be saved, and 2/3 probability that no people will be saved.

Which of the two programs would you favor?

# Answers

72% of the participants preferred the safe alternative A, only 28% were willing to take the 1/3 chance program B was offering.

........................................................................................................................

Kahneman and Tversky asked the second half of the participants precisely the same question, but this time they phrased the alternative programs in another way.

# **Example 10.5**

Consider the situation in Example 10.4. Now assume that the consequences of the programs are as follows:

- If program A is adopted, 400 people will die.
- If program B is adopted, there is 1/3 probability that nobody will die, and 2/3 probability that 600 people will die.

Which of the two programs would you favor?

# Answers

Now only 22% of the participants preferred program A, whereas 78% were willing to take the risky alternative B.

........................................................................................................................

This result is astonishing, because the problem and the consequences are clearly identical in both situations. Yet a large number of decision makers reversed preferences, only because the decision was framed in another way. In the first case, the problem was framed in terms of survivors, or gains. In the second case, the frame was in terms of fatalities, or losses.

There are two lessons to be learned from this observation of actual decision behavior. First, there was no reference to the number of people living in the US at all. Expected utility theory was build around the idea that absolute risk aversion should be linked to the current level of wealth of an individual. The result of the Asian disease problem on the other hand indicates that gains and losses are the relevant quantities in the decision process. Second, agents are obviously risk averse in the domain of gains, but risk seeking in the domain of losses. This in turn implies that gains and losses have to be measured with respect to a specific reference point, where a symmetry breaking occurs. So how did Kahneman and Tversky set this reference point to frame precisely the same consequences in the domain of gains one time, and in the domain of losses the other time? In the first version, the programs were formulated in terms of saved lives in the future, implying that no one has been saved yet, at least not until a program is chosen. Thus, the test persons set their reference point to zero saved lives. But this also means that they accept the 600 threatened persons to be doomed. In the second version, the consequences were phrased in terms of lost lives in the future, again implying that currently no lives have been lost, at least not until a decision is made. So in this case, test persons again chose zero lost lives as a reference point, even though the disease threatens 600 lives. Now it is also clear that Kahneman and Tversky had to split the test subjects into two groups to allow them to establish different views on the 600 threatened lives.

If the conclusion that individuals are risk averse with respect to gains and risk seeking with respect to losses is correct, then why do people participate in organized lotteries and enter into insurance contracts? In the first case, the expected gains from common lotteries are negative, so it seems that in this case the behavior of decision makers is risk seeking. In the second case, insurance contracts provide coverage against low probability events of damage or loss. Usually the insurance premium is not actuarially fair, which means it is clearly higher than the expected loss. So the behavior of decision makers seem to be risk averse in this situation. This is quite the opposite of the behavior observed in the Asian disease problem. So what is the difference between these observations? In the case of the Asian disease, we were dealing with mid-range probabilities, namely 1/3 and 2/3, whereas in the case of organized lotteries and insurance contracts, we are talking about extremely low probability events. Table 10.1 summarizes our observations of actual decision patterns. Kahneman and Tversky (1979) suggested a decision theory explaining this pattern, called prospect theory. We will analyze its elements in the next section, in order to understand how it generates predictions that are in line with the observed decision behavior.

Tversky and Kahneman (1991) report another important fact about gains and losses. They find that when offered a fair coin flip game with a potential gain *G* and a potential loss *L*, the ratio *G*/*L* for which the game is barely acceptable for an agent is usually higher than 2/1. That means losses loom larger than gains. This principle is called loss aversion and is to be distinguished from risk aversion. Loss aversion means

| <b>Table 10.1</b> Observed decision patterns |                             |                             |  |  |  |  |
|----------------------------------------------|-----------------------------|-----------------------------|--|--|--|--|
|                                              | Domain                      |                             |  |  |  |  |
|                                              | Gains                       | Losses                      |  |  |  |  |
| Mid-range probabilities<br>Low probabilities | risk averse<br>risk seeking | risk seeking<br>risk averse |  |  |  |  |

# that a decision maker fears a loss of certain magnitude more than she values a gain of the same magnitude. Thus, a risk-averse decision maker in expected utility theory is automatically loss averse. But a loss averse decision maker in prospect theory may be risk seeking; see Table 10.1.

# **Prospect Theory**

In its original form, prospect theory, suggested by Kahneman and Tyersky (1979), was a purely descriptive approach to explain actual decision behavior of agents under risk, incorporating the psychological effects we already studied. Even though qualitatively explaining a decision after it was made is important in order to understand the relevant mechanisms, Kahneman and Tversky did much more. They created a quantitative framework to predict how agents will actually decide, which is a much harder task. Their approach has a superficial similarity with expected utility theory, but differs from it considerably with respect to its construction and substantial features. In expected utility theory we encountered a utility function  $u: W \to \mathbb{R}$ , mapping the outcome of some random variable into the real numbers. Its existence and properties were the consequences of the four axioms of von Neumann and Morgenstern (4.5) on page 62. Prospect theory has a value function  $v: X \to \mathbb{R}$ , doing the same mapping. This time the set of possible realizations is labeled  $X$ , to indicate that the random variable  $X$ is in terms of gains and losses, and not with respect to an initial wealth. The value function was not derived from a set of axioms, but was designed to accommodate the anomalies observed in actual decision making. Kahneman and Tversky suggested the following form

$$v(x) = \begin{cases} x^{\alpha} & \text{for } x \ge 0\\ -\lambda(-x)^{\beta} & \text{for } x < 0, \end{cases} \tag{10.9}$$

with  $0 < \alpha, \beta < 1$ , and  $\lambda > 1$ . The parameters were calibrated in numerous empirical experiments. Kahneman and Tversky found that the values  $\alpha = \beta = 0.88$  and  $\lambda = 2.25$ reasonably reflect observed decision behavior. The resulting value function is illustrated in Figure 10.3 left. For gains,  $x \ge 0$ , the value function is concave, indicating

10.3

![](_page_8_Figure_1.jpeg)

**Fig. 10.3** Functions of prospect theory – Value function (left) and probability weighting function (right)

risk-averse behavior of decision makers. In the domain of losses, *x* < 0, it is convex, accommodating risk-seeking behavior of decision makers. Loss aversion is also encoded in the value function. Because λ > 1, the slope for *x* < 0 is steeper than for *x* > 0 and thus, the reduction in value due to a loss of given magnitude is higher than the increase in value due to a gain of the same magnitude.

**Quick calculation 10.2** Verify the last statement for *x* = ±1.

What is not explained in terms of the value function is the lottery/insurance puzzle. Kahneman and Tversky came up with a very elegant solution to this problem. They concluded that decision makers presumably have a good intuition for mid-range probabilities, because that is what they face very often in everyday life, but they overestimate small probabilities considerably. This idea was formalized in terms of a suitable probability weighting function. If the true probability is *p*, then for example the perceived probability weight could be given by

$$w(p) = \frac{p^{\gamma}}{(p^{\gamma} + (1 - p)^{\gamma})^{1/\gamma}},\tag{10.10}$$

with γ > 0. The weighting function for γ = 0.65 is illustrated in Figure 10.3 right.<sup>1</sup> The form of the probability weighting function is by no means completely arbitrary. One

<sup>1</sup> Kahneman and Tversky (1992) suggested this functional form, but they defined two versions, *w* + (*p*) and *w* − (*p*) with parameters γ and δ, one for each domain. Their parameter values, determined from experiment, did not differ very much (γ = 0.61 and δ = 0.69) and thus, it is common practice in finance to define only one weighting function with the arithmetic mean γ = 0.65.

can assume that decision makers agree on the probabilities of the certain and the impossible event. This requires that

$$w(0) = 0$$
 and  $w(1) = 1$  (10.11)

holds. Furthermore, decision makers are expected to correctly preserve order relations across different probabilities, which means they still know which event has larger probability, even if they have a biased perception of the total magnitude of the respective probabilities. This requires the weighting function to be monotonically increasing in *p*,

$$\frac{dw}{dp} \ge 0,\tag{10.12}$$

for 0 ≤ *p* ≤ 1.<sup>2</sup>

Now that we know the fundamental building blocks of prospect theory, the value functional can be defined. Assume that a lottery *L* has *N* different possible outcomes, collected in the set X, then the value functional is

$$V[L] = \sum_{n=1}^{N} v(x_n) w(p_n).$$
 (10.13)

This functional can be understood as expectation of the value function with respect to the weighted probability measure *w*(*P*). It is used precisely in the same way as the *von Neumann–Morgenstern*-utility functional. If there are two lotteries, *L*<sup>1</sup> and *L*2, with probability measures *P* and *Q*, then the equivalence

$$L_1 \gtrsim L_2 \quad \Leftrightarrow \quad \sum_{n=1}^N v(x_n) w(p_n) \ge \sum_{n=1}^N v(x_n) w(q_n) \tag{10.14}$$

holds.

Prospect theory laid the foundations of behavioral economics and its sub-field behavioral finance. It also had an enormous impact on corporate finance. Because of its psychological roots, it was able to provide a new perspective on agency relations. But there were also problems with the original formulation, both technically and conceptually. Technical difficulties are mostly related to the fact that prospect theory is discontinuous (for details see Hens and Rieger, 2010, sect. 2.4.5). Conceptually, the greatest obstacle is that prospect theory does not respect first order stochastic dominance. These shortcomings were remedied with the formulation of cumulative prospect theory.

<sup>2</sup> Remarkably, this property does not hold for the weighting function (10.10) of Kahneman and Tversky for γ ≤ 0.278, as pointed out by Rieger and Wang (2006).

# **10.4 Cumulative Prospect Theory (CPT)**

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • Let's start our discussion by looking at the definition of first order stochastic dominance.

**Theorem 10.1 (First order stochastic dominance)** *Let X*, *Y* : Ω → R *be two real random variables. Then X dominates Y stochastically to first order, if for all x* ∈ R

$$P(X > x) \ge P(Y > x)$$

*holds, with strict inequality for some x*.

Because the probability distribution function of a random variable *X* is defined as *FX*(*x*) = *P*(*X* ≤ *x*), the condition of Theorem 10.1 can alternatively be expressed as

$$F_X(x) \le F_Y(x). \tag{10.15}$$

**Quick calculation 10.3** Give a formal verification of this statement.

# **Example 10.6**

Consider the set X = {1, 2, 3} of possible outcomes, and the lotteries

$$L_A = \left(\frac{1}{2}, \frac{1}{2}, 0\right)$$
 and  $L_B = \left(\frac{1}{2}, 0, \frac{1}{2}\right)$ .

Does one of the lotteries dominate the other stochastically to first order?

Solution

It is easy to see that lottery *L<sup>B</sup>* dominates lottery *LA*, because the distribution function can be immediately extracted from the lotteries, see Table 10.2. For all *x*, condition (10.15) holds, and additionally

$$F_B(x) < F_A(x)$$

for 2 ≤ *x* < 3.

| Table 10.2 |     |            | Distribution functions of lotteries A and B |     |
|------------|-----|------------|---------------------------------------------|-----|
|            | x<1 | 1≤x <<br>2 | 2≤x<3                                       | x≥3 |
| FA(x)      | 0   | 1/2        | 1                                           | 1   |
| FB(x)      | 0   | 1/2        | 1/2                                         | 1   |

Now, consider a family of lotteries  $L_N$  with possible outcomes in  $X_N$ , given by

$$x_n = 1 - \frac{n-1}{N} \tag{10.16}$$

for  $n \leq N$ , and uniform probabilities

$$p_n = \frac{1}{N}.\tag{10.17}$$

The first member of this family,  $L_1$ , is the lottery that provides a sure gain of one unit of currency. The next lottery  $L_2$  is given by

$$L_2 = \left(\frac{1}{2}, \frac{1}{2}\right)$$
 on  $X_2 = \left\{1, \frac{1}{2}\right\}.$  (10.18)

It is easy to see that  $L_1$  dominates  $L_2$  stochastically to first order.

**Quick calculation 10.4** Sketch the distribution functions of  $L_1$  and  $L_2$ .

In fact,  $L_1$  dominates all lotteries  $L_N$  for  $N > 1$ , because none of them can pay off more than one unit of currency and in  $L_1$  this outcome is certain. Unfortunately, in prospect theory one can find an  $N^*$ , such that the lotteries  $L_N$  with  $N \geq N^*$  are preferred over  $L_1$ . Let's try to understand why this is the case. Using the value function (10.9) and the probability weighting (10.10) on page 189, we can compute the value functional for  $L_1$ explicitly,

$$V[L_1] = v(1)w(1) = 1. \t(10.19)$$

For  $N > 1$ , this computation is not that easy, but we can compute a lower limit. Because  $\alpha < 1$  holds, we have  $v(x) \ge x$  for positive  $x \le 1$ , and thus

$$V[L_N] \ge \sum_{n=1}^{N} x_n w(p_n). \tag{10.20}$$

The next step is to put a lower limit on the probability weighting function

$$w(p) = \frac{p^{\gamma}}{(p^{\gamma} + (1-p)^{\gamma})^{1/\gamma}}.$$
 (10.21)

To this end, let's see, for which  $p$  the denominator takes its maximum value,

$$\frac{d}{dp}(p^{\gamma} + (1-p)^{\gamma})^{1/\gamma} = \frac{p^{\gamma - 1} - (1-p)^{\gamma - 1}}{(p^{\gamma} + (1-p)^{\gamma})^{1 - 1/\gamma}} \stackrel{!}{=} 0. \tag{10.22}$$

**Quick calculation 10.5** Confirm this derivative.

In order for the derivative to vanish, the numerator has to become zero. This is precisely the case for *p* ∗ = 1 2 . Thus, we obtain the following lower bound on the probability weight

$$w(p) \ge \frac{p^{\gamma}}{(2 \cdot 2^{-\gamma})^{1/\gamma}} = 2^{1-1/\gamma} p^{\gamma}.$$
 (10.23)

for γ < 1, as estimated by Kahneman and Tversky. Combining (10.16), (10.17), and (10.23), we can compute a lower limit of the value functional

$$V[L_N] \ge 2^{1-1/\gamma} N^{-\gamma} \sum_{n=1}^N \left(1 - \frac{n-1}{N}\right) = 2^{1-1/\gamma} N^{-\gamma} \cdot \frac{N+1}{2}$$
  
=  $2^{-1/\gamma} N^{1-\gamma} + 2^{-1/\gamma} N^{-\gamma}$  (10.24)

The second term is negligible if *N* grows large and thus, we obtain the final inequality

$$V[L_N] \ge 2^{-1/\gamma} N^{1-\gamma}.$$
 (10.25)

The term on the right hand side of (10.25) is unbounded and thus, there has to exist an *N*<sup>∗</sup> , such that

$$V[L_N] \ge 1 = V[L_1] \tag{10.26}$$

for all *N* ≥ *N*<sup>∗</sup> and 0 < γ < 1. This proves that prospect theory can make predictions that do not respect first order stochastic dominance.

Violation of first order stochastic dominance was an undesired feature of the theory, because Kahneman and Tversky assumed that there exists a so-called **editing phase**, where the decision maker systematically arranges and analyzes the available alternatives, before the actual decision is made. Unfortunately, the editing phase is not a formalized part of the theory, but it is generally assumed that an economic agent identifies stochastically dominated and thus inferior alternatives. These problems were solved with the formulation of cumulative prospect theory (CPT, Kahneman and Tversky, 1992). The major difference to prospect theory is that probability weighting is now done with respect to the (cumulative) distribution function; hence the name cumulative prospect theory. This results in a new value functional<sup>3</sup>

$$V[L] = \int_{-\infty}^{\infty} v(x) dw(F(x)). \tag{10.27}$$

To see, why this functional respects first order stochastic dominance, let's first manipulate the integral a bit. If the distribution function is invertible, and the weighting function is strictly monotonic increasing and differentiable, which is usually the case,

<sup>3</sup> Because originally Kahneman and Tversky defined the weighting functions *w* + (*x*) and *w* − (*x*) separately, they obtained two functionals *V* + [*L*] and *V* − [*L*] on the respective half lines, which, besides some technical conditions, requires a proper rescaling (for details see the appendix of Kahneman and Tversky, 1992).

the integral in (10.27) can be transformed with the help of the substitution *F*(*x*) = *u*. The result is

$$V[L] = \int_0^1 v(F^{-1}(u))w'(u)du,$$
 (10.28)

where *w* ′ (*u*) is the derivative of the weighting function with respect to its argument, and *F* −1 (*u*) is the quantile function.

# **Quick calculation 10.6** Confirm this result.

Consider the lotteries *L<sup>A</sup>* and *L<sup>B</sup>* and assume that *L<sup>A</sup>* stochastically dominates *L<sup>B</sup>* to first order. Due to Theorem 10.1 this means *FA*(*x*) ≤ *FB*(*x*) for all *x* ∈ R. But it is not hard to see that this also means

$$F_A^{-1}(u) \ge F_B^{-1}(u). \tag{10.29}$$

**Quick calculation 10.7** Make the substitution *FA*(*x*) = *u* to prove (10.29).

Because *w* ′ (*u*) is a positive function for *u* ∈ [0, 1] and *v*(*x*) is strictly monotonic increasing, we also have

$$V[L_A] = \int_0^1 v(F_A^{-1}(u))w'(u)du \ge \int_0^1 v(F_B^{-1}(u))w'(u)du = V[L_B]. \tag{10.30}$$

For discrete probability distributions, a related proof can be found in Hens and Rieger (2010, p. 64).

What enhances the appeal of CPT even further is the fact that it can be axiomatized in the same way as expected utility theory. To this end, the independence axiom has to be modified and some care has to be taken of sign changes. The technical details are quite intricate but this was an important breakthrough, finally resulting in the Nobel Prize for Daniel Kahneman in 2002 (sadly Amos Tversky had already died).

# **10.5 CPT and the Equity Premium Puzzle**

The equity premium puzzle of Mehra and Prescott (1985), extensively discussed in Chapter 9, is probably the most prominent conundrum of expected utility theory. In a nutshell, observed equity premia of more than 6% and risk-free interest rates of less than 1% are irreconcilable with a common order of magnitude of risk aversion. We have seen that this puzzle is resolvable within the model of Campbell and Cochrane (1999), if we assume that utility does not only depend on individual consumption, but also on a particular common level of consumption. Apart from a translation, the *Campbell–Cochrane*-utility function was

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

$$u(c,x) = \frac{(c-x)^{1-\gamma}}{1-\gamma},\tag{10.31}$$

where c indicates consumption and x acts as a reference level, against which consumption is measured as gain or loss relative to a common level. Why the reference point may be a substantial ingredient is best understood by looking at the often-quoted anecdote of Paul Samuelson's chat with a colleague. Samuelson asked his colleague, whose name he did not reveal until many years later, if he was willing to accept a fair coin flip gamble, in which he could either win \$200 or lose \$100. After some contemplation the colleague turned his bet down, but announced that he would happily accept 100 such bets. Samuelson (1963) retaliated with his famous publication "Risk and Uncertainty: A Fallacy of Large Numbers," in which he proved that his colleague was acting irrationally.

To understand the argument we have to pay close attention to Samuelson's requirement that for the whole range of wealth levels attainable in such a sequence of coin tosses, one isolated bet should always be unfavorable. Assume we start at the wealth level  $w_0$  and the coin toss leaves us either with  $w_+$  with probability p, or  $w_-$  with probability  $1 - p$ . Because this single bet is unfavorable to us, we can conclude that the certainty equivalent  $w_0^*$  is smaller than our initial wealth  $w_0$ . Because of the same argument we also have

$$w_{+}^{*} < w_{+} \quad \text{and} \quad w_{-}^{*} < w_{-}.$$
 (10.32)

But the decision maker is indifferent between the second stage lotteries and the respective certainty equivalents, and because the experiments in the sequence are independent, it does not matter if she takes the lottery or the certainty equivalent in stage two.

**Quick calculation 10.8** Provide a formal argument for this statement.

However, taking  $w_{+/-}^*$  in stage two, leaves us with a lottery in stage one, where we either obtain  $w^*_+$  with probability p or  $w^*_-$  with probability  $1 - p$ . This lottery is stochastically dominated by the initial isolated lottery, and thus if one rejects the single bet, a double bet should even more be rejected. This argument can be iterated over the whole sequence and is conceptually the core of Samuelson's proof.

However, things change dramatically in the framework of cumulative prospect theory. This can already be seen from the very simplified situation

$$v(x) = \begin{cases} x & \text{for } x \ge 0\\ 2.5x & \text{for } x < 0 \end{cases} \quad \text{and} \quad w(p) = p,\tag{10.33}$$

where all characteristic effects of prospect theory, with the exception of loss aversion, have been switched off. The decision maker should participate in the lottery  $L$ , if  $V[L] > 0$ , which means that the prospective value is positive.

**Quick calculation 10.9** Show that the single bet of Samuelson's fair coin flip gamble is not favorable, but the double bet has already positive prospective value.

So Samuelson's colleague may have been irrational with respect to expected utility theory, but in the light of prospect theory, his answer was perfectly justified. Here is another subtlety. What happens, if the colleague evaluates his gains and losses after every single toss of the coin? In this case, he would reset his reference point after every round of the game. We shall call this an evaluation period of only one round. Why is that a problem? Well, if he does so, he would not participate in the last round of the game, because it would seem unfavorable to him. But knowing that he will actually participate only in 99 rounds would mean he has to refuse participation in round 98, too. That means he would eventually turn down the complete sequence. Only for an evaluation period of at least two rounds, which means he restrains himself from checking the result of every coin flip, does the whole sequence become an attractive investment. The longer the evaluation period, the less severe the risk is perceived. On the other hand, two factors seem to prevent decision makers from taking risky investment opportunities, loss aversion and short evaluation periods. The combination of both is called **myopic loss aversion**.

Can myopic loss aversion explain the equity premium puzzle? This question was analyzed by Benartzi and Thaler (1995). They assumed that market participants act on average according to CPT, with the parameter set determined by Kahneman and Tversky (1992). Under this hypothesis, they conducted historical simulations by drawing random samples with replacement from monthly returns on stocks, bonds, and treasury bills between 1926 and 1990, to answer the question: For which evaluation period will the average market participant be indifferent between holding stocks and bonds? Or in other words, for which evaluation period has the observed equity premium exactly the right magnitude to compensate for the perceived risk associated with stocks? The desired period is the one at which the prospective value of a bond investment equals the prospective value of a stock investment. From their simulation studies, Benartzi and Thaler (1995) discovered a nearly perfectly linear relationship between prospective value and the evaluation period, sketched in Figure 10.4 left. It is easily seen that both lines cross in the vicinity of 12 months, which means that agents have an average evaluation period of about one year. It is hard to believe in a coincidence because an evaluation period of one year is highly plausible. As the authors already

![](_page_15_Figure_3.jpeg)

**Fig. 10.4** Prospective value (left) and equilibrium equity premium (right) as a function of length of the evaluation period – Schematic illustration of the results of Benartzi and Thaler (1995)

emphasized: "Individual investors file taxes annually, receive their most comprehensive reports from their brokers, mutual funds, and retirement accounts once a year, and institutional investors also take the annual reports most seriously."

Benartzi and Thaler asked another question: How does the equilibrium equity premium change with respect to the evaluation period? This time they used real returns, because the time horizon is considerably longer. The result is a nearly perfectly exponentially decaying function, sketched in Figure 10.4 right. Empirical results, demonstrating such a clear linear or exponential relationship, often point in the direction of a potentially deep and fundamental law. So if decision makers indeed act according to CPT, the exceptionally high observed equity premium could be generated by the short evaluation period of one year. In the long term, investors would require a much lower premium to prefer a stock position over a riskless investment. So it seems that the equity premium puzzle can be resolved in cumulative prospect theory, even though we cannot verify, if this explanation is the right one. Nevertheless, we can now understand much better, why the *Campbell–Cochrane*-model was so successful; namely because it adopted the idea of a variable reference point. Furthermore, it was formulated as a time series model with a natural lag of precisely one year.

# **10.6 The Price Momentum Effect**

From the perspective of standard equilibrium models, one of the most bizarre observations is the price momentum effect, first analyzed by Jegadeesh and Titman (1993). In short, this effect describes the tendency of the best (worst) performing stocks over a three- to twelve-month period to perform well (poorly) over the subsequent period of the same length. This strange effect also does not seem to be limited to the USA, but could be observed in European markets, too (Rouwenhorst, 1998). Jegadeesh and Titman presented an impressive analysis of momentum strategies. They assembled portfolios of the top decile performing stocks, funded by short selling the decile of the poorest performing stocks over the respective period. The composition of their momentum portfolios was adjusted periodically in the same self-financing way. So their strategies did not cost anything, but surprisingly, monthly returns of nearly 1.5% were observed between 1965 and 1989. Subsequent work confirms the persistence of this effect in the more recent past. If markets were efficient, there should not be any statistically significant return on average at all, because a zero net investment should not generate positive payoffs in the future.

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

Several explanations for this effect have been suggested. A particularly interesting one, referring to prospect theory, is the one of Grinblatt and Han (2005). They assume that there is a fraction of rational decision makers in the market, but also another fraction, behaving only partially rational. To be more precise, they assume that those agents behave consistently with prospect theory and **mental accounting**. In behavioral economics, mental accounting describes the tendency of decision makers to mentally separate the development of different assets and to account for the gains and losses individually. For such a decision maker, there is a clear difference between losing a ticket worth \$50, say for a baseball match, and losing the same amount in cash. She

has mental accounts for baseball cards and for cash money. If she loses the ticket, she would have to buy a new one, but that would make the ticket cost \$100. She might well decide that this is too expensive for seeing a baseball match and will no longer attend it, even though she had not even thought about selling her ticket, if she had lost the \$50 in cash instead. That is because cash money and baseball tickets were held in different mental accounts. In finance, mental accounting means that the investor does not aggregate gains and losses on a portfolio level, but accounts them to specific assets. That is a central element in the reasoning of Grinblatt and Han. The rest of their model uses the following slightly simplified framework: Assume that the fundamental value *V* of an arbitrary stock *S* follows a random walk

$$V_{t+1} = V_t + \epsilon_{t+1}, \tag{10.34}$$

where ϵ*<sup>t</sup>* is a zero mean random innovation. There is a fraction µ of partially rational (PT) decision makers, and a fraction 1 − µ of entirely rational (EU) decision makers in the market. The excess demand *X*, generated by both groups is

$$X_t^{\text{EU}} = b(V_t - S_t) X_t^{\text{PT}} = b((V_t - S_t) + \lambda (R_t - S_t)),$$
 (10.35)

with *b*, λ > 0, and the time-dependent reference point *R<sup>t</sup>* , which is usually the individual buying price of the stock. In case of market clearing, we must have

$$X_t = \mu X_t^{\text{PT}} + (1 - \mu) X_t^{\text{EU}} = 0. \tag{10.36}$$

Plugging in (10.35) and conducting some algebraic manipulations, one obtains

$$S_t = wV_t + (1 - w)R_t$$
 with  $w = \frac{1}{1 + \mu\lambda}$ . (10.37)

That means, the observed stock price is a linear combination of the fundamental value and the (average) individual reference point.

# **Quick calculation 10.10** Verify the last result formally.

The next question is, how does the reference point change with time? Grinblatt and Han assume that it reverts back to the observed stock price,

$$R_{t+1} = \gamma S_t + (1 - \gamma)R_t, \tag{10.38}$$

where 0 < γ < 1 is related to the stock's turnover ratio. Take a minute to appreciate the elegant idea behind this simple formula. Through trading, a large number of stocks change hands. Every time a PT decision maker trades the stock, her reference point is reset to the current price and so, gradually, the average reference point tends to the current price of the stock. Figure 10.5 illustrates the dynamics of the key quantities in the *Grinblatt–Han*-model, simulated over a twelve month period. All quantities start in equilibrium at \$100, and then a shock is injected, possibly an unanticipated event

![](_page_18_Figure_1.jpeg)

**Fig. 10.5** Simulation of fundamental value (gray), stock price (black), and average reference point (dashed) in the *Grinblatt–Han*-model

or information affecting the respective company, pushing the fundamental value of the stock to \$125. The shock is partially absorbed in the stock price immediately, but the remaining difference builds up momentum that is released over the next 12 months.

It is also interesting to analyze the expected returns, predicted by the *Grinblatt– Han*-model. Because the random innovation in (10.34) has zero expectation, one obtains

$$E[S_{t+1} - S_t] = (1 - w)\gamma(S_t - R_t) \tag{10.39}$$

for the expected change in stock price.

**Quick calculation 10.11** Verify this result.

After dividing by the current price of the stock, the expected return is

$$E\left[\frac{S_{t+1} - S_t}{S_t}\right] = (1 - w)\gamma \frac{S_t - R_t}{S_t}.$$
 (10.40)

The quotient on the right hand side of (10.40) is called the investor's (percentage) unrealized capital gain. So even if we do not have a reliable equilibrium model, observing large returns may be evidence for an unrealized capital gain, which in turn provides momentum for future returns. This mechanism can easily explain why the momentum strategy of Jegadeesh and Titman worked so well, while under the efficient market hypothesis the very existence of such a mechanism is inexplicable.

# **10.7 Unifying CPT and Modern Portfolio Theory**

Until now, we have seen that prospect theory is able to explain some of the most challenging puzzles in modern financial markets remarkably well. So the big question

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

is, does CPT fit in the framework of modern portfolio theory? This question was thoroughly addressed under different aspects by De Giorgi and Hens (2006). Unfortunately, the answer is no. But the good news is, it can be made compatible with very few modifications. To understand why this is necessary, let's look at some problems of CPT, when it encounters modern portfolio theory.

One of the oldest paradoxes is the one generated by the so-called St. Petersburg game. It is a fair coin flip game, where the payoff depends on the first occurrence of heads in a sequence of trials. If heads occurs in the first toss, the gain is \$1 and the game is over. If the first toss is tails, the game continues and the potential gain is doubled. The game always ends if the coin shows heads for the first time. Thus, the set of elementary states of this game is  $\Omega = \mathbb{N}$ , and the potential gain X is represented by the random variable

$$X(\omega_n) = 2^{n-1}.\tag{10.41}$$

The actual paradox arises from the fact that in the eighteenth century, the expected gain was considered a fair price for participating in a game of chance. But in case of the St. Petersburg game, the expected gain is

$$E[X] = \sum_{n=1}^{\infty} 2^{n-1} \cdot \frac{1}{2^n} = \frac{1}{2} \sum_{n=1}^{\infty} 1 = \infty.$$
 (10.42)

Obviously nobody would be willing to pay an infinite amount of money for participation in this game. In 1738, Daniel Bernoulli offered a way out of this paradox. He suggested not to focus on the expected gain, but on the expected utility, induced by the gain. He assumed that the utility of money grows logarithmically,  $u(x) = \log x$  and hence, the expected utility was

$$E[u(X)] = \sum_{n=1}^{\infty} \log(2^{n-1}) \frac{1}{2^n} = \log 2 \sum_{n=1}^{\infty} \frac{n-1}{2^n} = \log 2 \cdot \frac{1}{2} \sum_{n=1}^{\infty} \frac{n}{2^n} = \log 2. \tag{10.43}$$

The last equality holds, because for  $|x| < 1$ , differentiating the standard infinite geometric series yields

$$\frac{d}{dx}\sum_{n=0}^{\infty}x^{n}=\sum_{n=1}^{\infty}nx^{n-1}=\frac{1}{(1-x)^{2}}.\t(10.44)$$

**Ouick calculation 10.12** Use this relation to verify the last equality in (10.43).

Even though Bernoulli anticipated the idea of expected utility, his solution lacked the axiomatic structure to be introduced more than 200 years later by von Neumann and Morgenstern. Another problem occurred, when the payoff of the St. Petersburg game was modified appropriately. For example, if the gains were represented by the random variable

$$X(\omega_n) = \exp(2^{n-1}),\tag{10.45}$$

the expected *Bernoulli*-utility would still be infinite. A sufficient condition to guarantee finite expected utility, no matter which utility function is used, is to limit the trials to

a possibly large, but finite number N. That is to say, we limit the set of "fair" games to those with a finite expectation value. In case of the St. Petersburg game, this expectation is obviously

$$E[X] = \frac{N}{2}.\tag{10.46}$$

Surprisingly, this precaution is generally not sufficient, if we replace the expected utility functional by the prospective value functional of CPT. This was shown by Rieger and Wang (2006), with the help of the probability density function

$$f_q(x) = \begin{cases} 0 & \text{for } x \le 1\\ (q-1)x^{-q} & \text{for } x > 1, \end{cases}$$
(10.47)

with  $q > 2$ . It is easy to see that the expectation of a random variable with this density function is finite.

$$E[X] = (q-1) \int_{1}^{\infty} x^{1-q} dx = \frac{q-1}{q-2} < \infty.$$
 (10.48)

**Quick calculation 10.13** Verify this result.

Note that the probability distribution function and also the quantile function, corresponding to the density  $(10.47)$ , are available analytically,

$$F_q(x) = 1 - x^{1-q}$$
 and  $F_q^{-1}(u) = (1-u)^{1/(1-q)},$  (10.49)

with  $x \in [1, \infty)$ . Using the standard value and weighting functions (10.9) and (10.10) on pages 188–189, for  $0 < \gamma < \alpha < 1$ , as estimated by Kahneman and Tversky, it can be shown that there is a  $q^* > 2$ , such that the prospective value functional

$$V[X] = \int_{1}^{\infty} v(x)dw(F_q(x)) = \int_{0}^{1} (1-u)^{\frac{\alpha}{1-q}} w'(u)du \qquad (10.50)$$

diverges for  $q \rightarrow q^*$ . Let's see if we can prove this claim. First of all, we have to compute the derivative of the weighting function,

$$w'(u) = \frac{\gamma u^{\gamma - 1}}{\left(u^{\gamma} + (1 - u)^{\gamma}\right)^{1/\gamma}} - \frac{u^{2\gamma - 1}}{\left(u^{\gamma} + (1 - u)^{\gamma}\right)^{1 + 1/\gamma}} + \frac{u^{\gamma} (1 - u)^{\gamma - 1}}{\left(u^{\gamma} + (1 - u)^{\gamma}\right)^{1 + 1/\gamma}}.\tag{10.51}$$

We have already shown that the terms in the denominator of  $(10.51)$  have a maximum at  $u = \frac{1}{2}$ . Furthermore it is not hard to see that they achieve their minimum value at  $u = 0$  or  $u = 1$ , respectively. That means, if we use the maximum in the positive terms on the right hand side of  $(10.51)$ , and the minimum in the negative term, we can bound the derivative of the probability weighting function from below by

$$w'(u) \ge \gamma 2^{1-1/\gamma} u^{\gamma-1} - u^{2\gamma-1} + 2^{\gamma-1/\gamma} u^{\gamma} (1-u)^{\gamma-1}.$$
 (10.52)

Using this expression in  $(10.50)$ , we can represent the lower limit of the value functional as a sum of the generic form

$$\int_0^1 (1-u)^{\frac{\alpha}{1-q}} w'(u) du \ge \sum_{n=1}^3 c_n \int_0^1 (1-u)^{a_n} u^{b_n} du. \tag{10.53}$$

The integrals on the right hand side of  $(10.53)$  are immediately identified as betafunctions (see for example Abramowitz and Stegun, 1970, p. 258)

$$B(x,y) = \int_0^1 (1-t)^{x-1} t^{y-1} dt = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)},$$
(10.54)

for  $x, y > 0$ , where capital gamma denotes the *Euler*- $\Gamma$ -function

$$\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt.$$
 (10.55)

The  $\Gamma$ -function diverges to infinity, if its argument tends to zero. Thus, we have to check if one of the terms on the right hand side of  $(10.53)$  contains a suitable exponent. It turns out that the first two integrals are finite in any case, but the third one is not. Plugging in the numbers, one obtains

$$\int_0^1 (1-u)^{\frac{\alpha}{1-q} + \gamma - 1} u^{\gamma} du = \frac{\Gamma\left(\frac{\alpha}{1-q} + \gamma\right) \Gamma(1+\gamma)}{\Gamma\left(1 + \frac{\alpha}{1-q} + 2\gamma\right)}.$$
(10.56)

The argument of the first  $\Gamma$ -function in the numerator becomes zero for

$$q^* = \frac{\alpha}{\gamma} + 1,\tag{10.57}$$

and because  $c_3 = 2^{\gamma - 1/\gamma} > 0$ , the prospective value functional diverges to plus infinity,

$$\lim_{q \to q^*} \int_0^1 (1-u)^{\frac{a}{1-q}} w'(u) du = \infty, \tag{10.58}$$

if q approaches  $q^*$  from below. That means, probability weighting can cause infinite prospective value, even though the expectation value of the associated random variable is finite.

To remedy this problem, De Giorgi and Hens (2006) suggest using the alternative value function

$$v(x) = \begin{cases} \lambda^+(1 - e^{-\alpha x}) & \text{for } x \ge 0\\ \lambda^-(e^{\alpha x} - 1) & \text{for } x < 0. \end{cases}$$
(10.59)

Unlike the original value function  $(10.9)$  on page 188, this modified version is bounded. Surprisingly, this is enough to prevent the prospective value functional from becoming infinite. This can be seen from the simple inequality

$$V[X] = \int_{-\infty}^{\infty} v(x) dw(F(x)) \le \lambda^{+} \int_{0}^{1} dw(u) = \lambda^{+}, \qquad (10.60)$$

where again the substitution  $F(x) = u$  was used in the second step. De Giorgi and Hens chose the parameters  $\alpha = 0.2$ ,  $\lambda^+ = 6.52$ , and  $\lambda^- = 14.7$ , in order to align their version with the original unbounded value function as closely as possible for small and moderate values of  $x$ ; see Figure 10.6. In this way, the experimental results of Kahneman and Tversky still remain valid. Note that the quotient  $\lambda^{-}/\lambda^{+} \approx 2.25$  is approximately

![](_page_22_Figure_1.jpeg)

**Fig. 10.6** Original CPT value function (gray) and bounded version (black) by De Giorgi and Hens (2006)

equal to the original coefficient of loss aversion, estimated by Kahneman and Tversky (1992).

Unifying prospect theory and modern portfolio theory is still an ongoing process. The existence of a market equilibrium is of paramount importance for the success of such a synthesis. So what happens, if we replace the classical mean-variance functional by an appropriate CPT functional? First of all, we have to consider two features of CPT. On the one hand, it is formulated in terms of gains and losses, not in terms of returns, and on the other hand, gains and losses are measured with respect to an appropriate reference point. If we assume that the current price of a portfolio *P* of risky assets is *X*0, then it is quite natural to stipulate the risk-free capital gains *rX*<sup>0</sup> as the reference point. Thus, the prospective value functional becomes

$$V[X_P] = \int_{-\infty}^{\infty} v((x - r)X_0) dw(F(x)), \qquad (10.61)$$

where now *F*(*x*) is the distribution of the portfolio returns *RP*. De Giorgi and Hens (2006) show that if there is a market portfolio, with

$$V[X_{\rm MP}] > 0,\tag{10.62}$$

and short selling is allowed, then the original CPT functional does not permit an equilibrium, because agents would try to infinitely leverage the portfolio. To see this, remember that the capital market line contains all variance efficient portfolios with return

$$R_P = (1 - \lambda)r + \lambda R_{\rm MP}.\tag{10.63}$$

Even though we have not shown at this point that a CPT decision maker would prefer a variance efficient portfolio, we can confidently make this assumption, because usually the expected return of the market portfolio is higher than the risk-free interest rate, and the CPT decision maker is risk averse in the domain of gains. It is not hard to see that using the original value function (10.9) on page 188 with α = β, the relation

$$v(\lambda x) = \lambda^{\alpha} v(x) \tag{10.64}$$

holds, for λ > 0.

![](_page_23_Figure_1.jpeg)

**Fig. 10.7** CPT isoquants and capital market line for *X*<sup>0</sup> = 20 – Original value function of Kahneman and Tversky (left) and piecewise exponential version of De Giorgi and Hens (right)

**Quick calculation 10.14** Confirm this result.

If *G*(*x*) is the distribution function of the market portfolio returns, then, collecting all pieces, we obtain

$$V[X_P] = \int_{-\infty}^{\infty} \lambda^{\alpha} v((x-r)X_0) dw(G(x)) = \lambda^{\alpha} V[X_{\rm MP}]. \tag{10.65}$$

Because α > 0, and we have no short selling constraints, the prospective value of a leveraged position grows without limit for λ → ∞. This does not happen, if we use the modified value function (10.59) on page 202, because the factorization argument (10.64) does not apply in this case.

We can analyze the situation a little bit further by assuming that returns are normally distributed, which is the case in classical portfolio theory. After standard normal transformation, the prospective value functional becomes

$$V[X_P] = \int_{-\infty}^{\infty} v((\sigma x + \mu - r)X_0) dw(\Phi(x)). \tag{10.66}$$

This functional can be evaluated numerically and the resulting isoquants can be illustrated in a µ-σ-diagram. It is not surprising that in this more restrictive setting the infinite leverage problem shows up again; see Figure 10.7 left. Because the isoquants become parallel for large σ, it is advantageous to lever the portfolio without limit, if the slope of the capital market line (CML) is steeper than the slope of the isoquants for σ → ∞. If the slope of the CML is shallower, agents exclusively invest in the riskless asset. This leads to another strange effect, called the robustness problem by De Giorgi

and Hens (2006). If the slope of the CML is just below the critical point, only the riskless asset is held. But a very small change in the expected return or standard deviation of the market portfolio can increase the slope beyond the critical point. In this situation the demand for the market portfolio immediately jumps to an infinite amount. There is no smooth transition from risk-free to risky portfolio portions, but only a sharp switch. Thus, very small changes in µ, σ, or *r* can have a tremendous impact on the resulting market configuration. This is certainly not a desirable situation.

It can be shown rigorously, but also be seen from Figure 10.7 right, that these problems do not occur and an equilibrium exists, if the piecewise exponential value function (10.59) is used. The terminal slope of the isoquants increases with increasing level of prospective value. Hence, there is only one isoquant tangent to the capital market line and its tangential point yields the expected return and standard deviation of the agent's equilibrium portfolio *P*. Even though it seems that this is the end of the story, there is one further subtle point. A necessary requirement for the existence of an equilibrium was the absence of the scaling property (10.64) of the value function. But on the other hand, this scaling property is necessary to proceed from gains to returns without any qualitative changes. Put another way, the equilibrium *P* indicated in Figure 10.7 right is only valid for an initial investment of *X*<sup>0</sup> = \$20. A decision maker, who wants to invest for example \$1000 would realize a completely different equilibrium portfolio on the capital market line. In principle, this problem can be remedied by individually adjusting the parameters of the value function. But CPT is a descriptive theory of decision making and λ + , λ − , and α were chosen to closely resemble the original, empirically estimated value function of Kahneman and Tversky in the experimentally relevant range. So even if CPT is a promising candidate for a realistic theory of decision making in financial markets, there are fundamental questions still to be answered.

# **10.8 Further Reading**

The classical sources are the papers of Kahneman and Tversky (1979, 1992). A concise introduction to prospect theory is found in Hens and Rieger (2010, sect. 2.4). A more comprehensive survey is Barberis and Thaler (2005). There is also a very accessible textbook on behavioral finance by Ackert and Deaves (2010). For a rigorous axiomatic foundation of cumulative prospect theory, see Wakker and Tversky (1993). Explanations for momentum and reversal anomalies were suggested by Barberis et al. (1998), Daniel et al. (2001), and Grinblatt and Han (2005). A full scale formal treatment of CPT equilibrium asset pricing theory is found in De Giorgi et al. (2011).

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

# **10.9 Problems**

**10.1** Consider the following problem presented by Tversky and Kahneman (1983): Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •

and social justice, and also participated in anti-nuclear demonstrations. Which is more probable?

- 1. Linda is a bank teller.
- 2. Linda is a bank teller and is active in the feminist movement.

Most people choose the second answer, but this is a so-called conjunction fallacy. Provide a formal argument why alternative two cannot be more likely than alternative one.

**10.2** Prelec (1998) suggested the alternative probability weighting function

$$w(p) = e^{-(-\log p)^{\gamma}},$$

with 0 < γ < 1. Show that *w*(*p*) satisfies the necessary conditions for a weighting function to be admissible.

- **10.3** Reconsider Problem 10.2 and its solution to show that *w*(*p*), like the original weighting function of Kahneman and Tversky, is *S*-shaped, with concave curvature for small *p* and convex curvature for large *p*.
- **10.4** Birnbaum and Navarrete (1998) analyzed choices between two gambles. One of their decision problems was the following:

| Gamble A:           | Gamble B:           |  |  |
|---------------------|---------------------|--|--|
| •                   | •                   |  |  |
| \$12 with prob. 5%  | \$12 with prob. 10% |  |  |
| •                   | •                   |  |  |
| \$14 with prob. 5%  | \$90 with prob. 5%  |  |  |
| •                   | •                   |  |  |
| \$96 with prob. 90% | \$96 with prob. 85% |  |  |
|                     |                     |  |  |

73% of the test subjects chose gamble B. Provide a fundamental argument why this is a bad choice.

**10.5** A random variable *X* is said to dominate another random variable *Y* statewise, if

$$X(\omega) \ge Y(\omega)$$

holds, for all ω∈ Ω, with strict inequality for at least one ω. Statewise dominance implies first order stochastic dominance, but the converse is not true. Construct two random variables *X* and *Y* on the probability space of a fair coin flip experiment, such that *X* dominates *Y* stochastically to first order, but no statewise dominance relation can be established.